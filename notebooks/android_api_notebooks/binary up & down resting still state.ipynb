{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "rest = pd.read_csv(\"../../data_files/data_from_android_api/rest/rest_25_mins.csv\")\n",
    "\n",
    "up = pd.read_csv(\"../../data_files/data_from_android_api/up_down_still/up5mins_still.csv\")\n",
    "down = pd.read_csv(\"../../data_files/data_from_android_api/up_down_still/down5mins_still.csv\")\n",
    "\n",
    "\n",
    "dataDF = pd.concat([up, down])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "u = [up.delta.mean(), up.theta.mean(), up.alphaLow.mean(), \n",
    "     up.betaHigh.mean(), up.betaLow.mean(), up.alphaHigh.mean(), \n",
    "     up.gammaLow.mean(), up.gammaMid.mean()]\n",
    "\n",
    "d = [down.delta.mean(), down.theta.mean(), down.alphaLow.mean(), \n",
    "     down.betaHigh.mean(), down.betaLow.mean(), down.alphaHigh.mean(), \n",
    "     down.gammaLow.mean(), down.gammaMid.mean()]\n",
    "\n",
    "\n",
    "index = ['delta', 'theta', 'alphaLow','alphaHigh', 'betaLow', 'betaHigh', 'gammaLow', 'gammaMid']\n",
    "\n",
    "df = pd.DataFrame({'up': u, 'down': d}, index=index)\n",
    "ax = df.plot.bar(rot=0, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# create an array of shape 30706, 9 = number of records by the features\n",
    "data = np.array([[0 for x in range(8)] for y in range(len(dataDF))])\n",
    "for i in range(len(dataDF)):\n",
    "    data[i] = [dataDF.delta.values[i],\n",
    "                       dataDF.theta.values[i],\n",
    "                       dataDF.alphaLow.values[i],\n",
    "                       dataDF.alphaHigh.values[i],\n",
    "                       dataDF.betaLow.values[i],\n",
    "                       dataDF.betaHigh.values[i],\n",
    "                       dataDF.gammaLow.values[i],\n",
    "                       dataDF.gammaMid.values[i]]\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "labels = encoder.fit_transform(dataDF.action.values)\n",
    "\n",
    "# creating training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "stan_scaler = StandardScaler()\n",
    "\n",
    "x_train = stan_scaler.fit_transform(x_train)\n",
    "x_test = stan_scaler.transform(x_test)\n",
    "\n",
    "all_data = dataDF.drop(['action'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12600197 0.12819726 0.11712773 0.09682568 0.10940296 0.147043\n",
      " 0.14410301 0.1312984 ]\n",
      "The score for Random Forest  0.43231441048034935\n",
      "684\n",
      "Accuracy for x_test: 0.43231441048034935\n",
      "Cross Validation Accuracy: 0.37 (+/- 0.57)\n",
      "[0.34408602 0.         0.01098901 0.02197802 0.08791209 0.63736264\n",
      " 0.63736264 0.65934066 0.63736264 0.61538462]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Random Forrest\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "print(rfc.feature_importances_)\n",
    "\n",
    "print(\"The score for Random Forest \", rfc.score(x_test, y_test))\n",
    "y_pred = rfc.predict(x_test)\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(len(y_train))\n",
    "print(\"Accuracy for x_test:\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "scores = cross_val_score(rfc, all_data, labels, cv=10, scoring='accuracy')\n",
    "print(\"Cross Validation Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "The score for XGBoost  0.5327510917030568\n",
      "Accuracy for x_test: 0.5327510917030568\n",
      "Accuracy: 53.28%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train, y_train)\n",
    "print(xgb)\n",
    "print(\"The score for XGBoost \", xgb.score(x_test, y_test))\n",
    "y_pred = xgb.predict(x_test)\n",
    "\n",
    "print(\"Accuracy for x_test:\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_50 (Dense)             (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/7\n",
      "684/684 [==============================] - 1s 1ms/step - loss: 0.7057 - acc: 0.5643\n",
      "Epoch 2/7\n",
      "684/684 [==============================] - 0s 227us/step - loss: 0.6873 - acc: 0.5789\n",
      "Epoch 3/7\n",
      "684/684 [==============================] - 0s 219us/step - loss: 0.6741 - acc: 0.5936\n",
      "Epoch 4/7\n",
      "684/684 [==============================] - 0s 195us/step - loss: 0.6648 - acc: 0.5921\n",
      "Epoch 5/7\n",
      "684/684 [==============================] - 0s 213us/step - loss: 0.6580 - acc: 0.5950\n",
      "Epoch 6/7\n",
      "684/684 [==============================] - 0s 195us/step - loss: 0.6531 - acc: 0.6418\n",
      "Epoch 7/7\n",
      "684/684 [==============================] - 0s 201us/step - loss: 0.6496 - acc: 0.6579\n",
      "229/229 [==============================] - 0s 973us/step\n",
      "loss and metrics [0.6442484558930043, 0.6855895201712197]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "stan_scaler = StandardScaler()\n",
    "\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# from keras import regularizers kernel_regularizer=regularizers.l2(0.01), \n",
    "from keras.optimizers import Adam\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(1, input_shape=(8,), activation='sigmoid'))\n",
    "\n",
    "# Adam = Adam(lr=0.05)\n",
    "network.compile(optimizer='Adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['acc'])\n",
    "\n",
    "network.summary()\n",
    "\n",
    "history = network.fit(x_train, y_train,\n",
    "                      epochs=7, verbose=1, batch_size=5)\n",
    "\n",
    "loss_and_metrics = network.evaluate(x_test, y_test)\n",
    "print('loss and metrics', loss_and_metrics)\n",
    "\n",
    "# print('prediction: ', network.predict(test_data))\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "all_labels = dataDF.action.values\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "all_labels = encoder.fit_transform(all_labels)\n",
    "    \n",
    "# create an array of shape 30706, 9 = number of records by the features\n",
    "all_data = np.array([[0 for x in range(8)] for y in range(len(dataDF))])\n",
    "for i in range(len(dataDF)):\n",
    "    all_data[i] = [dataDF.delta.values[i],\n",
    "                       dataDF.theta.values[i],\n",
    "                       dataDF.alphaLow.values[i],\n",
    "                       dataDF.alphaHigh.values[i],\n",
    "                       dataDF.betaLow.values[i],\n",
    "                       dataDF.betaHigh.values[i],\n",
    "                       dataDF.gammaLow.values[i],\n",
    "                       dataDF.gammaMid.values[i]]\n",
    "    \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "stan_scaler = StandardScaler()\n",
    "\n",
    "all_data = scaler.fit_transform(all_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 656 samples, validate on 164 samples\n",
      "Epoch 1/40\n",
      "656/656 [==============================] - 1s 2ms/step - loss: 0.6811 - acc: 0.5884 - val_loss: 0.5104 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "656/656 [==============================] - 0s 521us/step - loss: 0.6784 - acc: 0.5884 - val_loss: 0.5202 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "656/656 [==============================] - 0s 523us/step - loss: 0.6782 - acc: 0.5884 - val_loss: 0.5342 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "656/656 [==============================] - 0s 600us/step - loss: 0.6783 - acc: 0.5884 - val_loss: 0.5121 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "656/656 [==============================] - 0s 629us/step - loss: 0.6778 - acc: 0.5884 - val_loss: 0.5167 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "656/656 [==============================] - 0s 501us/step - loss: 0.6783 - acc: 0.5884 - val_loss: 0.5156 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "656/656 [==============================] - 0s 519us/step - loss: 0.6781 - acc: 0.5884 - val_loss: 0.5328 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "656/656 [==============================] - 0s 650us/step - loss: 0.6776 - acc: 0.5884 - val_loss: 0.5232 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "656/656 [==============================] - 0s 512us/step - loss: 0.6783 - acc: 0.5884 - val_loss: 0.5243 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "656/656 [==============================] - 0s 516us/step - loss: 0.6777 - acc: 0.5884 - val_loss: 0.5297 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "656/656 [==============================] - 0s 518us/step - loss: 0.6775 - acc: 0.5884 - val_loss: 0.5301 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "656/656 [==============================] - 0s 523us/step - loss: 0.6775 - acc: 0.5884 - val_loss: 0.5197 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "656/656 [==============================] - 0s 495us/step - loss: 0.6771 - acc: 0.5884 - val_loss: 0.5430 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "656/656 [==============================] - 0s 494us/step - loss: 0.6776 - acc: 0.5884 - val_loss: 0.5388 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "656/656 [==============================] - 0s 500us/step - loss: 0.6768 - acc: 0.5884 - val_loss: 0.5123 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "656/656 [==============================] - 0s 505us/step - loss: 0.6771 - acc: 0.5884 - val_loss: 0.5264 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "656/656 [==============================] - 0s 540us/step - loss: 0.6771 - acc: 0.5884 - val_loss: 0.5314 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "656/656 [==============================] - 0s 495us/step - loss: 0.6775 - acc: 0.5884 - val_loss: 0.5227 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "656/656 [==============================] - 0s 497us/step - loss: 0.6764 - acc: 0.5884 - val_loss: 0.5160 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "656/656 [==============================] - 0s 510us/step - loss: 0.6766 - acc: 0.5884 - val_loss: 0.5250 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "656/656 [==============================] - 0s 518us/step - loss: 0.6768 - acc: 0.5884 - val_loss: 0.5230 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "656/656 [==============================] - 0s 517us/step - loss: 0.6765 - acc: 0.5884 - val_loss: 0.5349 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "656/656 [==============================] - 0s 538us/step - loss: 0.6764 - acc: 0.5884 - val_loss: 0.5350 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "656/656 [==============================] - 0s 520us/step - loss: 0.6778 - acc: 0.5884 - val_loss: 0.5266 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "656/656 [==============================] - 0s 526us/step - loss: 0.6766 - acc: 0.5884 - val_loss: 0.5457 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "656/656 [==============================] - 0s 528us/step - loss: 0.6770 - acc: 0.5884 - val_loss: 0.5348 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "656/656 [==============================] - 0s 534us/step - loss: 0.6764 - acc: 0.5884 - val_loss: 0.5350 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "656/656 [==============================] - 0s 533us/step - loss: 0.6769 - acc: 0.5869 - val_loss: 0.5361 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "656/656 [==============================] - 0s 523us/step - loss: 0.6762 - acc: 0.5884 - val_loss: 0.5236 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "656/656 [==============================] - 0s 552us/step - loss: 0.6765 - acc: 0.5884 - val_loss: 0.5389 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "656/656 [==============================] - 0s 501us/step - loss: 0.6760 - acc: 0.5884 - val_loss: 0.5323 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "656/656 [==============================] - 0s 538us/step - loss: 0.6761 - acc: 0.5899 - val_loss: 0.5230 - val_acc: 1.0000\n",
      "Epoch 33/40\n",
      "656/656 [==============================] - 0s 536us/step - loss: 0.6762 - acc: 0.5884 - val_loss: 0.5434 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "656/656 [==============================] - 0s 526us/step - loss: 0.6761 - acc: 0.5854 - val_loss: 0.5395 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "656/656 [==============================] - 0s 539us/step - loss: 0.6760 - acc: 0.5899 - val_loss: 0.5249 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "656/656 [==============================] - 0s 512us/step - loss: 0.6762 - acc: 0.5899 - val_loss: 0.5424 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "656/656 [==============================] - 0s 512us/step - loss: 0.6763 - acc: 0.5884 - val_loss: 0.5417 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "656/656 [==============================] - 0s 494us/step - loss: 0.6756 - acc: 0.5884 - val_loss: 0.5241 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "656/656 [==============================] - 0s 510us/step - loss: 0.6762 - acc: 0.5884 - val_loss: 0.5299 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "656/656 [==============================] - 0s 530us/step - loss: 0.6762 - acc: 0.5884 - val_loss: 0.5334 - val_acc: 1.0000\n",
      "93/93 [==============================] - 0s 24us/step\n",
      "Average accuracy of model on the dev set =  0.6559139727264323\n",
      "Training on fold 2/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 656 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "656/656 [==============================] - 1s 2ms/step - loss: 0.6936 - acc: 0.5869 - val_loss: 0.4812 - val_acc: 1.0000\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656/656 [==============================] - 0s 508us/step - loss: 0.6791 - acc: 0.5869 - val_loss: 0.5321 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "656/656 [==============================] - 0s 500us/step - loss: 0.6780 - acc: 0.5869 - val_loss: 0.5498 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "656/656 [==============================] - 0s 523us/step - loss: 0.6773 - acc: 0.5869 - val_loss: 0.5394 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "656/656 [==============================] - 0s 491us/step - loss: 0.6776 - acc: 0.5869 - val_loss: 0.5512 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "656/656 [==============================] - 0s 501us/step - loss: 0.6775 - acc: 0.5869 - val_loss: 0.5408 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "656/656 [==============================] - 0s 495us/step - loss: 0.6775 - acc: 0.5869 - val_loss: 0.5274 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "656/656 [==============================] - 0s 497us/step - loss: 0.6767 - acc: 0.5869 - val_loss: 0.5672 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "656/656 [==============================] - 0s 518us/step - loss: 0.6763 - acc: 0.5869 - val_loss: 0.5471 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "656/656 [==============================] - 0s 499us/step - loss: 0.6766 - acc: 0.5869 - val_loss: 0.5319 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "656/656 [==============================] - 0s 500us/step - loss: 0.6764 - acc: 0.5869 - val_loss: 0.5425 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "656/656 [==============================] - 0s 507us/step - loss: 0.6762 - acc: 0.5869 - val_loss: 0.5424 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "656/656 [==============================] - 0s 502us/step - loss: 0.6762 - acc: 0.5869 - val_loss: 0.5456 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "656/656 [==============================] - 0s 571us/step - loss: 0.6767 - acc: 0.5869 - val_loss: 0.5392 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "656/656 [==============================] - 0s 498us/step - loss: 0.6761 - acc: 0.5869 - val_loss: 0.5327 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "656/656 [==============================] - 0s 503us/step - loss: 0.6759 - acc: 0.5869 - val_loss: 0.5385 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "656/656 [==============================] - 0s 510us/step - loss: 0.6759 - acc: 0.5869 - val_loss: 0.5520 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "656/656 [==============================] - 0s 502us/step - loss: 0.6760 - acc: 0.5884 - val_loss: 0.5367 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "656/656 [==============================] - 0s 503us/step - loss: 0.6761 - acc: 0.5884 - val_loss: 0.5430 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "656/656 [==============================] - 0s 517us/step - loss: 0.6757 - acc: 0.5869 - val_loss: 0.5394 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "656/656 [==============================] - 0s 550us/step - loss: 0.6761 - acc: 0.5884 - val_loss: 0.5688 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "656/656 [==============================] - 0s 558us/step - loss: 0.6758 - acc: 0.5869 - val_loss: 0.5403 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "656/656 [==============================] - 0s 505us/step - loss: 0.6757 - acc: 0.5884 - val_loss: 0.5521 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "656/656 [==============================] - 0s 502us/step - loss: 0.6754 - acc: 0.5884 - val_loss: 0.5524 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "656/656 [==============================] - 0s 508us/step - loss: 0.6758 - acc: 0.5884 - val_loss: 0.5520 - val_acc: 0.9939\n",
      "Epoch 26/40\n",
      "656/656 [==============================] - 0s 511us/step - loss: 0.6756 - acc: 0.5854 - val_loss: 0.5386 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "656/656 [==============================] - 0s 516us/step - loss: 0.6756 - acc: 0.5884 - val_loss: 0.5418 - val_acc: 0.9939\n",
      "Epoch 28/40\n",
      "656/656 [==============================] - 0s 541us/step - loss: 0.6756 - acc: 0.5854 - val_loss: 0.5362 - val_acc: 0.9939\n",
      "Epoch 29/40\n",
      "656/656 [==============================] - 0s 515us/step - loss: 0.6755 - acc: 0.5884 - val_loss: 0.5511 - val_acc: 0.9939\n",
      "Epoch 30/40\n",
      "656/656 [==============================] - 0s 517us/step - loss: 0.6762 - acc: 0.5884 - val_loss: 0.5590 - val_acc: 0.9939\n",
      "Epoch 31/40\n",
      "656/656 [==============================] - 0s 517us/step - loss: 0.6753 - acc: 0.5854 - val_loss: 0.5588 - val_acc: 0.9939\n",
      "Epoch 32/40\n",
      "656/656 [==============================] - 0s 545us/step - loss: 0.6756 - acc: 0.5884 - val_loss: 0.5539 - val_acc: 0.9939\n",
      "Epoch 33/40\n",
      "656/656 [==============================] - 0s 512us/step - loss: 0.6748 - acc: 0.5899 - val_loss: 0.5517 - val_acc: 0.9939\n",
      "Epoch 34/40\n",
      "656/656 [==============================] - 0s 527us/step - loss: 0.6749 - acc: 0.5884 - val_loss: 0.5433 - val_acc: 0.9939\n",
      "Epoch 35/40\n",
      "656/656 [==============================] - 0s 547us/step - loss: 0.6751 - acc: 0.5884 - val_loss: 0.5391 - val_acc: 0.9939\n",
      "Epoch 36/40\n",
      "656/656 [==============================] - 0s 526us/step - loss: 0.6751 - acc: 0.5869 - val_loss: 0.5441 - val_acc: 0.9939\n",
      "Epoch 37/40\n",
      "656/656 [==============================] - 0s 543us/step - loss: 0.6748 - acc: 0.5899 - val_loss: 0.5440 - val_acc: 0.9939\n",
      "Epoch 38/40\n",
      "656/656 [==============================] - 0s 534us/step - loss: 0.6752 - acc: 0.5899 - val_loss: 0.5615 - val_acc: 0.9091\n",
      "Epoch 39/40\n",
      "656/656 [==============================] - 0s 501us/step - loss: 0.6747 - acc: 0.5884 - val_loss: 0.5463 - val_acc: 0.9939\n",
      "Epoch 40/40\n",
      "656/656 [==============================] - 0s 519us/step - loss: 0.6756 - acc: 0.5915 - val_loss: 0.5422 - val_acc: 0.9939\n",
      "92/92 [==============================] - 0s 25us/step\n",
      "Average accuracy of model on the dev set =  0.6540439454764615\n",
      "Training on fold 3/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5203 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 520us/step - loss: 0.6786 - acc: 0.5875 - val_loss: 0.5475 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 528us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5496 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 523us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5361 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 528us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5425 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 528us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5267 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 527us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5164 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 512us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5315 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 503us/step - loss: 0.6777 - acc: 0.5875 - val_loss: 0.5179 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 519us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5244 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 524us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5389 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 531us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5238 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 520us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.4999 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 515us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5222 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 517us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5400 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 509us/step - loss: 0.6771 - acc: 0.5875 - val_loss: 0.5239 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 515us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5461 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 518us/step - loss: 0.6773 - acc: 0.5875 - val_loss: 0.5269 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 516us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5397 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 512us/step - loss: 0.6766 - acc: 0.5875 - val_loss: 0.5438 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 517us/step - loss: 0.6773 - acc: 0.5875 - val_loss: 0.5264 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 504us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5335 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 521us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5186 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 554us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5200 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 612us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5143 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 584us/step - loss: 0.6764 - acc: 0.5875 - val_loss: 0.5367 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 670us/step - loss: 0.6773 - acc: 0.5875 - val_loss: 0.5272 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 559us/step - loss: 0.6758 - acc: 0.5875 - val_loss: 0.5409 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 565us/step - loss: 0.6766 - acc: 0.5875 - val_loss: 0.5253 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 559us/step - loss: 0.6764 - acc: 0.5875 - val_loss: 0.4947 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 572us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5083 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 574us/step - loss: 0.6762 - acc: 0.5875 - val_loss: 0.5329 - val_acc: 1.0000\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 564us/step - loss: 0.6759 - acc: 0.5875 - val_loss: 0.5246 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 565us/step - loss: 0.6758 - acc: 0.5875 - val_loss: 0.4966 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 559us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5185 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 594us/step - loss: 0.6764 - acc: 0.5875 - val_loss: 0.5390 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6764 - acc: 0.5875 - val_loss: 0.5157 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 561us/step - loss: 0.6764 - acc: 0.5875 - val_loss: 0.5417 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 545us/step - loss: 0.6759 - acc: 0.5875 - val_loss: 0.5533 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 520us/step - loss: 0.6755 - acc: 0.5875 - val_loss: 0.5079 - val_acc: 1.0000\n",
      "91/91 [==============================] - 0s 26us/step\n",
      "Average accuracy of model on the dev set =  0.659472520427531\n",
      "Training on fold 4/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.6800 - acc: 0.5875 - val_loss: 0.5187 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 563us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5078 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 559us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5435 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 517us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5508 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 530us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5249 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 564us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5229 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 558us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5487 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 559us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5377 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 576us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5310 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 563us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5410 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 607us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5352 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 619us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5220 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 597us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5379 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 573us/step - loss: 0.6762 - acc: 0.5875 - val_loss: 0.5228 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 600us/step - loss: 0.6761 - acc: 0.5875 - val_loss: 0.5094 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 636us/step - loss: 0.6765 - acc: 0.5875 - val_loss: 0.5137 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 657us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5305 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 672us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5247 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 604us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5283 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 566us/step - loss: 0.6756 - acc: 0.5875 - val_loss: 0.5450 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 567us/step - loss: 0.6752 - acc: 0.5875 - val_loss: 0.5201 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 534us/step - loss: 0.6756 - acc: 0.5875 - val_loss: 0.5258 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 556us/step - loss: 0.6758 - acc: 0.5875 - val_loss: 0.5240 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 536us/step - loss: 0.6752 - acc: 0.5906 - val_loss: 0.5041 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 545us/step - loss: 0.6758 - acc: 0.5890 - val_loss: 0.5303 - val_acc: 0.9697\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 562us/step - loss: 0.6745 - acc: 0.5921 - val_loss: 0.5175 - val_acc: 0.9697\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 538us/step - loss: 0.6749 - acc: 0.5906 - val_loss: 0.5184 - val_acc: 0.9697\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 556us/step - loss: 0.6747 - acc: 0.5921 - val_loss: 0.5283 - val_acc: 0.9697\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 527us/step - loss: 0.6752 - acc: 0.5921 - val_loss: 0.5423 - val_acc: 0.9697\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6745 - acc: 0.5921 - val_loss: 0.5485 - val_acc: 0.9697\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 548us/step - loss: 0.6744 - acc: 0.5921 - val_loss: 0.5476 - val_acc: 0.9697\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 563us/step - loss: 0.6744 - acc: 0.5921 - val_loss: 0.5702 - val_acc: 0.9697\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 571us/step - loss: 0.6744 - acc: 0.5921 - val_loss: 0.5404 - val_acc: 0.9697\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 552us/step - loss: 0.6737 - acc: 0.5921 - val_loss: 0.5529 - val_acc: 0.9697\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 549us/step - loss: 0.6738 - acc: 0.5921 - val_loss: 0.5405 - val_acc: 0.9697\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 532us/step - loss: 0.6739 - acc: 0.5921 - val_loss: 0.5420 - val_acc: 0.9697\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 532us/step - loss: 0.6739 - acc: 0.5921 - val_loss: 0.5228 - val_acc: 0.9697\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 557us/step - loss: 0.6732 - acc: 0.5921 - val_loss: 0.5363 - val_acc: 0.9697\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 613us/step - loss: 0.6736 - acc: 0.5921 - val_loss: 0.5497 - val_acc: 0.9697\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 581us/step - loss: 0.6730 - acc: 0.5921 - val_loss: 0.5316 - val_acc: 0.9697\n",
      "91/91 [==============================] - 0s 34us/step\n",
      "Average accuracy of model on the dev set =  0.6539450512987979\n",
      "Training on fold 5/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.6969 - acc: 0.5145 - val_loss: 0.5913 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 562us/step - loss: 0.6800 - acc: 0.5875 - val_loss: 0.5570 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 584us/step - loss: 0.6795 - acc: 0.5875 - val_loss: 0.5575 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 647us/step - loss: 0.6790 - acc: 0.5875 - val_loss: 0.5427 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 570us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5260 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 557us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5178 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 592us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5225 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 503us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5542 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 589us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5408 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 552us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5475 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 541us/step - loss: 0.6770 - acc: 0.5875 - val_loss: 0.5326 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 563us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5535 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 558us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5347 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 593us/step - loss: 0.6770 - acc: 0.5875 - val_loss: 0.5413 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 644us/step - loss: 0.6777 - acc: 0.5875 - val_loss: 0.5455 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 598us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5459 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 562us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5545 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 552us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5525 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 531us/step - loss: 0.6766 - acc: 0.5875 - val_loss: 0.5081 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 571us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5477 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 554us/step - loss: 0.6762 - acc: 0.5875 - val_loss: 0.5447 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 548us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5319 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 532us/step - loss: 0.6761 - acc: 0.5875 - val_loss: 0.5392 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 567us/step - loss: 0.6764 - acc: 0.5860 - val_loss: 0.5367 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 553us/step - loss: 0.6758 - acc: 0.5875 - val_loss: 0.5612 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 606us/step - loss: 0.6758 - acc: 0.5845 - val_loss: 0.5259 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 610us/step - loss: 0.6763 - acc: 0.5906 - val_loss: 0.5573 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 604us/step - loss: 0.6766 - acc: 0.5906 - val_loss: 0.5500 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 560us/step - loss: 0.6769 - acc: 0.5860 - val_loss: 0.5620 - val_acc: 0.9758\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 584us/step - loss: 0.6759 - acc: 0.5875 - val_loss: 0.5347 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 574us/step - loss: 0.6763 - acc: 0.5860 - val_loss: 0.5481 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 564us/step - loss: 0.6759 - acc: 0.5906 - val_loss: 0.5693 - val_acc: 0.9697\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 605us/step - loss: 0.6760 - acc: 0.5845 - val_loss: 0.5470 - val_acc: 0.9758\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 567us/step - loss: 0.6758 - acc: 0.5845 - val_loss: 0.5396 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 565us/step - loss: 0.6766 - acc: 0.5845 - val_loss: 0.5399 - val_acc: 0.9758\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 619us/step - loss: 0.6756 - acc: 0.5860 - val_loss: 0.5482 - val_acc: 0.9758\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 618us/step - loss: 0.6764 - acc: 0.5860 - val_loss: 0.5583 - val_acc: 0.9697\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 573us/step - loss: 0.6765 - acc: 0.5906 - val_loss: 0.5716 - val_acc: 0.9697\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 604us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5557 - val_acc: 0.9697\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 531us/step - loss: 0.6778 - acc: 0.5860 - val_loss: 0.5506 - val_acc: 0.9697\n",
      "91/91 [==============================] - 0s 26us/step\n",
      "Average accuracy of model on the dev set =  0.6550241729071702\n",
      "Training on fold 6/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_81 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.7004 - acc: 0.5875 - val_loss: 0.4717 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 560us/step - loss: 0.6807 - acc: 0.5875 - val_loss: 0.5057 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 580us/step - loss: 0.6793 - acc: 0.5875 - val_loss: 0.5378 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 579us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5320 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 567us/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5177 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5407 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5362 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 558us/step - loss: 0.6786 - acc: 0.5875 - val_loss: 0.5453 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 536us/step - loss: 0.6790 - acc: 0.5875 - val_loss: 0.5479 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 538us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5422 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 542us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5442 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 560us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5572 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 552us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5270 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 536us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5639 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 558us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5460 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 567us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5440 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 549us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5618 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 537us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5652 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 557us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5269 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 556us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5354 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 554us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5215 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 561us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5250 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 580us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5397 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 575us/step - loss: 0.6775 - acc: 0.5860 - val_loss: 0.5282 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5256 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 562us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5347 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 561us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5091 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 587us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5215 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 552us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5679 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 548us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5456 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 543us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5298 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 550us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5278 - val_acc: 1.0000\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 547us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5183 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 556us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5457 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 560us/step - loss: 0.6774 - acc: 0.5860 - val_loss: 0.5299 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 559us/step - loss: 0.6770 - acc: 0.5875 - val_loss: 0.5387 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 545us/step - loss: 0.6773 - acc: 0.5860 - val_loss: 0.5490 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 541us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5474 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 543us/step - loss: 0.6765 - acc: 0.5875 - val_loss: 0.5626 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 563us/step - loss: 0.6770 - acc: 0.5875 - val_loss: 0.5429 - val_acc: 1.0000\n",
      "91/91 [==============================] - 0s 31us/step\n",
      "Average accuracy of model on the dev set =  0.6575750891442534\n",
      "Training on fold 7/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_87 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.6794 - acc: 0.5875 - val_loss: 0.5365 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5283 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 538us/step - loss: 0.6789 - acc: 0.5875 - val_loss: 0.5171 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 549us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5074 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 600us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5232 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 575us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5192 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 634us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5339 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 596us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5347 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 544us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5265 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 554us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5163 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 588us/step - loss: 0.6765 - acc: 0.5875 - val_loss: 0.5510 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 576us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5360 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 572us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5533 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 573us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5633 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 577us/step - loss: 0.6775 - acc: 0.5830 - val_loss: 0.5427 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 587us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5385 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 575us/step - loss: 0.6765 - acc: 0.5860 - val_loss: 0.5318 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 577us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5362 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 584us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5509 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 568us/step - loss: 0.6771 - acc: 0.5875 - val_loss: 0.5615 - val_acc: 0.9576\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 571us/step - loss: 0.6770 - acc: 0.5860 - val_loss: 0.5440 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 577us/step - loss: 0.6761 - acc: 0.5845 - val_loss: 0.5280 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 578us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5247 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6762 - acc: 0.5875 - val_loss: 0.5341 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 588us/step - loss: 0.6766 - acc: 0.5860 - val_loss: 0.5325 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 583us/step - loss: 0.6759 - acc: 0.5890 - val_loss: 0.5316 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 575us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5321 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 565us/step - loss: 0.6763 - acc: 0.5890 - val_loss: 0.5323 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 573us/step - loss: 0.6767 - acc: 0.5860 - val_loss: 0.5416 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 586us/step - loss: 0.6764 - acc: 0.5845 - val_loss: 0.5448 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 583us/step - loss: 0.6760 - acc: 0.5845 - val_loss: 0.5285 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 594us/step - loss: 0.6759 - acc: 0.5875 - val_loss: 0.5284 - val_acc: 1.0000\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 584us/step - loss: 0.6764 - acc: 0.5860 - val_loss: 0.5342 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 585us/step - loss: 0.6754 - acc: 0.5814 - val_loss: 0.5098 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 594us/step - loss: 0.6759 - acc: 0.5875 - val_loss: 0.5247 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 591us/step - loss: 0.6766 - acc: 0.5875 - val_loss: 0.5367 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 555us/step - loss: 0.6762 - acc: 0.5860 - val_loss: 0.5309 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 556us/step - loss: 0.6753 - acc: 0.5814 - val_loss: 0.5024 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 566us/step - loss: 0.6766 - acc: 0.5860 - val_loss: 0.5221 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 661us/step - loss: 0.6766 - acc: 0.5860 - val_loss: 0.5144 - val_acc: 1.0000\n",
      "91/91 [==============================] - 0s 28us/step\n",
      "Average accuracy of model on the dev set =  0.6593971721707417\n",
      "Training on fold 8/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_93 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.7028 - acc: 0.4916 - val_loss: 0.6020 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 579us/step - loss: 0.6804 - acc: 0.5875 - val_loss: 0.5431 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 575us/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5718 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 554us/step - loss: 0.6789 - acc: 0.5875 - val_loss: 0.5251 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 572us/step - loss: 0.6796 - acc: 0.5875 - val_loss: 0.5268 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 578us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5314 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 687us/step - loss: 0.6794 - acc: 0.5875 - val_loss: 0.5185 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 572us/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5349 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 574us/step - loss: 0.6796 - acc: 0.5875 - val_loss: 0.5392 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 690us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5331 - val_acc: 1.0000\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "657/657 [==============================] - 0s 675us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5189 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 596us/step - loss: 0.6793 - acc: 0.5875 - val_loss: 0.5370 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 692us/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5458 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 684us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5381 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 1s 774us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5070 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 578us/step - loss: 0.6791 - acc: 0.5875 - val_loss: 0.5209 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 580us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5194 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 600us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5495 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 607us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5290 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 619us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5327 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 589us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5523 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 605us/step - loss: 0.6786 - acc: 0.5875 - val_loss: 0.5171 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 582us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5403 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 612us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5260 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 661us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5088 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 610us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5256 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 609us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5469 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 602us/step - loss: 0.6777 - acc: 0.5875 - val_loss: 0.5137 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 626us/step - loss: 0.6777 - acc: 0.5875 - val_loss: 0.5081 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 664us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5252 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 574us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5064 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 576us/step - loss: 0.6777 - acc: 0.5875 - val_loss: 0.5266 - val_acc: 1.0000\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 583us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5361 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 614us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5103 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 664us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5437 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 623us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5256 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 628us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5008 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 585us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5233 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 617us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5322 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 601us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5297 - val_acc: 1.0000\n",
      "91/91 [==============================] - 0s 40us/step\n",
      "Average accuracy of model on the dev set =  0.6607637344406077\n",
      "Training on fold 9/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_99 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.6951 - acc: 0.5875 - val_loss: 0.4649 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 608us/step - loss: 0.6798 - acc: 0.5875 - val_loss: 0.5180 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 611us/step - loss: 0.6790 - acc: 0.5875 - val_loss: 0.5389 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 590us/step - loss: 0.6790 - acc: 0.5875 - val_loss: 0.5346 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 580us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5501 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 612us/step - loss: 0.6789 - acc: 0.5875 - val_loss: 0.5471 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 612us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5512 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 671us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5476 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 699us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5185 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 714us/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5354 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 733us/step - loss: 0.6786 - acc: 0.5875 - val_loss: 0.5329 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 750us/step - loss: 0.6791 - acc: 0.5875 - val_loss: 0.5174 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 727us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5376 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 1s 761us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5390 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 707us/step - loss: 0.6786 - acc: 0.5875 - val_loss: 0.5562 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 677us/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5472 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 706us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5374 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 631us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5370 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 612us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5189 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 609us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5241 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 647us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5485 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 587us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5495 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 571us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5348 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 601us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5381 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 588us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5061 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 611us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5425 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 591us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5243 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 617us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5376 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 611us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5462 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 609us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5440 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 591us/step - loss: 0.6789 - acc: 0.5875 - val_loss: 0.5633 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 606us/step - loss: 0.6783 - acc: 0.5860 - val_loss: 0.5570 - val_acc: 1.0000\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 593us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5215 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 583us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5208 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 591us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5304 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 587us/step - loss: 0.6777 - acc: 0.5875 - val_loss: 0.5370 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 581us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5334 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 578us/step - loss: 0.6786 - acc: 0.5875 - val_loss: 0.5423 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 586us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5377 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 584us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5470 - val_acc: 1.0000\n",
      "91/91 [==============================] - 0s 29us/step\n",
      "Average accuracy of model on the dev set =  0.6618266162060591\n",
      "Training on fold 10/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_105 (Dense)            (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.6932 - acc: 0.5875 - val_loss: 0.4771 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 613us/step - loss: 0.6794 - acc: 0.5875 - val_loss: 0.5241 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 606us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5423 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 601us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5141 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 607us/step - loss: 0.6789 - acc: 0.5875 - val_loss: 0.5099 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 573us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5195 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 644us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5202 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 707us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5172 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 730us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5343 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 702us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5271 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 640us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5314 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 640us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5373 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 662us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5412 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 645us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5420 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 719us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5282 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 710us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5276 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 711us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5154 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 708us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5210 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 667us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5287 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 618us/step - loss: 0.6765 - acc: 0.5875 - val_loss: 0.5288 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 618us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5302 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 746us/step - loss: 0.6761 - acc: 0.5875 - val_loss: 0.5127 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 638us/step - loss: 0.6762 - acc: 0.5875 - val_loss: 0.4855 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 638us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5191 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 656us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5170 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 587us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5211 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 608us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5275 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 605us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5011 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 689us/step - loss: 0.6766 - acc: 0.5875 - val_loss: 0.5200 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 712us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5204 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 710us/step - loss: 0.6761 - acc: 0.5875 - val_loss: 0.5128 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 727us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5315 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 615us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5422 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 744us/step - loss: 0.6766 - acc: 0.5875 - val_loss: 0.5424 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 608us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5312 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 619us/step - loss: 0.6762 - acc: 0.5875 - val_loss: 0.5119 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 621us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5141 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 644us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5156 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 567us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5031 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 622us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5110 - val_acc: 1.0000\n",
      "91/91 [==============================] - 0s 38us/step\n",
      "Average accuracy of model on the dev set =  0.6626769216184203\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=12)\n",
    "avg_loss = []\n",
    "avg_acc = []\n",
    "# Loop through the indices the split() method returns\n",
    "for index, (train_index, test_index) in enumerate(skf.split(all_data, labels)):\n",
    "    print(\"Training on fold \" + str(index + 1) + \"/10.............................................\")\n",
    "    # Generate batches from indices\n",
    "    x_train, x_test = all_data[train_index], all_data[test_index]\n",
    "    # use one-hot vectors as labels\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    network = models.Sequential()\n",
    "    \n",
    "\n",
    "    network.add(layers.Dense(32, input_shape=(8,)))\n",
    "    network.add(layers.Dense(32, activation=\"relu\"))\n",
    "    network.add(layers.Dense(16, activation=\"relu\"))\n",
    "    # network.add(layers.Dropout(0.3))\n",
    "    network.add(layers.Dense(16, activation=\"relu\"))\n",
    "    # network.add(layers.Dropout(0.3))\n",
    "    network.add(layers.Dense(32, activation=\"sigmoid\"))\n",
    "    network.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Adam = Adam(lr=0.05)\n",
    "    network.compile(optimizer=Adam(lr=0.0004),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['acc'])\n",
    "\n",
    "    network.summary()\n",
    "\n",
    "    history = network.fit(x_train, y_train, validation_split=0.2,\n",
    "                          epochs=40, verbose=1, batch_size=3)\n",
    "\n",
    "    loss, accuracy = network.evaluate(x_test, y_test)\n",
    "\n",
    "    # evaluate and store the accuracy\n",
    "#     loss, accuracy = model.evaluate(xtest_imagelist, ytest, verbose=1)\n",
    "    avg_loss.append(loss)\n",
    "    avg_acc.append(accuracy)\n",
    "\n",
    "    # cross validation score\n",
    "    print(\"Average accuracy of model on the dev set = \", np.mean(avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
