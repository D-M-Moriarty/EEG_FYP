{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "rest = pd.read_csv(\"../../data_files/data_from_android_api/rest/rest_25_mins.csv\")\n",
    "\n",
    "left = pd.read_csv(\"../../data_files/data_from_android_api/bicep_lifts/left_bicep_5mins.csv\")\n",
    "right = pd.read_csv(\"../../data_files/data_from_android_api/bicep_lifts/right_bicep_5mins.csv\")\n",
    "\n",
    "\n",
    "dataDF = pd.concat([left, right])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "u = [left.delta.mean(), left.theta.mean(), left.alphaLow.mean(), \n",
    "     left.betaHigh.mean(), left.betaLow.mean(), left.alphaHigh.mean(), \n",
    "     left.gammaLow.mean(), left.gammaMid.mean()]\n",
    "\n",
    "d = [right.delta.mean(), right.theta.mean(), right.alphaLow.mean(), \n",
    "     right.betaHigh.mean(), right.betaLow.mean(), right.alphaHigh.mean(), \n",
    "     right.gammaLow.mean(), right.gammaMid.mean()]\n",
    "\n",
    "\n",
    "index = ['delta', 'theta', 'alphaLow','alphaHigh', 'betaLow', 'betaHigh', 'gammaLow', 'gammaMid']\n",
    "\n",
    "df = pd.DataFrame({'left': u, 'right': d}, index=index)\n",
    "ax = df.plot.bar(rot=0, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# create an array of shape 30706, 9 = number of records by the features\n",
    "data = np.array([[0 for x in range(8)] for y in range(len(dataDF))])\n",
    "for i in range(len(dataDF)):\n",
    "    data[i] = [dataDF.delta.values[i],\n",
    "                       dataDF.theta.values[i],\n",
    "                       dataDF.alphaLow.values[i],\n",
    "                       dataDF.alphaHigh.values[i],\n",
    "                       dataDF.betaLow.values[i],\n",
    "                       dataDF.betaHigh.values[i],\n",
    "                       dataDF.gammaLow.values[i],\n",
    "                       dataDF.gammaMid.values[i]]\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "labels = encoder.fit_transform(dataDF.action.values)\n",
    "\n",
    "# creating training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "stan_scaler = StandardScaler()\n",
    "\n",
    "x_train = stan_scaler.fit_transform(x_train)\n",
    "x_test = stan_scaler.transform(x_test)\n",
    "\n",
    "all_data = dataDF.drop(['action'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15847817 0.10743699 0.13243939 0.11967829 0.12559019 0.11192092\n",
      " 0.13621121 0.10824484]\n",
      "The score for Random Forest  0.62\n",
      "450\n",
      "Accuracy for x_test: 0.62\n",
      "Cross Validation Accuracy: 0.54 (+/- 0.10)\n",
      "[0.6        0.51666667 0.5        0.6        0.53333333 0.45\n",
      " 0.51666667 0.6        0.55       0.5       ]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Random Forrest\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "print(rfc.feature_importances_)\n",
    "\n",
    "print(\"The score for Random Forest \", rfc.score(x_test, y_test))\n",
    "y_pred = rfc.predict(x_test)\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(len(y_train))\n",
    "print(\"Accuracy for x_test:\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "scores = cross_val_score(rfc, all_data, labels, cv=10, scoring='accuracy')\n",
    "print(\"Cross Validation Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "The score for XGBoost  0.6133333333333333\n",
      "Accuracy for x_test: 0.6133333333333333\n",
      "Accuracy: 61.33%\n",
      "Cross Validation Accuracy: 0.60 (+/- 0.24)\n",
      "[0.65 0.6  0.8  0.55 0.5  0.55 0.85 0.7  0.4  0.7  0.55 0.65 0.85 0.7\n",
      " 0.55 0.5  0.45 0.5  0.65 0.5  0.55 0.6  0.6  0.85 0.45 0.6  0.6  0.45\n",
      " 0.65 0.55]\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train, y_train)\n",
    "print(xgb)\n",
    "print(\"The score for XGBoost \", xgb.score(x_test, y_test))\n",
    "y_pred = xgb.predict(x_test)\n",
    "\n",
    "print(\"Accuracy for x_test:\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "scores = cross_val_score(xgb, all_data, labels, cv=30, scoring='accuracy')\n",
    "print(\"Cross Validation Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_54 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 9,729\n",
      "Trainable params: 9,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/100\n",
      "450/450 [==============================] - 1s 2ms/step - loss: 0.6973 - acc: 0.5378\n",
      "Epoch 2/100\n",
      "450/450 [==============================] - 0s 514us/step - loss: 0.6855 - acc: 0.5400\n",
      "Epoch 3/100\n",
      "450/450 [==============================] - 0s 495us/step - loss: 0.6843 - acc: 0.5556\n",
      "Epoch 4/100\n",
      "450/450 [==============================] - 0s 482us/step - loss: 0.6741 - acc: 0.5822\n",
      "Epoch 5/100\n",
      "450/450 [==============================] - 0s 529us/step - loss: 0.6672 - acc: 0.6133\n",
      "Epoch 6/100\n",
      "450/450 [==============================] - 0s 524us/step - loss: 0.6624 - acc: 0.5822\n",
      "Epoch 7/100\n",
      "450/450 [==============================] - 0s 525us/step - loss: 0.6458 - acc: 0.6133\n",
      "Epoch 8/100\n",
      "450/450 [==============================] - 0s 513us/step - loss: 0.6479 - acc: 0.6067\n",
      "Epoch 9/100\n",
      "450/450 [==============================] - 0s 500us/step - loss: 0.6398 - acc: 0.6333\n",
      "Epoch 10/100\n",
      "450/450 [==============================] - 0s 522us/step - loss: 0.6493 - acc: 0.6067\n",
      "Epoch 11/100\n",
      "450/450 [==============================] - 0s 520us/step - loss: 0.6420 - acc: 0.6156\n",
      "Epoch 12/100\n",
      "450/450 [==============================] - 0s 519us/step - loss: 0.6285 - acc: 0.6378\n",
      "Epoch 13/100\n",
      "450/450 [==============================] - 0s 524us/step - loss: 0.6167 - acc: 0.6400\n",
      "Epoch 14/100\n",
      "450/450 [==============================] - 0s 525us/step - loss: 0.6375 - acc: 0.6067\n",
      "Epoch 15/100\n",
      "450/450 [==============================] - 0s 522us/step - loss: 0.6172 - acc: 0.6356\n",
      "Epoch 16/100\n",
      "450/450 [==============================] - 0s 522us/step - loss: 0.6115 - acc: 0.6356\n",
      "Epoch 17/100\n",
      "450/450 [==============================] - 0s 520us/step - loss: 0.6287 - acc: 0.6200\n",
      "Epoch 18/100\n",
      "450/450 [==============================] - 0s 535us/step - loss: 0.6219 - acc: 0.6267\n",
      "Epoch 19/100\n",
      "450/450 [==============================] - 0s 523us/step - loss: 0.6054 - acc: 0.6444\n",
      "Epoch 20/100\n",
      "450/450 [==============================] - 0s 542us/step - loss: 0.6130 - acc: 0.6556\n",
      "Epoch 21/100\n",
      "450/450 [==============================] - 0s 507us/step - loss: 0.6008 - acc: 0.6756\n",
      "Epoch 22/100\n",
      "450/450 [==============================] - 0s 537us/step - loss: 0.6074 - acc: 0.6489\n",
      "Epoch 23/100\n",
      "450/450 [==============================] - 0s 532us/step - loss: 0.6150 - acc: 0.6356\n",
      "Epoch 24/100\n",
      "450/450 [==============================] - 0s 522us/step - loss: 0.5903 - acc: 0.6489\n",
      "Epoch 25/100\n",
      "450/450 [==============================] - 0s 563us/step - loss: 0.5846 - acc: 0.6533\n",
      "Epoch 26/100\n",
      "450/450 [==============================] - 0s 579us/step - loss: 0.5956 - acc: 0.6467\n",
      "Epoch 27/100\n",
      "450/450 [==============================] - 0s 544us/step - loss: 0.5793 - acc: 0.6867\n",
      "Epoch 28/100\n",
      "450/450 [==============================] - 0s 533us/step - loss: 0.5821 - acc: 0.6444\n",
      "Epoch 29/100\n",
      "450/450 [==============================] - 0s 547us/step - loss: 0.5787 - acc: 0.6600\n",
      "Epoch 30/100\n",
      "450/450 [==============================] - 0s 536us/step - loss: 0.5821 - acc: 0.6756\n",
      "Epoch 31/100\n",
      "450/450 [==============================] - 0s 542us/step - loss: 0.5838 - acc: 0.6622\n",
      "Epoch 32/100\n",
      "450/450 [==============================] - 0s 515us/step - loss: 0.5961 - acc: 0.6533\n",
      "Epoch 33/100\n",
      "450/450 [==============================] - 0s 531us/step - loss: 0.5897 - acc: 0.6489\n",
      "Epoch 34/100\n",
      "450/450 [==============================] - 0s 564us/step - loss: 0.5813 - acc: 0.6733\n",
      "Epoch 35/100\n",
      "450/450 [==============================] - 0s 503us/step - loss: 0.5688 - acc: 0.6867\n",
      "Epoch 36/100\n",
      "450/450 [==============================] - 0s 559us/step - loss: 0.5742 - acc: 0.6644\n",
      "Epoch 37/100\n",
      "450/450 [==============================] - 0s 575us/step - loss: 0.5599 - acc: 0.6911\n",
      "Epoch 38/100\n",
      "450/450 [==============================] - 0s 557us/step - loss: 0.5627 - acc: 0.6889\n",
      "Epoch 39/100\n",
      "450/450 [==============================] - 0s 517us/step - loss: 0.5825 - acc: 0.6556\n",
      "Epoch 40/100\n",
      "450/450 [==============================] - 0s 546us/step - loss: 0.5765 - acc: 0.6711\n",
      "Epoch 41/100\n",
      "450/450 [==============================] - 0s 548us/step - loss: 0.5683 - acc: 0.6644\n",
      "Epoch 42/100\n",
      "450/450 [==============================] - 0s 496us/step - loss: 0.5604 - acc: 0.6578\n",
      "Epoch 43/100\n",
      "450/450 [==============================] - 0s 506us/step - loss: 0.5598 - acc: 0.6978\n",
      "Epoch 44/100\n",
      "450/450 [==============================] - 0s 495us/step - loss: 0.5469 - acc: 0.6956\n",
      "Epoch 45/100\n",
      "450/450 [==============================] - 0s 472us/step - loss: 0.5638 - acc: 0.6667\n",
      "Epoch 46/100\n",
      "450/450 [==============================] - 0s 557us/step - loss: 0.5756 - acc: 0.6844\n",
      "Epoch 47/100\n",
      "450/450 [==============================] - 0s 515us/step - loss: 0.5499 - acc: 0.6844\n",
      "Epoch 48/100\n",
      "450/450 [==============================] - 0s 493us/step - loss: 0.5541 - acc: 0.7044\n",
      "Epoch 49/100\n",
      "450/450 [==============================] - 0s 502us/step - loss: 0.5725 - acc: 0.6733\n",
      "Epoch 50/100\n",
      "450/450 [==============================] - 0s 522us/step - loss: 0.5334 - acc: 0.6933\n",
      "Epoch 51/100\n",
      "450/450 [==============================] - 0s 513us/step - loss: 0.5516 - acc: 0.6867\n",
      "Epoch 52/100\n",
      "450/450 [==============================] - 0s 519us/step - loss: 0.5385 - acc: 0.6689\n",
      "Epoch 53/100\n",
      "450/450 [==============================] - 0s 537us/step - loss: 0.5543 - acc: 0.6756\n",
      "Epoch 54/100\n",
      "450/450 [==============================] - 0s 499us/step - loss: 0.5375 - acc: 0.7022\n",
      "Epoch 55/100\n",
      "450/450 [==============================] - 0s 522us/step - loss: 0.5213 - acc: 0.6933\n",
      "Epoch 56/100\n",
      "450/450 [==============================] - 0s 500us/step - loss: 0.5320 - acc: 0.6933\n",
      "Epoch 57/100\n",
      "450/450 [==============================] - 0s 515us/step - loss: 0.5261 - acc: 0.7244\n",
      "Epoch 58/100\n",
      "450/450 [==============================] - 0s 518us/step - loss: 0.5421 - acc: 0.7133\n",
      "Epoch 59/100\n",
      "450/450 [==============================] - 0s 543us/step - loss: 0.5370 - acc: 0.7333\n",
      "Epoch 60/100\n",
      "450/450 [==============================] - 0s 498us/step - loss: 0.5298 - acc: 0.6978\n",
      "Epoch 61/100\n",
      "450/450 [==============================] - 0s 511us/step - loss: 0.5311 - acc: 0.6911\n",
      "Epoch 62/100\n",
      "450/450 [==============================] - 0s 521us/step - loss: 0.5321 - acc: 0.7111\n",
      "Epoch 63/100\n",
      "450/450 [==============================] - 0s 522us/step - loss: 0.5381 - acc: 0.7178\n",
      "Epoch 64/100\n",
      "450/450 [==============================] - 0s 517us/step - loss: 0.5176 - acc: 0.7133\n",
      "Epoch 65/100\n",
      "450/450 [==============================] - 0s 508us/step - loss: 0.5331 - acc: 0.6889\n",
      "Epoch 66/100\n",
      "450/450 [==============================] - 0s 519us/step - loss: 0.5058 - acc: 0.7267\n",
      "Epoch 67/100\n",
      "450/450 [==============================] - 0s 512us/step - loss: 0.5238 - acc: 0.7089\n",
      "Epoch 68/100\n",
      "450/450 [==============================] - 0s 478us/step - loss: 0.5194 - acc: 0.7178\n",
      "Epoch 69/100\n",
      "450/450 [==============================] - 0s 527us/step - loss: 0.5128 - acc: 0.7222\n",
      "Epoch 70/100\n",
      "450/450 [==============================] - 0s 518us/step - loss: 0.5156 - acc: 0.6978\n",
      "Epoch 71/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450/450 [==============================] - 0s 519us/step - loss: 0.5085 - acc: 0.7422\n",
      "Epoch 72/100\n",
      "450/450 [==============================] - 0s 490us/step - loss: 0.4983 - acc: 0.7311\n",
      "Epoch 73/100\n",
      "450/450 [==============================] - 0s 495us/step - loss: 0.4726 - acc: 0.7289\n",
      "Epoch 74/100\n",
      "450/450 [==============================] - 0s 490us/step - loss: 0.5419 - acc: 0.6778\n",
      "Epoch 75/100\n",
      "450/450 [==============================] - 0s 500us/step - loss: 0.5036 - acc: 0.7044\n",
      "Epoch 76/100\n",
      "450/450 [==============================] - 0s 529us/step - loss: 0.5058 - acc: 0.7244\n",
      "Epoch 77/100\n",
      "450/450 [==============================] - 0s 495us/step - loss: 0.5197 - acc: 0.7244\n",
      "Epoch 78/100\n",
      "450/450 [==============================] - 0s 496us/step - loss: 0.5145 - acc: 0.7022\n",
      "Epoch 79/100\n",
      "450/450 [==============================] - 0s 495us/step - loss: 0.5005 - acc: 0.7156\n",
      "Epoch 80/100\n",
      "450/450 [==============================] - 0s 502us/step - loss: 0.5072 - acc: 0.7311\n",
      "Epoch 81/100\n",
      "450/450 [==============================] - 0s 497us/step - loss: 0.4945 - acc: 0.7267\n",
      "Epoch 82/100\n",
      "450/450 [==============================] - 0s 490us/step - loss: 0.4822 - acc: 0.7222\n",
      "Epoch 83/100\n",
      "450/450 [==============================] - 0s 496us/step - loss: 0.4922 - acc: 0.7289\n",
      "Epoch 84/100\n",
      "450/450 [==============================] - 0s 499us/step - loss: 0.4869 - acc: 0.7600\n",
      "Epoch 85/100\n",
      "450/450 [==============================] - 0s 501us/step - loss: 0.4970 - acc: 0.7467\n",
      "Epoch 86/100\n",
      "450/450 [==============================] - 0s 495us/step - loss: 0.4871 - acc: 0.7444\n",
      "Epoch 87/100\n",
      "450/450 [==============================] - 0s 496us/step - loss: 0.4726 - acc: 0.7200\n",
      "Epoch 88/100\n",
      "450/450 [==============================] - 0s 519us/step - loss: 0.4887 - acc: 0.7222\n",
      "Epoch 89/100\n",
      "450/450 [==============================] - 0s 593us/step - loss: 0.4731 - acc: 0.7444\n",
      "Epoch 90/100\n",
      "450/450 [==============================] - 0s 579us/step - loss: 0.4905 - acc: 0.7200\n",
      "Epoch 91/100\n",
      "450/450 [==============================] - 0s 531us/step - loss: 0.5011 - acc: 0.7200\n",
      "Epoch 92/100\n",
      "450/450 [==============================] - 0s 531us/step - loss: 0.4893 - acc: 0.7356\n",
      "Epoch 93/100\n",
      "450/450 [==============================] - 0s 535us/step - loss: 0.4967 - acc: 0.7267\n",
      "Epoch 94/100\n",
      "450/450 [==============================] - 0s 493us/step - loss: 0.4769 - acc: 0.7511\n",
      "Epoch 95/100\n",
      "450/450 [==============================] - 0s 502us/step - loss: 0.4850 - acc: 0.7311\n",
      "Epoch 96/100\n",
      "450/450 [==============================] - 0s 500us/step - loss: 0.4833 - acc: 0.7267\n",
      "Epoch 97/100\n",
      "450/450 [==============================] - 0s 496us/step - loss: 0.4685 - acc: 0.7356\n",
      "Epoch 98/100\n",
      "450/450 [==============================] - 0s 511us/step - loss: 0.4596 - acc: 0.7556\n",
      "Epoch 99/100\n",
      "450/450 [==============================] - 0s 542us/step - loss: 0.4823 - acc: 0.7400\n",
      "Epoch 100/100\n",
      "450/450 [==============================] - 0s 543us/step - loss: 0.4706 - acc: 0.7444\n",
      "150/150 [==============================] - 0s 1ms/step\n",
      "loss and metrics [0.9939631883303325, 0.49999999602635703]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4wLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvqOYd8AAAIABJREFUeJzs3Xl8pFWZ6PHfk8pS2fets3anF3qBbkIDNouIsgkz4BVF8DIuc5XR667MvXivow5z7wzOuAyjXtRRuI4oiN4ZRQcHVARlk266G+iFNJ10Z+ksnaWSVJJKKqk694/3fStVlapUpTuVdCfP9/PJh9Rb71t1isD71DnPec4RYwxKKaXUfNKWuwFKKaXOfBoslFJKJaTBQimlVEIaLJRSSiWkwUIppVRCGiyUUkolpMFCrXoi0igiRkTSkzj3fSLyzFK0S6kziQYLdVYRkeMi4heRsqjj++0bfuPytEyplU2DhTobHQNucx6IyLlA9vI158yQTM9IqVOlwUKdjX4AvCfs8XuBfwk/QUQKReRfRKRfRNpF5HMikmY/5xKRL4vIgIi0ATfEuPZ7ItIjIidE5H+JiCuZhonIT0SkV0RGROT3IrI17LlsEfmK3Z4REXlGRLLt5y4TkedEZFhEOkXkffbxp0TkA2GvETEMZvemPiIirwOv28futV9jVEReEpHLw853icj/EJFWEfHaz9eJyDdF5CtRn+UXIvLJZD63Wvk0WKiz0QtAgYhstm/i7wIejDrn60AhsA64Aiu4vN9+7oPAnwDnAzuBd0Rd+31gBlhvn3MN8AGS8ytgA1AB7AV+GPbcl4ELgEuAEuC/AUERqbev+zpQDuwA9if5fgBvAy4GttiPd9uvUQL8CPiJiLjt5z6N1Su7HigA/hyYsD/zbWEBtQx4C/DQAtqhVjJjjP7oz1nzAxwHrgI+B/wdcB3wayAdMEAj4AKmgC1h1/0F8JT9+5PAh8Keu8a+Nh2otK/NDnv+NuB39u/vA55Jsq1F9usWYn0x8wHbY5z3WeDf4rzGU8AHwh5HvL/9+m9O0A6P875AC3BTnPMOA1fbv38UeGy5/976c+b86BinOlv9APg9sJaoISigDMgE2sOOtQM19u9rgM6o5xwNQAbQIyLOsbSo82Oyezn/G3gnVg8hGNaeLMANtMa4tC7O8WRFtE1EPoPVE1qDFUwK7DYkeq/vA7djBd/bgXtPo01qhdFhKHVWMsa0YyW6rwf+NerpAWAa68bvqAdO2L/3YN00w59zdGL1LMqMMUX2T4ExZiuJvRu4CavnU4jVywEQu02TQFOM6zrjHAcYB3LCHlfFOCe0dLSdn/jvwC1AsTGmCBix25DovR4EbhKR7cBm4GdxzlOrkAYLdTb7L1hDMOPhB40xAeAR4H+LSL6INGCN1Tt5jUeAj4tIrYgUA3eFXdsDPAF8RUQKRCRNRJpE5Iok2pOPFWgGsW7wfxv2ukHgfuCrIrLGTjTvEpEsrLzGVSJyi4iki0ipiOywL90PvF1EckRkvf2ZE7VhBugH0kXk81g9C8d3gb8RkQ1iOU9ESu02dmHlO34A/D9jjC+Jz6xWCQ0W6qxljGk1xuyJ8/THsL6VtwHPYCV677ef+2fgceBlrCR0dM/kPVjDWIewxvt/ClQn0aR/wRrSOmFf+0LU83cCr2LdkIeALwFpxpgOrB7SZ+zj+4Ht9jVfA/xAH9Yw0Q+Z3+NYyfIjdlsmiRym+ipWsHwCGAW+R+S04+8D52IFDKVCxBjd/EgpZRGRN2L1wBrt3pBSgPYslFI2EckAPgF8VwOFiqbBQimFiGwGhrGG2/5xmZujzkA6DKWUUioh7VkopZRKaMUU5ZWVlZnGxsblboZSSp1VXnrppQFjTHmi81ZMsGhsbGTPnnizKJVSSsUiIu2Jz9JhKKWUUknQYKGUUiohDRZKKaUSWjE5i1imp6fp6upicnJyuZuyZNxuN7W1tWRkZCx3U5RSK8iKDhZdXV3k5+fT2NhI2HLTK5YxhsHBQbq6uli7du1yN0cptYKs6GGoyclJSktLV0WgABARSktLV1VPSim1NFZ0sABWTaBwrLbPq5RaGis+WCilVDxPtZzk2MB44hOVBotUGhwcZMeOHezYsYOqqipqampCj/1+f1Kv8f73v5+WlpYUt1Sp1enjD+3jnl8dXu5mnBVWdIJ7uZWWlrJ//34AvvjFL5KXl8edd94ZcY6zGXpaWuy4/cADD6S8nUqtRt7JaUYnZ3i+dZBA0OBK0yHc+WjPYhkcPXqUbdu28aEPfYjm5mZ6enq444472LlzJ1u3buXuu+8OnXvZZZexf/9+ZmZmKCoq4q677mL79u3s2rWLkydPLuOnUOrs1jtiTQQZnZzhUPfoMrfmzLdqehZ//YuDi/4fxJY1BXzhT7ee0rWHDh3igQce4Fvf+hYA99xzDyUlJczMzHDllVfyjne8gy1btkRcMzIywhVXXME999zDpz/9ae6//37uuuuuWC+vlEqge2R21uCzrQOcW1u4jK1J7DeH+qgqdLOtZnnaqT2LZdLU1MSFF14YevzQQw/R3NxMc3Mzhw8f5tChQ3Ouyc7O5q1vfSsAF1xwAcePH1+q5iq14vSO+AAozM7gudbBZW7N/IwxfOYnL/MPjy9f/nLV9CxOtQeQKrm5uaHfX3/9de69915efPFFioqKuP3222PWSmRmZoZ+d7lczMzMLElblVqJuocnEYEbzqvm3/aewD8TJDP9zPz+3Dc6xYhvmn0dHoJBQ9oy5FdS+m9GRK4TkRYROSoic8ZLRORrIrLf/jkiIsNhzwXCnns0le1cbqOjo+Tn51NQUEBPTw+PP/74cjdJqRWvd2SS8rws3rihHN90gP2dw4kvWiYtfV7Ayq+0DYwtSxtS1rMQERfwTeBqoAvYLSKPGmNC4yvGmE+Fnf8x4Pywl/AZY3akqn1nkubmZrZs2cK2bdtYt24dl1566XI3SakVr2d0kupCN7vWlSICz7UOcNHakiV570DQ8K2nW7n94gYKcxKv43ak1xv6fW/7MOsr8lPZvJhSOQx1EXDUGNMGICIPAzcBcwfjLbcBX0hhe5bVF7/4xdDv69evD02pBavq+gc/+EHM65555pnQ78PDs998br31Vm699dbFb6hSq0TPsI+m8jwKczLYtqaQ544O8smrlua993cO8w+Pt1CYncHtb2hIeP5rvV7K8rKYDgTZ2+HhlgvrlqCVkVI5DFUDdIY97rKPzSEiDcBa4Mmww24R2SMiL4jI2+Jcd4d9zp7+/v7FardSahXoHZmkqtANwCXrS9nX6WHCvzR5wPZBq2r8SJ83wZmEzttcnc/59UXs7fCksmlxpTJYxMrAmDjn3gr81BgTCDtWb4zZCbwb+EcRaZrzYsZ8xxiz0xizs7w84RaySikFWAV53qkZqp1g0VTGdMCw5/jS3IiPD04A0NKbOFgEgobXT3rZWJlPc30xr58cY3RyOtVNnCOVwaILCO8r1QLdcc69FXgo/IAxptv+ZxvwFJH5jKQZEy8+rUyr7fMqdSqcgrzqomwALmwsJsMl/OLlboLB1P8/dHxgtmeR6P/ZzqEJJqeDbLKDhTGwv2Ppk/GpDBa7gQ0islZEMrECwpxZTSKyCSgGng87ViwiWfbvZcClxM91xOV2uxkcHFw1N1BnPwu3273cTVHqjOYU5Dk9i5zMdP7T+TX85KUubvn28xw9mdoZR84wlGdimn7v1Lznvmb3PjZV5bO9rhARlmUoKmUJbmPMjIh8FHgccAH3G2MOisjdwB5jjBM4bgMeNpF39M3At0UkiBXQ7gmfRZWs2tpaurq6WE35DGenPKVUfE5BnhMsAL5083lctLaUv/nlIa6/9w/83dvP5eYL4v+/9ErXMC+0DXLHG+eMkCd0fHCCpvJcWvvHaenzUlEQ/wuek9fYUJlHTmY6myrz2RvWs7jvqVZ8/hk+edXGlNZfpLQozxjzGPBY1LHPRz3+YozrngPOPd33z8jI0B3jlFJz9IxYBXmVYTdpEeEdF9RyxcZy/ux7f+R7zxybN1g8+EI7j+zp4s/e0Eh2pivp9x6e8DPim+bdF9dz31OttPR6uXxD/JxrS5+X+pIccjKt2/X59cX88hVruOz5tkH+4fHXuP7calK9lc2ZWa6olFoRvv/ccZ4+cub17HuGrYK8DNfcW2B5fhZvWFdK++D4vEPYrf3WUFLH0MSC3ttJbjfXF1OWl5VwRtSRXiu57WiuL8I7OcMzRwf42EP7aCrP40s3n5fyjc80WCilUubrT77Ol5dxPaN4nIK8eBpLcxj3BxgYi7/vTGu/ldc4PriwzZOcfMXashw2VeXNOyNqaiZA28A451SFBYuGYgA+9OBLTE0HuO/2C8jNSv3KTRoslFIpEQwaPBPTvHpihO5h33I3J0LPsI/qwuy4zzeUWWu3tccJBEPjfoYnpuc9J55jA+OIQG1xDhsr8znSNxZ3BlZb/ziBoGFjWLBYV5ZLYXYGE/4A//DO7ayvyFvQ+58qDRZKqZTwTs4QsG+Cvznct8ytiRRekBdLY6kVLJwho2hOr2K+c+JpH5xgTWE27gwXmyrz8U0H6PLEDqbOENWmsGEoEeFDVzTx2beew/XnVi/ovU+HBgulVEoMTcwO4Txx8MwJFtEFebHUFGXjSpO4vYZWe2pteX7WgnsWxwfHaSjNAazpsDC7UGC0ll4v6WnC2rLciOMfflMTf3HFwmdhnQ4NFkqplBgat4LF5uoCXmgbZMS39FXHsUQX5MWSmZ5GTVE2xwbiBIv+MbLS09i1rpTjAwvvWTTYPZcNdo+hpTf2xmwtvV6ayvPOiKXTl78FSqkVyWMHi1svrGMmaHiq5czYBji6IC+exrJc2uMOQ42ztiyXdeW5dI/4mJwOxDzvxLCPjz20jxE7vzHim2Zo3M/aMqtnkZeVTm1xNi19c4sAjTG81uuNyFcsJw0WSqmUcIahrtxUQXl+1hkzFBWrIC+WxtIcjseZPtvaP0ZTeR6NpbkYA12e2EHlsVd6+MXL3fzkJWtNVWfIyulZgJWPOBJjRtSBE6OcGPYt2bLpiWiwUGqFeODZY/zojx3L3YwQp2dRmpfJVZsrearlJFMzsb+BL6VYBXmxNJTm4p2cwTMROXw2NROgc8iqwHZyD/GGopxlOR7Z04kxJpQMbwwPFlX5tPaP4Z8JRlz74z0dZKWnceP2NQv7gCmiwUKpFeK7fzjGj3efOcFiaMJPZnoaOZkurtlaybg/cEbsdT1fQV64RicQRCWw2wcnCBpoqsgLJZ5j1VoYY9jb4SEvK50jfWPs6xym3c6B1JfkhM7bVJXPTNBE5Ed8/gA/39fN9edWU5ideHOkpaDBQqkVwDs5zYlhHycTLEq3lDzjfkpyMhERLmkqJTfTxf3PHEtZotsz7ud9D7zIoe7YyWJHooI8hzNUdDwqye3MhGoqz6MoJ5PC7IyYuY3ukUn6Rqf48JuayMl08cjuTo4PTlBV4I5YHsSpzt7fObs44K8O9OCdmuFdy7DJUTwaLJRaAV63b2D93qklWWI7GUPj0xTnZgKQle7ik1dt5NmjA1z91af5jwO9i/5+v3+9n6da+vmLB/eEEsqxJCrIc9SVZJMmc+sonBoLp1fh5Dai7W23bv6XbyjjhnOrefTlbg71jNJYlhNx3oaKPM6pyufvfvUanfbSIQ/v7qSxNIeLz5B8BWiwUGpFcJaMmAkaPBPxl6hYSp4JPyW5s0MoH3zjOn7+kcsozcviQw++tOjLgOxt95CZnkbvyCSfemR/zKA5EwjSPeybtyDPkZXuYk1R9pw6itb+cdYUukNLbDSU5sYMFvs6hnFnpLG5uoBbL6pjwh/gcM9oRL4CIN2Vxn23X0AgYPivP9zLa72jvHhsiFsurEv5ek8LocFCqRUgfH2hM2UoyjPupzgnM+LYubWFPPrRS7mwsXjRq7r3dgxzQX0xn7thC0++dpJv/u7onHP+de8Jxv0BLl1fltRrNpbmzulZtPWP0RS2xEZjaQ4nPL45Ceq9HR7Oqykiw5VGc30xTeVWkGiIChZg9VK+cst2Xj0xwn/+5z/iShPe0XxmbTWgwUKpFeBIn5cMl/UtNNFmOgtx0jvJbd95IVTIthBDE3ODBUCGK436klxGFzF34bO/tTc3FPGeXQ28bccavvqbI/w+bMXbqZkA9/72dbbXFnLV5oqkXrehNCeiZ2GMobV/nHVhFdWNZbkEo6bPTk4HONg9wvkNRYC1RMetF9Zb55dGDkM5rtlaxYff1MTguJ8rN1XMu8fFctBgodQKcKTPS3O9tRrpYvYs9ncM83zbYMRNNxkzgSAjvtmcRbSinIxFTXS/emKEmaChub4YEeFv334uGyvy+cTD+0I38R/v7uTEsI/PXLMp6eGdxtJchiemGbaH9k56pxibmonoWTg9hfAk98HuEaYDJvQ3Abj1ojr+4op1XL4x/t4Vn7l6I3957Sbueus5yX/4JaLBQqmzzJ0/eZmfvtQVejwwNsXAmJ/LN1hDKye9C+8FxOMs0X2ge2RB1434pjEGSnJiT/sszM5g3B9gOhCM+TzAEwd7uexLT3LpPdbPzfc9N2eox+HUM+yos77J52Sm860/u4CZgOEjP9zLyMQ0X3/yKBetLQn9e0pGqI7CDgThM6EcsabY7m23drILDxb57gw++9bN5M2znHi6K42PXLl+yVaSXQgNFkqdRQJBw7/tO8H3njkWOuZU/+6oKyYvK52To4vXsxgYs17rwImFBQunkC1ez8KpHZivd/H4wT6GJ6bZ1VRKY1kOL7V7aBuIvTf23nYPjaU5lOZlhY6tLcvly7ds5+WuEf70G8/Q753izgX0KsAaYoLZymtnJlR4sCjJzSQ/Kz2iZ7G3w0NdSTbl+VmsFBoslDqLDIxNEQgaDveMhqZZOiuWbqzKoyI/a1FzFs5rHe7xhpYbjzY07ued33qOJ1+bTVg7M7JKTiNYHOnzcn59EV9+53b+x/WbAWg9Ga/4bTjiW7zj2q1VfOiKJjqGJnjjxvIFL51RX5KDiFWhPTTu599f7SE300VlwWwQEBEaynIiiur2dQxzft3c9pzNNFgodRYJ30ToiUPWzflIn5finAzK87Ioz89a5GEoK1j4pgMci/GtPhA0fPyhfew+7uHpltm8hrPibKwEN0ChPTw1HKceIhA0HOnzhvZxWFdmfZMP30fC0eXxMTA2xfkNsW/Od16zkc/dsJm//U/bYj4/H3eGi+oCN7860MNVX32aPcc9MXMejaW5od5H97CP3tFJmuuLFvx+ZzINFkqdRZxZSTmZLn59yCpsa7H3aBYRO1gsbs+iLM+64R84Mbcy+mu/PsIzRwdwZ6RFTDF11oVK1LOINyOqY2iCqZlgaMXV7EwXNUXZMYOFk6+Id3NOd6XxgcvXUVscexZSIg2lubzW66W+JIdffvwy/vyytXPOcabY7rj7Ca752u+t9sQJXmer1G/cqpRaNM7y2m9vruFHf+xgaNzPkb4x3t5cA0BFvpuToycxxixKQdfA2BQXry3lN4f7OHBihLedXxN67jeH+vjG745yy85axv0BDoblNZwVZ+P2LBIMQzl1I+E7xDVV5NHWH7tSOifTFXHuYrrz2k209Y/x9uZaXGmx/52+60Kr6C4QtBLwpXlZbFtTmJL2LBcNFkqdRXpHfLgz0njnBXU8+EIHD77QztjUTGjHtYqCLHzTAcb9gXln3QD84uVuvvuHNv75PTvjzunv905RWeDmnOqCiBlRwxN+Pv3IfrauKeDum7bx9Sdf5/EDvUwHgmS40vCM+8nOcEWsgRSuKEGwONLnRQQ2VM4mkpvKc/nx8aE5gXBf5zDba4tIT7Aw4Km6oKGYCxL0EupKcvj8n25JyfufKXQYSqmzSPfIJNWF2ZxXW0hVgZv7n7VmRTnfqivs2TcnR+fPW0xOB/ibXx7i5a4RPvqjfTGnsPr8VtApy89k65oCDnaPhvZ2+Nm+E4xOzvClm8/DneGisTSXmaDhhL2X9ND4dNwhKICC7PlzFi32sE9O5mzAayrPY8IfoDfss01OBzjUPcr5Kyw/cCbSYKHUWaR3xFoxVUS4ektl6Ga7IRQsrB5CorzFD55v56R3ivfuauDF40N86VevzTnHSW6X20Mq3skZOod8GGN4eHcn59YUsq3GGmppjFqq2zPhpzg3/tLaGa40cjNd8Yeh+ryh1Vgd6+zlMsJnRL3cORwqxlOppcFCqbNIT9gieNdsrQSsHd+cHECFPaVzvmAxNjXDfU+3cvmGMv76pm28d1cD333mGP/+Sk/Eec5rlOVnsa2mALCK8149McJrvd6I5bOd4jWn1mAoxrpQ0YpyMmMGi6mZAMcGxufkINbbtQ3htRYvtA0hAhc2njmrs65UGiyUOksEgoY+71RoL4aL15aS704P5SsguWGoB545xtC4n89cswmA/3nDFprri/hvP30Z7+TszTu8Z7GxMp/0NOHAiREe3t2JOyONG3fM7uBWnpdFTqYromcx3zAUWENRI765K+S29Y8TCJo5e0+X52eRn5UeqqIGeLZ1gG1rCkNTcVXqaLBQ6izR77UK8py9GDLT0/j27RdErCNUmJ1BpistbmHeyMQ03/lDG1dvqQwtjZGZnsZ/fdN6xv0BjobdiJ3XKM/Pwp3hYkNlPruPD/GL/dYObgXu2Ru0iNBQmrugnkVhdnrMnoUzE+qcqGAhIqyryKPVnhHl8wfY1+HhkqbSed9HLQ4NFkqdJXpGrORx+C5vl6wv45yqgtDjRLUWD/6xHe/kDJ++emPE8eicA8z2LJwewtY1Bew+7rF2cNs5dwe3tWXWJkDTgSDeyZnEw1DZsYehWuwVdKP3fQBrRpRTa7H7+BDTAcMlSS43rk6PBgulzhI9do1Fol3eyudZ8mNfxzAbKvLYXF0QcbyuJDu0rIWj3ztFSW5maK/qbWusa9aW5cZcNqOhNJfOoYmwIDP/0FBhdkbM2VBHer2sK8sjM33u7ampPI+ekUnGp2Z4rnWQ9DThwkZNbi8FDRZKnSVmg8X8+xxUzLPkx5E+75xcANi7whVG7go3MDZbvQ3WxkUAt+yMvYNbY2kO0wHDQbvSO94igo7COMuUt8RpIxDaQOjYwDjPtQ5wfn1RxPRalToaLJQ6S/QMWwV5RQmSuRUFsYehxqdm6BiaiFvp3FiWE7Fkx8CYP2LV1Ob6Yv7ptvN5/6WNMa939nXY12ktv1GSMGeRwdRMkMnpQOjY2NQMXR7fnHyFw1ntdW+HhwMnRrikSYeglooGC6XOEj2jVkFeomU8KvLdDE9MMzUTiDj+up28jq5fcDSELYYHzrpQkaur3rh9De6M2FXZa+28x74Oay+HhD2LGOtDHXFW0I3TxvrSHFxpwo/+2EHQoMntJaTBQqnTZIwJVTanUs+wL+EQFMxOn43OWxyJM8vIsbY0F8/ENCN2HsEahkp+P4aK/CzcGWm80mUtC5Jo6qwTLIbDg0WMNaHCZaW7qC/J4bVeL+6MNHZo5faS0WCh1Gn6+pNHueGfnkn5+/SOTIYK8uYTrzCvpc+6wdaVxF59tSFsx7fxqRkm/IEFbd4jYs1gGpuaAUg4XOY8PxLRsxgjO8NFbXH8JL6Tt7iwsYSs9Ni9HLX4UhosROQ6EWkRkaMicleM578mIvvtnyMiMhz23HtF5HX7572pbKdSp+P51kEO947G3fJzIaKHjhxOQd6aBDOhIGzJj6gd81p6vWyoyI+7cmr49FlnRtNCehYwG3DystIT3shDK8+GzYjqGBqnoTSHtDhtBFhn5y00X7G0UhYsRMQFfBN4K7AFuE1EIpZlNMZ8yhizwxizA/g68K/2tSXAF4CLgYuAL4iIzo9TZxxjDC19Xow5/b2v+71TnPfFJ/iPAz0xnwsETVI9i/LQMFRke2KttxSuvmR2yY7wgryFcGoj5lsXyhFrGKpjaCLhvhPOZ7hM6yuWVCp7FhcBR40xbcYYP/AwcNM8598GPGT/fi3wa2PMkDHGA/wauC6FbVXqlAyM+UO7wjlTW0/VkT4vUzNBHnj2+JznYhXkxVOam4lIZM5iaNxPv3cqbr4C7F3hCt1RPYv58w7RnN5JoplQYBXlwewwlDGGziFfKGjFc+P2NfzwAxeHpvKqpZHKYFEDdIY97rKPzSEiDcBa4MmFXqvUcnJm78DpBwtnT+0/HhuK2M85/LUTFeSBtTNcaW7k9NnQLKN5ggXYO74NjNM/ZgXAhfYsnGGoRDOhAPLd6YjMBovBcT++6QB1JfN/xsz0NC7VXsWSS2WwiDXoGG/KyK3AT40xzoBtUteKyB0iskdE9vT398e4RKnk+PwBAsGFz2hy1jECa7bS6ej0TOBKE1xpwiN7OiOeS7Ygz1ERteRHrJ3nYmksywkNQ4kk10OIuL40+Z5FWpqQn5UemjrrBMu6U9z+VKVWKoNFFxC+gEwt0B3n3FuZHYJK+lpjzHeMMTuNMTvLy8tPs7lqNbv6a09z/b1/YJ+9n3OyWnq9lORmkpeVvgg9Cx9ritxcuamcn77UxUzYhkTJFuQ5qgvdHOnzhl6jpc9LgTudyoL5ewoNpbkMjvs5NjBOSU7mgnefqypwk+9OTyq3AtYy5cP2Fqyd9sZJ8WZrqeWVymCxG9ggImtFJBMrIDwafZKIbAKKgefDDj8OXCMixXZi+xr7mFKLbsJvVQ0fOenl7fc9x1//4iAT/pmkrm3p87KpMp/qQje9pxssPBPUFedwy846+r1T/K5ltrecbEGe4507a+ny+PjZfus71pFeL+dUFSS8vtEeRnrp+NCCh6DA6i3864cv4S+uaErq/MLs2SU/nJ7FfNNm1fJJWbAwxswAH8W6yR8GHjHGHBSRu0XkxrBTbwMeNmFVTcaYIeBvsALObuBu+5hSi27QHp//qxu2cPvFDTzw7HG+8sSRhNcFg4bX+7xsqsqnqtAdSkKfqs6hCepLcrjynArK87P48e6O0HPJFuQ5rt1axbaaAv7xN0fwzwTt9ZbyEl7nLNnRPTK54Gmzjg2V+aGZTolEB4uyvExyE+wdrpZHSv8qxpjHgMeijn0+6vEX41x7P3B/yhqnlG3Qns3UWJYY6s2qAAAgAElEQVTDn1+2lldPjETkIuI5Mexj3B9gY2U+Pn9gzjWBoGFwfCpU9zCfCf8MA2N+6kpyyHClcXNzLf/8hzaeOzpAnjudLo+PyzYkn9QVET5zzSbe/8Bu7v3tEbyTMwnzFTCboIaFJ7dPRWF2Bt12kO30JJ42q5aPVnCrVW/ATgSX5lo3x5ribE4kkawOJY3tnkX/2FREYd6Pd3dy2T2/C+2/MJ8ue7zeGYJ514V1BI3h3d/9Izd+41lOeqdoKJm7v8N83rSxnJ0Nxdz3VCsQf72lcDmZs3mNhU6bPRWFORlhCW6f5ivOYBos1FkrGDSnPfQDMDhuBwv75lhTZAWLROs9tYQWvctjTZF7TmHevg4P/kCQr/068ZBWaCaQfbNcW5bLzz9yKd97706+996dPPC+C/ngG9cu6HOJCHdeuwlnktemBNNmHc5Q1KkOQy2EMww1EwjSPeyjTvMVZywNFuqs9ejL3ez6uyf59CP78YzP3cs5WQN2ziLUsyjKxj8TDB2P50ifl5qibPLdGVTZ9Q/hSW6ntuGXr/RwuGd03teKNW30vNoi3rK5krdsruTKcypOad+GN6wr5fINZdSVZFOU5DRYJ8m9VMNQ0wHDsYFxZoJGexZnMA0W6qx1qGeUNIFH93dz1Vef5pevxJuZPb/BMT+5mS6yM621jGqKrBt/oqGoll4vGyutpLGTfO62g0UwaDjSN8bbm2vId6cnTJh3DPnIznClZOjn//znZn58x66kz1/KnkWRnQh/9YS1Um2i6m21fDRYqLNW59AEjWW5/OJjl1FTnM1Hf7SP41GVz8kYHJ+iLOxb9Bo7WHTPEyymA0Fa+8fYZO9/7QSLXntYrMvjwzcd4OK1Jdxx+Tp+c7hv3hqOTs+EvbVpclNjFyLfnRH6TMnYsqYAkaW5cTuzpg7Yu+tpQd6ZS4OFOm1t/WMET6H6OVlWHcTEnONOXcLm6gL+6k+sNSrbh+ael8jgmJ/SsOUpauxx8xOe+MHi+MA40wHDJns6ar47g7ysdLqHrZ7Fa73WzW9jZT7vv2wtJbmZfHWe3EXn0MQZc6N808Zynr7zytA6T6k0GyxGSBOoLkp+erBaWhos1Gnp8kxw1Vef5jeH+1L2Hl9+/Ah/+vVn5gQka/aMdWOvKrBuMn2nUBg3MDZFadiQS2F2BvlZ6fMOQ7XE2NEtvDDPyVdsqMwnLyudO964jj+8PkBbjJlRxhi6PGfOTCARob50adpSaFekH+weobowm4wFVoyrpaN/GXVaTnh8BM3sUg2p8NSRk3gmpukdnQ0Eo5PTjPimQ9/GnWRs3+ipBAv/nFzBmqK502f7Rif57eE+++ckrjQJ7QkNRBTmtfSNUVucTZ5dYHbFRms5GmdsPtzwxDRjUzOrsnLZ6VmM+xMvIKiWl5ZKqtPiFLQNjU8lOPPU9I5M0tZv5SFa+8dCY+/RU03dGS6KczIiAkoygkHD0PhUaCaUo6Y4e84w1Ccf3s/zbYOhx1uqCyL2o64udIdqL1p6RyOWA19fkUdmehoHu0e5aUfkAsodUZ9lNQmv9Nbk9plNg4U6LYP2vgdDpzF1dT7PtQ6Efm89OcblG6xv6LGmmlYWuBfcsxj2TRM0szUWjjVFbl5qn01IB4OGl7uGuXH7Gj5wuVXvEF1tXF2YTf/YFBP+Gdr6x7lqc2XouQxXGudU5XMgRs+i07N6V1vNy0rHlSYEgmZVfv6ziQYLdVqcWoTBBDUJp+q51kGKczKYCRjawmY6dQ5Z3/rDv41awWJhPRwn2JVGTROtKcphxGcND+VlpXNscJwJf4DLNpRxXm1RzNeqLrQK815oG2QmaOYUwW1dU8i/v9KNMSZi1pPzWVbjMIyIUJidwdC4f1X2rM4mmrNQp8Wpfk5Fz8IYw/Otg+xqKmVdRV7Eshmdngny3emhBClYSe6FDkM5wS46Z+HMiHKmzx7stmY3bVsTf3e2anuI7Gl7tdjo5TW21RQwOjkTWtoj/LMU52SQ705u8b2VxhmKWo3B8myiwUKdFqdHkYpg0T44wYlhH7uaymgqz6X1ZHjPYu5U08qCLAbGpiL2gUjECXbRBWg19hROJ8l98MQIma40NlTGX7nVqbV46kg/6VHJb5gNNAe7I4eiOocmVvW36lCw0GGoM5oGC3VanGAxmIJg8VyrlUy+tKmUpvI8ekcnGZuy9pno9PjmfBOttIeB+seSH4oaDC31EdWzKLJuXE6S+0D3CJuq8ued2ukEi/bBCdaW5ZKZHnnupqp8XGkSKkBznEk1FsuhMDuDrPS0JVleRJ06DRbqtAzY38xHfNNML+AbfTKebR2gqsDN2rLc0Lf0tv4xjDGxexb2UuALyVsMjE2RJsxZN6k8P4v0NAktKHjgxCjbagrmfS2nMA9i73XtznCxoSKPA2E9i0DQcGLYR+0qHoJpKs9je21RSqrX1eLRYLFKnfROzhkOORUD3iky7W/bnon4vYtXuobp9yZ/Ew8GDS+0DnJJUykiwvoKq5q4rX+cfu8UUzPBOUM3VaElN5LPWwyM+SnJzcSVFnmjcqUJ1UVuuod9nBj2MeKbZus8+YroNsTbO2LrmsJQ/gOs2o3pwOqeCfQ/b9jMgx+4eLmboRLQYLFK/cN/tPDe+188rdfwzwQZnZxhXbl1I/eMT8c8bzoQ5LbvvMDf/epw0q995KSXwXE/l6y3NvypL8nFlSa09o+FpppGz8uvsPdhCF8mPJHBsbk1Fo6aIqvWwhk22laTOFg4Q1Hx9o7YVlNAv3eKk3Yi/vdHrGT4horEu9itVK40mTNkp848+hdapQ71jDIw5md0MvYNPhlOUtu5MQ7GKcx7rcfLuD/As0cHEu4R4Xj2qJWv2NVUCkBmehr1JTlWsIgz1bQs1xo6WkjPYnDcP6fGwuFUcR/sHsGVJhFFdvE4wSLeuU7AOdA9wtRMgK8/eZTtdUVctLYk6TYrtRw0WKxCgaDh9ZPWNNTOU1h4zzFgJ5KdZbrjzYjaa6+22jc6FVErMZ+9HR5qi7NDy4UDoRlRTpuji+LS0oSK/KwF5SwGo9aFCldblE3f6CT7O4dZX54XUa0dz3m1RdQUZced3bS52lrR9cCJUR5+sZMTwz7uvGajjterM54Gi1WofXA8tP2n8y39VDgzoDbYPYv5gkW2faN1Zjgl08boqadN5XkcGxzn+OAE5flZMW/eFVFV3MYY/v2VHianA7E/Q4x1oRw1xdkEDfyxbYitCZLbjtvf0MAz//3KOTkQR15WOmtLc9nT7uEbvzvKRWtLuGx98ntrK7VcEgYLEfmoiBQvRWPU0nDWL4LT61k41c/OTT1eFffeDg9XbCynpiib544OxDwnnDGG4wMToR3bHE3lefhngrzQNhh3+83owrzdxz185Ed7+T+/Ozrn3MnpAN6pmbib/DjrUPkDwXmL8aIl6iVsrSnk90f66fdO8ZfXbtJehTorJNOzqAJ2i8gjInKd6H/ZZ72WPi8ikJPpCiWLT4UTHCoLsijKyYjZs+j3TtE55KO5oYhLmkp5vm0w4d4Xg+N+xqZm5uyn0GTPiDox7Iu76FxlQVZEz2JP+xAA33vmWCi4hb8PzK2xcIQPgSWT3E7WtjVWL+WKjeVc2Ki5CnV2SBgsjDGfAzYA3wPeB7wuIn8rIk0pbptKkSN9XhpKcmgszT29nMX4FJnpaeRlpVOSmxkzWDi7wzXXF3PJ+lKGJ6Y5lGA/6vZBK6/RWBoZLNaVzQ5LxcsJVBa68U7OMOG3ivf2tg9TkpuJbzrAt55ujTg33rpQjvDd5TZXJ05uJ+vS9WUUZmfwl9duWrTXVCrVkspZGGsKS6/9MwMUAz8Vkb9PYdtUilh7R+dTX5JzWvtQDHj9lOVmIiKU5mbGnA21t2OYDJewraaQXeussfnnE+Qtjg9YAawhahiqODeTErsXEK8uIbwwzxjD/k4Pb9pUztvOr+Ffnm+P6HWEqrfj5Czc9p7Ya8tyF3Xdpm01hez//NWL2ltRKtWSyVl8XEReAv4eeBY41xjzYeAC4OYUt08tssnpAMcHJzinKp+6kmw6hyaSns4abXB8diZRvJ7F3g4PW9YU4s5wUVXoZl15bsSy47G0D46TJnNnO4E1IwqIW/EcXpjXOeRjYMxPc30xn3zLRgJBwzeenM1dOLO5yuLUWQBc0lTG1Vsq4z5/qnQ0V51tkulZlAFvN8Zca4z5iTFmGsAYEwT+JKWtU4uutX+MQNCwsSqfupIcpmaCC6qsDhc+k6gkN2tOsJgOBHmla5jm+tklvS9tKuPFY0PzLg1yfHCC2uKcmIVaTjI9bs8irDBvb9gQWH1pDu+6sI6Hd3eEht5COYs4PQuAf7rtfP7H9ZvjPq/UapFMsHgMGHIeiEi+iFwMYIxJviRXnRGcvaE3VeaHbrinmuQOr1Eozc3EMzEdkbx+rcfL5HSQ8+tnJ9Nd0lTKuD/AK13DcV/3+OD4nCEoxxs3lrOluiBU/BatsmC2Z7G3w0Nupiu0r8TH3rwBEeHe374ear87I42czMT1E0qtdskEi/uA8F3mx+1j6izU0jtGhktoLMsNVUCfSq2FMYaBsOrnktxMAkHDiG+2Inz2m/1sz+IN60oRma3QjvW6xwbG5yS3HdefW81jn7ic9Dirv+a7M8jNdNE3OsXeDg/b64pCNQ9VhW7+7A0N/OveLlr7x+y9t7N0SEipJCQTLMSEDWrbw0+6w95Z6kifl6byPDJcaaGcwKnMiBqbmsE/EwyN9ztBI3yp8r0dHirysyKmoBbnZnJebRG/OtAbM1cyPDGNd3Imbs8iGZUFbo4PjnO4x0tzfWSJ0Iff1IQ7w8XXfn2EgXmqt5VSkZIJFm12kjvD/vkE0JbqhqnUaOn1hoZl3BkuKvKzTmkYKnomkTNLaSgqWDTXF8/55v6OC2o53DM6Z18HsIagYO602YWoLHDz7NEBAkHD+fWRW6CW5WXx55eu5Zev9PDqiRHK4tRYKKUiJRMsPgRcApwAuoCLgTtS2SiVGt7JaU4M+yJWRK0ryaHjFHoWzjTZ8NlQAEP28YGx2WK8aDduX0NWehoP7+6Y81z7oNWW6IK8hagsyGLKXs7k/Pq5iw988PJ15LvTGZ6Ynje5rZSalUxR3kljzK3GmApjTKUx5t3GmJNL0Ti1uI70Wamn8L0W6oqzTyln0e+NrH52lvl2hqH2ts/ORIpWmJ3BDedW8+j+bnz+yDWbjg+OI3J6+zFX2snvtWW5oSAW8f45GfzFG9dZ7dZhKKWSkkydhVtEPiIi/0dE7nd+lqJxanGFZkJVRfYsekZ8C97lLnrv6uJcq2htyB6eCi/Gi+WWC+vwTs3w2Ks9EcePD4yzpjCbrPRTn6HkFOZFD0GFe/+la9nZUKxLgyuVpGSGoX6AtT7UtcDTQC3gnfcKdUZq6fWSm+mKSDjXleQQNNAznPweEDCbs3C+uWelu8jPSp/tWYQV48Vy8doSGktz+PGezojjxwcnaCw7vV3jnMK8WL0aR25WOj/98CVcuanitN5LqdUimWCx3hjzV8C4Meb7wA3AualtlkqFF9oG2VZTSFrY8tmnWmsxODZFYXZGROFcSZ5VxR2rGC+aiHDLhXW8eGyItv7Zmdntg/GnzSbr/PoittcV8eZzNBAotViSCRbOxPlhEdkGFAKNKWuRSonOoQle6/XOWbrCyQ0kSnKfHJ3kn377OuNT1gJ9AzF2mHOW/IhVjBfLO5prcaUJD71oJbpHJqbxTEyfdrCoLszm5x+5NGIhQKXU6UkmWHzH3s/ic8CjwCHgS8m8uL2keYuIHBWRu+Kcc4uIHBKRgyLyo7DjARHZb/88msz7qfieONQHMCdYVBdmk54m89Za+GeCfOjBl/jqr4/w3T8cA6yeRfSaStZigv6YxXixVBS4uXH7Gh549ji7jw/RPmRNmz2dGgulVGrMGyxEJA0YNcZ4jDG/N8ass2dFfTvRC4uIC/gm8FZgC3CbiGyJOmcD8FngUmPMVuCTYU/7jDE77J8bF/i5VJQnDvayqTKfhqhv7a40YU1R9ryrz/7tY4fZ2zHMurJcvvuHNoYn/AyOxetZTMUsxovnr2/aSl1JDh/54V52H7eCzOlMm1VKpca8wcKu1v7oKb72RcBRY0ybMcYPPAzcFHXOB4FvGmM89vvplNwU8Iz72X18iGu2xl491Vl9Npaf7z/B/33uOH9+6Vruu/0CxvwzfOvpNgZjDkNZiwnGK8aLpcCdwX23NzM6Oc3fPWYtNRZvYyOl1PJJZhjq1yJyp4jUiUiJ85PEdTVA+FSXLvtYuI3ARhF5VkReEJHrwp5zi8ge+/jbYr2BiNxhn7Onv78/iSatTr997SRBA9dsqYr5fH1JTsxg0dY/xl3/71UubCzms9efw6aqfG7cvob/+9wxPBP+UG2FozQ3k+mAiVuMF885VQXc8/bzmAka1hS6486gUkotn2TWePpz+58fCTtmgHUJrov1tTJ6MaB0rF343oQ1JfcPIrLNGDMM1BtjukVkHfCkiLxqjInY6swY8x3gOwA7d+48tU0ZVoEnDvZSXehmW01BzOc3VOTz0IudnBj2RQwd/WzfCaZmAnz9tmYy7IX7PnnVRn75Sg/GQFl+ZLAIL4Cbb9pqLG87v4b2wQmCp7i3hlIqtZKp4F4b4ydRoACrJ1EX9rgW6I5xzs+NMdPGmGNAC1bwwBjTbf+zDXgKOD+J91RRfP4Av3+9n6u3VMYdFtrVVArM3cHuudZBzq0tCtUtgFUV/Y7mWoA56yqV2MNS8xXjzecTV23gU1dvXPB1SqnUS6aC+z2xfpJ47d3ABhFZKyKZwK1Ys6nC/Qy40n6fMqxhqTYRKRaRrLDjl2LNwlIL9MzRASang/Pu9rapMp+S3EyeOzq7g9341Az7O4e5xA4k4T519Uau2lxBc0Nk78FZ+mO+Yjyl1NkpmWGoC8N+dwNvAfYC/zLfRcaYGRH5KPA44ALuN8YcFJG7gT3GmEft564RkUNAAPhLY8ygiFwCfFtEglgB7R5jjAaLU/DEwV7y3elcvHbuTd+RlibsairludZBjDGICC8eH2ImaLi0qWzO+VWFbr773gvnHHeGoRJNmVVKnX0SBgtjzMfCH4tIIdYSIAkZYx7D2mkv/Njnw343wKftn/BznkOrxBfFnnYPu9aVxtyiNNwlTaX8+ys9HBsYZ115Hs+3DpLpSuOChuRzD9WF2dzcXMs7Lqg93WYrpc4wycyGijaBnVdQZzbv5DTHBsY5N4n8wSV2D+JZO2/x7NEBmhuKyF7AlqOuNOErt2xn65qF5yuUUme2hD0LEfkFs7OY0rAK7B5JZaPU4jjcY633mEyyubE0hzWFbp5vHeBPzq3mUM8on7pKk81KKUsyOYsvh/0+A7QbY7pS1B4F7D4+xA+eb+dzN2ymosCd+II4DpwYAWBrnCmz4USEXU1lPPlaH8+3DWIMMZPbSqnVKZlg0QH0GGMmAUQkW0QajTHHU9qyVcg7Oc3f/0cLP3ihHYArNpZz82mM/x/oHqE8P4uK/OQCzqXrS/l/e7u4/5lj5GS62F6niWqllCWZnMVPgPCdcQL2MbWITnonufZrv+fBP7bznl0NAPSOLmyPiWgHT4yybU3iXoXDqbfY0+7horUloUI8pZRK5m6Qbq/tBID9u25cvMiePTpA98gkD7zvQu6+aRsF7nROnkawmJwOcLR/bEHFcdWF2ayzF/HTISilVLhkhqH6ReRGuy4CEbkJGEhwjVqgtv5xXGkSmpVUWeCe07MIBA0ffvAluuwVYtPS4FNXbeQtm+cW3L3W6yUQNAuemXTJ+lLaBsZD7VBKKUguWHwI+KGIfMN+3AUkU8GtFqC1f4z6kpxQPURVoZu+0amIc3pHJ3niUB9b1xRQXZjNi8cG+cmerpjBwklux1sPKp737GokNzOdLdULu04ptbIlszZUqzHmDVhTZrcaYy4xxhxNfdNWjudbB/nkw/sw8yyS13pynKby2X0cKvLd9EX1LLqHrR7Ff7vuHL773p28+ZwK9nZ4Yr7uwe5RCrMzktpTItzGynw+e/3miK1XlVIqmbWh/lZEiowxY8YYr71u0/9aisatFI8f7OVn+7vpH5uK+XwgaDg2ME5TeV7oWFVhFie9UwSDs4HghD385ASA5oZiTnqnODE8d+Oig90jbKspSGpPCaWUSiSZBPdb7SXDAbA3Kro+dU1aeZz9reNtMNTlmcAfCEYEi8oCN4GgYWB8NsA4QWFNkTUV1lkGfG/HMOGmA0Fe6/GyTSuplVKLJJlg4XJWgAWrzgLImud8FaUzFCxib13a1m/tPd1UMTsMVWkX4/WNRAaLktxMcjKtVNM5Vfm4M9LY2+6JeL3X+8bwB4JsPYVlwpVSKpZkEtwPAr8VkQfsx+8Hvp+6Jq0sxpjQ7KV4PYvW/jEA1pWFDUM5wWJ0knOxbvonPJGbE6W70jivtoh9HZHB4mC3Xbm9gBoLpZSaTzIJ7r8H/hewGSvJ/R9AQ4rbtWIMjPnxTQcA6PTEDxYluZkUh20m5PQswqfPnhj2hYagHM31xRzsHmXSfg+wktu5mS7WluailFKLIdkS3V6sKu6bsfazOJyyFq0wToAQiT8MFT0TCqAsL5M0IVSYZ4yhe9hHTVFOxHnN9UXMBA2v2lNlAfZ2eNhcXaAzmpRSiyZusBCRjSLyeRE5DHwD6ATEGHOlMeYb8a5TkZyhpy3VBfP2LMKT22ANMZXlZYV6FsMT00z4A3N7FvZ+E07e4rnWAV7pGuH6c6sX9XMopVa3+XoWr2H1Iv7UGHOZMebrWOtCqQVwgsWudaV0D/uYDgQjnh+e8DM47p8TLCCyMM+ZCVVbHFk3UZaXRX1JTqje4itPHKG60M27L65PxcdRSq1S8wWLm7GGn34nIv8sIm8BdFxjgTqHfJTlZbGhMo+ggZ7hyEK71hgzoRzhhXlOsIgehgJrKGpvxzBPtfTzUruHj715g+6BrZRaVHGDhTHm34wx7wLOAZ4CPgVUish9InLNErXvrNfpmaCuJJu64pzQ43DOTKjYPYus2WDhiayxCHd+fTH93ik+/+gB6ktyeOdO3dZUKbW4kpkNNW6M+aEx5k+AWmA/cFfKW7ZCdHomqCvOoa7EDhZDc4NFpiuN2uK5PYbKfDeeiWkmpwN0D/twZ6RRkjt3wV+nOK9zyMcnr9qgS4srpRbdgu4qxpghY8y3jTFvTlWDVpKZQJDu4UnqS3KoLnTjSpO5PYuT4zSW5eCKMXOpstDqRfTbS3rUFGXHXL7jnOp8sjNcrK/I46YdNan5MEqpVS2Zojx1inpGJgkEDXUl2aS70lhT5J4zfbatf4xNVfkxrw+vtbBqLGIvCpjhSuPeW3dQVxI76Cil1OnS8YoUcoacnHxFXXFOaJ0oAP9MkPahiZj5Cois4u4e9s2ZCRXumq1VbNZlxZVSKaLBIoWcIScnX1FXnENX2DBUx9AEgaCJORMKZoNF++AEA2P+BS83rpRSi0WDRQp1DvlwpQnVdu6hvjSHgTE/E/4ZAA73jAKRa0KFK8hOJys9LbT2U7xhKKWUSjUNFossfCOiTs8E1YVu0u3ZSc4wkrOw4M/3n6A8Pyvugn8iQlWhO7QEufYslFLLRYPFKfjEw/v4ws8PzDn+1SdaeNs3n2XGrtLuHJoI5SuAiOmzfaOT/K6ln3dcUBsKJrFU5rsZGvcD2rNQSi0fDRanYPexIXYf98w5/lKHh5e7RvjZ/m4AOj0+6kvCgkXxbLD46UtdBIKGW3bWzftezvTZNLGW/1BKqeWgwWKBAkFDn3cqYulwR8+Ideze3x5hdHKafu8UdSWzvYGyvEyyM1y0D03wyJ5OLl5bwtqy+ZcRr8y39pmqKnBrsZ1Satno3WeBBsamCAQNQ+P+iD0kjDH0jkyyqTKfziEfX33iCDA79ARWDqKuJJtfvtJD++AEt140f68CZnsTOgSllFpOGiwWqHt4tqiud2S2dzHqm2HCH+AdF9RyQUMx33/+OMCcZTzqinPo906R707nrdsSLyNeYU+frZmnxkIppVJNg8UChQeInvDfR60gUl3k5s5rNuFMigofhrIeW8HjbTtqkloZ1qm10JlQSqnlpMFigbojgsVsL8NZery6MJtdTaVctr6MvKx0yvOyIq53chTvujDxEBTM9igaSucuNKiUUktF14ZaoN4RHxkuYTpgInsWI06wsHoC/3jrDro8vjkL/71zZy2bqvLZVlOY1PvVFGXz0AffQHND0SJ9AqWUWjgNFgvUMzJJbXEOngl/ZM9ixEeaQIU9e6ksL4uyqF4FQE5mOm9YV7qg99zVtLDzlVJqsaV0GEpErhORFhE5KiIx98AQkVtE5JCIHBSRH4Udf6+IvG7/vDeV7VyInpFJqgrcVBW45+QvKvLd8xbYKaXU2SplPQsRcQHfBK4GuoDdIvKoMeZQ2DkbgM8ClxpjPCJSYR8vAb4A7AQM8JJ97dxKuCXWOzLJxetKyM50RQxD9Y5MatGcUmrFSuXX4IuAo8aYNmOMH3gYuCnqnA8C33SCgDHmpH38WuDX9mZLHuDXwHUpbGtSAkFD7+gk1YVuqgrdEcGie8QXc8tTpZRaCVIZLGqAzrDHXfaxcBuBjSLyrIi8ICLXLeDaJecU5FUVZlNd4A4V5jkFeVUFOr1VKbUypTLBHWvLNhP1OB3YALwJa3/vP4jItiSvRUTuAO4AqK+vP522JsXpSawpdJNt10j0jkxSnJPJhD+gPQul1IqVyp5FFxBeTFALdMc45+fGmGljzDGgBSt4JHMtxpjvGGN2GmN2lpeXL2rjY+mxq7erCt2hKbI9I5OhgjzNWSilVqpUBovdwAYRWSsimcCtwKNR5/wMuBJARMqwhqXagMeBa0SkWESKgWvsY8tqtmeRHRYsfBEFeUoptRKlbBjKGDMjIh/Fusm7gPuNMQdF5CzilxQAAAtTSURBVG5gjzHmUWaDwiEgAPylMWYQQET+BivgANxtjBlKVVuT1TPiIys9jaKcjNBSHT0jk0xOW/tXVGvPQim1QqW0KM8Y8xjwWNSxz4f9boBP2z/R194P3J/K9i1Uz4g1E0pEyM50UZSTQe/IJFPTgYiCPKWUWmm0gnsBrGAxO9RUVeCmZ8SHbzqgBXlKqRVN724L0Gv3LBzVdq2FFuQppVY6DRZJCgQNfaOTVIdNj60uyqZnZFIL8pRSK54GiyQNjE0xYxfkOZzCvC6PTwvylFIrmgaLJIUX5Dmq7Q2J/DNB7VkopVY0DRZJCi/Ic4TnLzRnoZRayTRYJGl2c6OwYaiIwKHDUEqplUuDRZJ6RyfJSk+jOCcjdCxeL0MppVYaDRZJ6h72hQryHDmZ6RRmZ2hBnlJqxdOivCT1RhXkOartFWi1IE8ptZLpHS5JTs8i2tY1hWxdU7AMLVJKqaWjPYsk+GeC9IxOUluSM+e5L9187tyNNpRSaoXRYJGE7mEfxkBd8dxhKB1+UkqtBnqnS0KnZwKA+hg9C6WUWg00WCShY8gKFnUaLJRSq5QGiyR0DvnIcAmVBVpLoZRanTRYJKHTM0FNUTauNEl8slJKrUAaLJLQNTShQ1BKqVVNg0USOj0+aos1WCilVi8NFgmMTc0wNO7XmVBKqVVNg0UCnaGZULqqrFJq9dJgkUAoWOgwlFJqFdNgkUCnx9r0SBPcSqnVTINFAp1DE+RmuiL2sVBKqdVGg0UCXR5r2mz4PhZKKbXaaLBIoENrLJRSSoPFfIwxdA75NLmtlFr1NFjMY3Dcj286oNNmlVKrngaLeei0WaWUsmiwmIdOm1VKKYsGi3k4PYvaGDvkKaXUaqLBYh6dQxOU5WWSm6W7zyqlVjcNFvPo9EzoarNKKYUGi3l1Dvk0X6GUUmiwiMs7OU2nZ4Km8tzlbopSSi07DRZxvNw5gjFwfn3xcjdFKaWWXUqDhYhcJyItInJURO6K8fz7RKRfRPbbPx8Iey4QdvzRVLYzlr0dHgB21BUt9VsrpdQZJ2XTfETEBXwTuBroAnaLyKPGmENRp/7YGPPRGC/hM8bsSFX7Etnb4WFDRR6F2brarFJKpbJncRFw1BjTZozxAw8DN6Xw/RaNMYZ9HcM06xCUUkoBqQ0WNUBn2OMu+1i0m0XkFRH5qYjUhR13i8geEXlBRN4W6w1E5A77nD39/f2L1vC2gXFGfNM0N+gQlFJKQWqDRawNIEzU418AjcaY84DfAN8Pe67eGLMTeDfwjyLSNOfFjPmOMWanMWZneXn5YrWbve1WvkJ7FkopZUllsOgCwnsKtUB3+AnGmEFjzJT98J+BC8Ke67b/2QY8BZyfwrZG2NsxTL47nabyvKV6S6WUOqOlMljsBjaIyFoRyQRuBSJmNYlIddjDG4HD9vFiEcmyfy8DLgWiE+Mps6/Dw466ItLSdHc8pZSCFM6GMsbMiMhHgccBF3C/MeagiNwN7DHGPAp8XERuBGaAIeB99uWbgW+LSBAroN0TYxZVSngnp2np83Lt1qqleDullDorpHSFPGPMY8BjUcc+H/b7Z4HPxrjuOeDcVLYtHqcYr7lB8xVKKeXQCu4oWoynlFJzabCIsk+L8ZRSag4NFmEO94yyp92jU2aVUiqK7uoDTE4H+ObvjnLfU60UZmfwn99Qv9xNUkqpM8qqDxadQxO874EXae0f5+3NNfzVDVsozs1c7mYppdQZZdUHi8oCNw2luXzhT7fyxo2LVwWulFIryaoPFpnpadz/vguXuxlKKXVG0wS3UkqphDRYKKWUSkiDhVJKqYQ0WCillEpIg4VSSqmENFgopZRKSIOFUkqphDRYKKWUSkiMid4W++wkIv1A+2m8RBkwsEjNOVusxs8Mq/Nzr8bPDKvzcy/0MzcYYxIuX7FigsXpEpE9xpidy92OpbQaPzOszs+9Gj8zrM7PnarPrMNQSimlEtJgoZRSKiENFrO+s9wNWAar8TPD6vzcq/Ezw+r83Cn5zJqzUEoplZD2LJRSSiWkwUIppVRCqz5YiMh1ItIiIkdF5K7lbk+qiEidiPxORA6LyEER+YR9vEREfi0ir9v/LF7uti42EXGJyD4R+aX9eK2I/NH+zD8WkRW3j66IFInIT0XkNftvvmul/61F5FP2f9sHROQhEXGvxL+1iNwvIidF5EDYsZh/W7H8k31/e0VEmk/1fVd1sBARF/BN4K3AFuA2EdmyvK1KmRngM8aYzcAbgI/Yn/Uu4LfGmA3Ab+3HK80ngMNhj78EfM3+zB7gvyxLq1LrXuA/jDHnANuxPv+K/VuLSA3wcWCnMWYb4AJuZWX+rf8vcF3UsXh/27cCG+yfO4D7TvVNV3WwAC4Cjhpj2owxfuBh4KZlblNKGGN6jDF77d+9WDePGqzP+337tO8Db1ueFqaGiNQCNwDftR8L8Gbgp/YpK/EzFwBvBL4HYIzxG2OGWeF/a6xtorNFJB3I+f/t3UuoVVUcx/HvDx9hSkhGYpmZJA2C0ogQayDWKCQHFRJGIjVxUg16UJMIahBEiBhBD4NAgigrR1FY9KCyEo2oZip5yyeh0oMw+zVY69bmcm/bK+d4bN/fBw5n7//Z3LM2/3PPf6+191kb2E8Hc237I+DnEeGxcrsSeMXF58BMSXNO530nerG4GNjXWB+qsU6TNB9YDGwHZtveD6WgABcOrmV9sR54CPirrs8Cjtr+s653MecLgMPAy3X47UVJ0+lwrm3/CDwN/EApEseAHXQ/18PGym3PvuMmerHQKLFOX0ssaQbwBnC/7eODbk8/SVoBHLK9oxkeZdOu5XwycA3wnO3FwK90aMhpNHWMfiVwGXARMJ0yBDNS13Ldpmef94leLIaASxrrc4GfBtSWvpM0hVIoNtveUsMHh7ul9fnQoNrXB9cDt0jaSxliXE7pacysQxXQzZwPAUO2t9f11ynFo8u5vgnYY/uw7RPAFmAp3c/1sLFy27PvuIleLL4EFtYrJqZSTohtHXCb+qKO1b8EfG/7mcZLW4E1dXkN8PaZblu/2H7E9lzb8ym5fd/2auAD4La6Waf2GcD2AWCfpCtq6EbgOzqca8rw0xJJ59bP+vA+dzrXDWPlditwV70qaglwbHi4arwm/C+4Jd1MOdqcBGyy/eSAm9QXkm4APga+4d/x+0cp5y1eA+ZR/uFutz3y5Nn/nqRlwAO2V0haQOlpnA/sBO60/ccg29drkhZRTupPBXYDaykHh53NtaTHgVWUK/92AvdQxuc7lWtJrwLLKFORHwQeA95ilNzWwrmRcvXUb8Ba21+d1vtO9GIRERHtJvowVEREnIIUi4iIaJViERERrVIsIiKiVYpFRES0SrGIGAdJJyXtajx69stoSfObM4lGnE0mt28SEQ2/21406EZEnGnpWUT0gKS9kp6S9EV9XF7jl0raVu8lsE3SvBqfLelNSV/Xx9L6pyZJeqHel+FdSdMGtlMRDSkWEeMzbcQw1KrGa8dtX0f5xez6GttImSL6KmAzsKHGNwAf2r6aMm/TtzW+EHjW9pXAUeDWPu9PxCnJL7gjxkHSL7ZnjBLfCyy3vbtO2HjA9ixJR4A5tk/U+H7bF0g6DMxtTj1Rp45/r97ABkkPA1NsP9H/PYv4b+lZRPSOx1gea5vRNOctOknOK8ZZIsUiondWNZ4/q8ufUma8BVgNfFKXtwHr4J97hJ93phoZcTpy1BIxPtMk7Wqsv2N7+PLZcyRtpxyE3VFj9wKbJD1IuXvd2hq/D3he0t2UHsQ6yh3eIs5KOWcR0QP1nMW1to8Mui0R/ZBhqIiIaJWeRUREtErPIiIiWqVYREREqxSLiIholWIRERGtUiwiIqLV3/xAn0VEtsUJAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "# from keras import regularizers kernel_regularizer=regularizers.l2(0.01), \n",
    "from keras.optimizers import Adam\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(32, input_shape=(8,)))\n",
    "network.add(layers.Dense(32, activation=\"relu\"))\n",
    "network.add(layers.Dense(64, activation=\"relu\"))\n",
    "network.add(layers.Dropout(.5))\n",
    "network.add(layers.Dense(64, activation=\"relu\"))\n",
    "network.add(layers.Dense(32, activation=\"sigmoid\"))\n",
    "network.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Adam = Adam(lr=0.05)\n",
    "network.compile(optimizer='Adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['acc'])\n",
    "\n",
    "network.summary()\n",
    "\n",
    "history = network.fit(x_train, y_train,\n",
    "                      epochs=100, verbose=1, batch_size=3)\n",
    "\n",
    "loss_and_metrics = network.evaluate(x_test, y_test)\n",
    "print('loss and metrics', loss_and_metrics)\n",
    "\n",
    "# print('prediction: ', network.predict(test_data))\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "all_labels = dataDF.action.values\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "all_labels = encoder.fit_transform(all_labels)\n",
    "    \n",
    "# create an array of shape 30706, 9 = number of records by the features\n",
    "all_data = np.array([[0 for x in range(8)] for y in range(len(dataDF))])\n",
    "for i in range(len(dataDF)):\n",
    "    all_data[i] = [dataDF.delta.values[i],\n",
    "                       dataDF.theta.values[i],\n",
    "                       dataDF.alphaLow.values[i],\n",
    "                       dataDF.alphaHigh.values[i],\n",
    "                       dataDF.betaLow.values[i],\n",
    "                       dataDF.betaHigh.values[i],\n",
    "                       dataDF.gammaLow.values[i],\n",
    "                       dataDF.gammaMid.values[i]]\n",
    "    \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "stan_scaler = StandardScaler()\n",
    "\n",
    "all_data = scaler.fit_transform(all_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_60 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_63 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 432 samples, validate on 108 samples\n",
      "Epoch 1/40\n",
      "432/432 [==============================] - 1s 3ms/step - loss: 0.7044 - acc: 0.4792 - val_loss: 0.7988 - val_acc: 0.0000e+00\n",
      "Epoch 2/40\n",
      "432/432 [==============================] - 0s 543us/step - loss: 0.6676 - acc: 0.6250 - val_loss: 0.9477 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "432/432 [==============================] - 0s 511us/step - loss: 0.6632 - acc: 0.6250 - val_loss: 0.9390 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "432/432 [==============================] - 0s 540us/step - loss: 0.6598 - acc: 0.6250 - val_loss: 0.9509 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "432/432 [==============================] - 0s 554us/step - loss: 0.6571 - acc: 0.6250 - val_loss: 0.9975 - val_acc: 0.0000e+00\n",
      "Epoch 6/40\n",
      "432/432 [==============================] - 0s 587us/step - loss: 0.6545 - acc: 0.6250 - val_loss: 1.0005 - val_acc: 0.0000e+00\n",
      "Epoch 7/40\n",
      "432/432 [==============================] - 0s 525us/step - loss: 0.6524 - acc: 0.6250 - val_loss: 0.9944 - val_acc: 0.0000e+00\n",
      "Epoch 8/40\n",
      "432/432 [==============================] - 0s 527us/step - loss: 0.6477 - acc: 0.6250 - val_loss: 0.9494 - val_acc: 0.0000e+00\n",
      "Epoch 9/40\n",
      "432/432 [==============================] - 0s 497us/step - loss: 0.6473 - acc: 0.6250 - val_loss: 0.9794 - val_acc: 0.0000e+00\n",
      "Epoch 10/40\n",
      "432/432 [==============================] - 0s 522us/step - loss: 0.6443 - acc: 0.6296 - val_loss: 0.9686 - val_acc: 0.0278\n",
      "Epoch 11/40\n",
      "432/432 [==============================] - 0s 540us/step - loss: 0.6418 - acc: 0.6273 - val_loss: 0.9430 - val_acc: 0.1111\n",
      "Epoch 12/40\n",
      "432/432 [==============================] - 0s 530us/step - loss: 0.6429 - acc: 0.6296 - val_loss: 0.9623 - val_acc: 0.0833\n",
      "Epoch 13/40\n",
      "432/432 [==============================] - 0s 549us/step - loss: 0.6400 - acc: 0.6528 - val_loss: 1.0263 - val_acc: 0.0833\n",
      "Epoch 14/40\n",
      "432/432 [==============================] - 0s 537us/step - loss: 0.6375 - acc: 0.6551 - val_loss: 0.9476 - val_acc: 0.1296\n",
      "Epoch 15/40\n",
      "432/432 [==============================] - 0s 531us/step - loss: 0.6384 - acc: 0.6574 - val_loss: 0.9660 - val_acc: 0.1296\n",
      "Epoch 16/40\n",
      "432/432 [==============================] - 0s 534us/step - loss: 0.6392 - acc: 0.6528 - val_loss: 0.9768 - val_acc: 0.1019\n",
      "Epoch 17/40\n",
      "432/432 [==============================] - 0s 535us/step - loss: 0.6361 - acc: 0.6574 - val_loss: 1.0210 - val_acc: 0.0833\n",
      "Epoch 18/40\n",
      "432/432 [==============================] - 0s 489us/step - loss: 0.6354 - acc: 0.6458 - val_loss: 0.9998 - val_acc: 0.1019\n",
      "Epoch 19/40\n",
      "432/432 [==============================] - 0s 499us/step - loss: 0.6345 - acc: 0.6551 - val_loss: 1.0044 - val_acc: 0.1204\n",
      "Epoch 20/40\n",
      "432/432 [==============================] - 0s 524us/step - loss: 0.6335 - acc: 0.6528 - val_loss: 1.0587 - val_acc: 0.0926\n",
      "Epoch 21/40\n",
      "432/432 [==============================] - 0s 543us/step - loss: 0.6341 - acc: 0.6574 - val_loss: 1.0114 - val_acc: 0.1019\n",
      "Epoch 22/40\n",
      "432/432 [==============================] - 0s 529us/step - loss: 0.6325 - acc: 0.6551 - val_loss: 0.9152 - val_acc: 0.1574\n",
      "Epoch 23/40\n",
      "432/432 [==============================] - 0s 529us/step - loss: 0.6321 - acc: 0.6481 - val_loss: 1.0118 - val_acc: 0.1204\n",
      "Epoch 24/40\n",
      "432/432 [==============================] - 0s 502us/step - loss: 0.6308 - acc: 0.6366 - val_loss: 0.9979 - val_acc: 0.1296\n",
      "Epoch 25/40\n",
      "432/432 [==============================] - 0s 553us/step - loss: 0.6266 - acc: 0.6574 - val_loss: 1.0789 - val_acc: 0.1019\n",
      "Epoch 26/40\n",
      "432/432 [==============================] - 0s 547us/step - loss: 0.6294 - acc: 0.6620 - val_loss: 1.0269 - val_acc: 0.1204\n",
      "Epoch 27/40\n",
      "432/432 [==============================] - 0s 538us/step - loss: 0.6266 - acc: 0.6620 - val_loss: 0.9926 - val_acc: 0.1204\n",
      "Epoch 28/40\n",
      "432/432 [==============================] - 0s 503us/step - loss: 0.6253 - acc: 0.6597 - val_loss: 0.9973 - val_acc: 0.1296\n",
      "Epoch 29/40\n",
      "432/432 [==============================] - 0s 553us/step - loss: 0.6259 - acc: 0.6528 - val_loss: 0.9519 - val_acc: 0.1759\n",
      "Epoch 30/40\n",
      "432/432 [==============================] - 0s 528us/step - loss: 0.6252 - acc: 0.6551 - val_loss: 1.0119 - val_acc: 0.1204\n",
      "Epoch 31/40\n",
      "432/432 [==============================] - 0s 523us/step - loss: 0.6232 - acc: 0.6736 - val_loss: 1.0912 - val_acc: 0.1111\n",
      "Epoch 32/40\n",
      "432/432 [==============================] - 0s 494us/step - loss: 0.6241 - acc: 0.6551 - val_loss: 1.0125 - val_acc: 0.1204\n",
      "Epoch 33/40\n",
      "432/432 [==============================] - 0s 523us/step - loss: 0.6219 - acc: 0.6597 - val_loss: 0.9828 - val_acc: 0.1481\n",
      "Epoch 34/40\n",
      "432/432 [==============================] - 0s 515us/step - loss: 0.6200 - acc: 0.6667 - val_loss: 0.9544 - val_acc: 0.1574\n",
      "Epoch 35/40\n",
      "432/432 [==============================] - 0s 558us/step - loss: 0.6212 - acc: 0.6644 - val_loss: 1.0336 - val_acc: 0.1204\n",
      "Epoch 36/40\n",
      "432/432 [==============================] - 0s 538us/step - loss: 0.6186 - acc: 0.6597 - val_loss: 0.9676 - val_acc: 0.1944\n",
      "Epoch 37/40\n",
      "432/432 [==============================] - 0s 579us/step - loss: 0.6198 - acc: 0.6574 - val_loss: 0.9088 - val_acc: 0.2315\n",
      "Epoch 38/40\n",
      "432/432 [==============================] - 0s 560us/step - loss: 0.6156 - acc: 0.6690 - val_loss: 1.0623 - val_acc: 0.1389\n",
      "Epoch 39/40\n",
      "432/432 [==============================] - 0s 614us/step - loss: 0.6168 - acc: 0.6597 - val_loss: 1.0142 - val_acc: 0.1389\n",
      "Epoch 40/40\n",
      "432/432 [==============================] - 0s 516us/step - loss: 0.6156 - acc: 0.6620 - val_loss: 0.9519 - val_acc: 0.2222\n",
      "60/60 [==============================] - 0s 39us/step\n",
      "Average accuracy of model on the dev set =  0.6\n",
      "Training on fold 2/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_66 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_69 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 432 samples, validate on 108 samples\n",
      "Epoch 1/40\n",
      "432/432 [==============================] - 1s 3ms/step - loss: 0.7121 - acc: 0.4537 - val_loss: 0.7600 - val_acc: 0.0000e+00\n",
      "Epoch 2/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - 0s 592us/step - loss: 0.6698 - acc: 0.6250 - val_loss: 0.8929 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "432/432 [==============================] - 0s 508us/step - loss: 0.6618 - acc: 0.6250 - val_loss: 0.9470 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "432/432 [==============================] - 0s 546us/step - loss: 0.6586 - acc: 0.6250 - val_loss: 0.9571 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "432/432 [==============================] - 0s 553us/step - loss: 0.6562 - acc: 0.6250 - val_loss: 0.9974 - val_acc: 0.0000e+00\n",
      "Epoch 6/40\n",
      "432/432 [==============================] - 0s 548us/step - loss: 0.6496 - acc: 0.6250 - val_loss: 0.9812 - val_acc: 0.0000e+00\n",
      "Epoch 7/40\n",
      "432/432 [==============================] - 0s 509us/step - loss: 0.6460 - acc: 0.6227 - val_loss: 0.9442 - val_acc: 0.0093\n",
      "Epoch 8/40\n",
      "432/432 [==============================] - 0s 543us/step - loss: 0.6430 - acc: 0.6204 - val_loss: 0.9332 - val_acc: 0.2037\n",
      "Epoch 9/40\n",
      "432/432 [==============================] - 0s 556us/step - loss: 0.6410 - acc: 0.6273 - val_loss: 0.8551 - val_acc: 0.2222\n",
      "Epoch 10/40\n",
      "432/432 [==============================] - 0s 512us/step - loss: 0.6392 - acc: 0.6435 - val_loss: 0.9596 - val_acc: 0.2037\n",
      "Epoch 11/40\n",
      "432/432 [==============================] - 0s 561us/step - loss: 0.6363 - acc: 0.6505 - val_loss: 0.9829 - val_acc: 0.2037\n",
      "Epoch 12/40\n",
      "432/432 [==============================] - 0s 565us/step - loss: 0.6356 - acc: 0.6412 - val_loss: 0.9889 - val_acc: 0.2037\n",
      "Epoch 13/40\n",
      "432/432 [==============================] - 0s 540us/step - loss: 0.6332 - acc: 0.6528 - val_loss: 1.0280 - val_acc: 0.1204\n",
      "Epoch 14/40\n",
      "432/432 [==============================] - 0s 572us/step - loss: 0.6336 - acc: 0.6551 - val_loss: 0.9690 - val_acc: 0.2037\n",
      "Epoch 15/40\n",
      "432/432 [==============================] - 0s 526us/step - loss: 0.6328 - acc: 0.6481 - val_loss: 0.9346 - val_acc: 0.2130\n",
      "Epoch 16/40\n",
      "432/432 [==============================] - 0s 557us/step - loss: 0.6315 - acc: 0.6481 - val_loss: 1.0350 - val_acc: 0.1204\n",
      "Epoch 17/40\n",
      "432/432 [==============================] - 0s 568us/step - loss: 0.6308 - acc: 0.6597 - val_loss: 0.9949 - val_acc: 0.1204\n",
      "Epoch 18/40\n",
      "432/432 [==============================] - 0s 560us/step - loss: 0.6283 - acc: 0.6597 - val_loss: 0.9034 - val_acc: 0.2500\n",
      "Epoch 19/40\n",
      "432/432 [==============================] - 0s 552us/step - loss: 0.6287 - acc: 0.6458 - val_loss: 0.9639 - val_acc: 0.1204\n",
      "Epoch 20/40\n",
      "432/432 [==============================] - 0s 564us/step - loss: 0.6265 - acc: 0.6597 - val_loss: 0.9203 - val_acc: 0.1296\n",
      "Epoch 21/40\n",
      "432/432 [==============================] - 0s 551us/step - loss: 0.6275 - acc: 0.6597 - val_loss: 0.9074 - val_acc: 0.1667\n",
      "Epoch 22/40\n",
      "432/432 [==============================] - 0s 575us/step - loss: 0.6251 - acc: 0.6620 - val_loss: 0.9779 - val_acc: 0.1204\n",
      "Epoch 23/40\n",
      "432/432 [==============================] - 0s 515us/step - loss: 0.6248 - acc: 0.6597 - val_loss: 1.0049 - val_acc: 0.1204\n",
      "Epoch 24/40\n",
      "432/432 [==============================] - 0s 536us/step - loss: 0.6244 - acc: 0.6528 - val_loss: 1.0225 - val_acc: 0.1204\n",
      "Epoch 25/40\n",
      "432/432 [==============================] - 0s 568us/step - loss: 0.6236 - acc: 0.6574 - val_loss: 0.9631 - val_acc: 0.1667\n",
      "Epoch 26/40\n",
      "432/432 [==============================] - 0s 569us/step - loss: 0.6238 - acc: 0.6574 - val_loss: 0.9590 - val_acc: 0.1667\n",
      "Epoch 27/40\n",
      "432/432 [==============================] - 0s 531us/step - loss: 0.6219 - acc: 0.6528 - val_loss: 1.0114 - val_acc: 0.1481\n",
      "Epoch 28/40\n",
      "432/432 [==============================] - 0s 571us/step - loss: 0.6220 - acc: 0.6574 - val_loss: 1.0211 - val_acc: 0.1481\n",
      "Epoch 29/40\n",
      "432/432 [==============================] - 0s 526us/step - loss: 0.6209 - acc: 0.6551 - val_loss: 0.9067 - val_acc: 0.1944\n",
      "Epoch 30/40\n",
      "432/432 [==============================] - 0s 601us/step - loss: 0.6194 - acc: 0.6667 - val_loss: 1.0202 - val_acc: 0.1574\n",
      "Epoch 31/40\n",
      "432/432 [==============================] - 0s 552us/step - loss: 0.6190 - acc: 0.6620 - val_loss: 1.0242 - val_acc: 0.1481\n",
      "Epoch 32/40\n",
      "432/432 [==============================] - 0s 553us/step - loss: 0.6204 - acc: 0.6574 - val_loss: 1.0400 - val_acc: 0.1389\n",
      "Epoch 33/40\n",
      "432/432 [==============================] - 0s 628us/step - loss: 0.6186 - acc: 0.6574 - val_loss: 0.9966 - val_acc: 0.1667\n",
      "Epoch 34/40\n",
      "432/432 [==============================] - 0s 609us/step - loss: 0.6177 - acc: 0.6620 - val_loss: 0.9922 - val_acc: 0.1574\n",
      "Epoch 35/40\n",
      "432/432 [==============================] - 0s 591us/step - loss: 0.6177 - acc: 0.6597 - val_loss: 0.9546 - val_acc: 0.1667\n",
      "Epoch 36/40\n",
      "432/432 [==============================] - 0s 576us/step - loss: 0.6166 - acc: 0.6597 - val_loss: 0.9796 - val_acc: 0.1759\n",
      "Epoch 37/40\n",
      "432/432 [==============================] - 0s 577us/step - loss: 0.6164 - acc: 0.6551 - val_loss: 0.9332 - val_acc: 0.2037\n",
      "Epoch 38/40\n",
      "432/432 [==============================] - 0s 630us/step - loss: 0.6163 - acc: 0.6736 - val_loss: 0.9634 - val_acc: 0.1852\n",
      "Epoch 39/40\n",
      "432/432 [==============================] - 0s 580us/step - loss: 0.6142 - acc: 0.6597 - val_loss: 1.0389 - val_acc: 0.1667\n",
      "Epoch 40/40\n",
      "432/432 [==============================] - 0s 580us/step - loss: 0.6141 - acc: 0.6644 - val_loss: 0.9723 - val_acc: 0.1944\n",
      "60/60 [==============================] - 0s 37us/step\n",
      "Average accuracy of model on the dev set =  0.5750000009934108\n",
      "Training on fold 3/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_72 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_75 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 432 samples, validate on 108 samples\n",
      "Epoch 1/40\n",
      "432/432 [==============================] - 1s 3ms/step - loss: 0.6664 - acc: 0.6250 - val_loss: 1.0430 - val_acc: 0.0000e+00\n",
      "Epoch 2/40\n",
      "432/432 [==============================] - 0s 643us/step - loss: 0.6593 - acc: 0.6250 - val_loss: 0.9874 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "432/432 [==============================] - 0s 625us/step - loss: 0.6566 - acc: 0.6250 - val_loss: 0.9501 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "432/432 [==============================] - 0s 621us/step - loss: 0.6546 - acc: 0.6227 - val_loss: 0.9608 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "432/432 [==============================] - 0s 656us/step - loss: 0.6527 - acc: 0.6227 - val_loss: 0.9942 - val_acc: 0.0000e+00\n",
      "Epoch 6/40\n",
      "432/432 [==============================] - 0s 613us/step - loss: 0.6496 - acc: 0.6042 - val_loss: 1.0399 - val_acc: 0.0000e+00\n",
      "Epoch 7/40\n",
      "432/432 [==============================] - 0s 604us/step - loss: 0.6511 - acc: 0.6204 - val_loss: 0.9538 - val_acc: 0.0185\n",
      "Epoch 8/40\n",
      "432/432 [==============================] - 0s 607us/step - loss: 0.6462 - acc: 0.6319 - val_loss: 1.0259 - val_acc: 0.0000e+00\n",
      "Epoch 9/40\n",
      "432/432 [==============================] - 0s 591us/step - loss: 0.6448 - acc: 0.6204 - val_loss: 0.9496 - val_acc: 0.1574\n",
      "Epoch 10/40\n",
      "432/432 [==============================] - 0s 642us/step - loss: 0.6424 - acc: 0.6250 - val_loss: 0.9371 - val_acc: 0.1759\n",
      "Epoch 11/40\n",
      "432/432 [==============================] - 0s 597us/step - loss: 0.6406 - acc: 0.6412 - val_loss: 0.9633 - val_acc: 0.1667\n",
      "Epoch 12/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - 0s 587us/step - loss: 0.6382 - acc: 0.6389 - val_loss: 0.9558 - val_acc: 0.1852\n",
      "Epoch 13/40\n",
      "432/432 [==============================] - 0s 502us/step - loss: 0.6370 - acc: 0.6528 - val_loss: 0.9857 - val_acc: 0.1574\n",
      "Epoch 14/40\n",
      "432/432 [==============================] - 0s 500us/step - loss: 0.6355 - acc: 0.6505 - val_loss: 0.9512 - val_acc: 0.1852\n",
      "Epoch 15/40\n",
      "432/432 [==============================] - 0s 487us/step - loss: 0.6315 - acc: 0.6620 - val_loss: 0.9149 - val_acc: 0.1852\n",
      "Epoch 16/40\n",
      "432/432 [==============================] - 0s 493us/step - loss: 0.6319 - acc: 0.6505 - val_loss: 0.9845 - val_acc: 0.1574\n",
      "Epoch 17/40\n",
      "432/432 [==============================] - 0s 526us/step - loss: 0.6298 - acc: 0.6412 - val_loss: 1.0009 - val_acc: 0.1574\n",
      "Epoch 18/40\n",
      "432/432 [==============================] - 0s 571us/step - loss: 0.6299 - acc: 0.6551 - val_loss: 0.9543 - val_acc: 0.1759\n",
      "Epoch 19/40\n",
      "432/432 [==============================] - 0s 563us/step - loss: 0.6307 - acc: 0.6528 - val_loss: 0.9409 - val_acc: 0.1574\n",
      "Epoch 20/40\n",
      "432/432 [==============================] - 0s 557us/step - loss: 0.6251 - acc: 0.6620 - val_loss: 0.9354 - val_acc: 0.1759\n",
      "Epoch 21/40\n",
      "432/432 [==============================] - 0s 557us/step - loss: 0.6263 - acc: 0.6620 - val_loss: 0.9300 - val_acc: 0.1759\n",
      "Epoch 22/40\n",
      "432/432 [==============================] - 0s 535us/step - loss: 0.6227 - acc: 0.6620 - val_loss: 1.0555 - val_acc: 0.1574\n",
      "Epoch 23/40\n",
      "432/432 [==============================] - 0s 566us/step - loss: 0.6214 - acc: 0.6574 - val_loss: 0.9495 - val_acc: 0.1759\n",
      "Epoch 24/40\n",
      "432/432 [==============================] - 0s 512us/step - loss: 0.6203 - acc: 0.6528 - val_loss: 0.9972 - val_acc: 0.1574\n",
      "Epoch 25/40\n",
      "432/432 [==============================] - 0s 575us/step - loss: 0.6202 - acc: 0.6667 - val_loss: 0.9654 - val_acc: 0.1759\n",
      "Epoch 26/40\n",
      "432/432 [==============================] - 0s 572us/step - loss: 0.6173 - acc: 0.6620 - val_loss: 0.9784 - val_acc: 0.1759\n",
      "Epoch 27/40\n",
      "432/432 [==============================] - 0s 560us/step - loss: 0.6173 - acc: 0.6667 - val_loss: 0.9494 - val_acc: 0.2407\n",
      "Epoch 28/40\n",
      "432/432 [==============================] - 0s 602us/step - loss: 0.6175 - acc: 0.6667 - val_loss: 0.9833 - val_acc: 0.1944\n",
      "Epoch 29/40\n",
      "432/432 [==============================] - 0s 610us/step - loss: 0.6143 - acc: 0.6713 - val_loss: 0.9365 - val_acc: 0.2685\n",
      "Epoch 30/40\n",
      "432/432 [==============================] - 0s 600us/step - loss: 0.6131 - acc: 0.6782 - val_loss: 0.9913 - val_acc: 0.2130\n",
      "Epoch 31/40\n",
      "432/432 [==============================] - 0s 563us/step - loss: 0.6156 - acc: 0.6667 - val_loss: 0.9668 - val_acc: 0.2222\n",
      "Epoch 32/40\n",
      "432/432 [==============================] - 0s 562us/step - loss: 0.6121 - acc: 0.6759 - val_loss: 0.9965 - val_acc: 0.2130\n",
      "Epoch 33/40\n",
      "432/432 [==============================] - 0s 569us/step - loss: 0.6122 - acc: 0.6759 - val_loss: 0.9995 - val_acc: 0.1759\n",
      "Epoch 34/40\n",
      "432/432 [==============================] - 0s 616us/step - loss: 0.6083 - acc: 0.6736 - val_loss: 0.9689 - val_acc: 0.2222\n",
      "Epoch 35/40\n",
      "432/432 [==============================] - 0s 594us/step - loss: 0.6086 - acc: 0.6690 - val_loss: 0.9422 - val_acc: 0.2685\n",
      "Epoch 36/40\n",
      "432/432 [==============================] - 0s 572us/step - loss: 0.6069 - acc: 0.6690 - val_loss: 0.9703 - val_acc: 0.2407\n",
      "Epoch 37/40\n",
      "432/432 [==============================] - 0s 574us/step - loss: 0.6062 - acc: 0.6713 - val_loss: 0.9630 - val_acc: 0.2685\n",
      "Epoch 38/40\n",
      "432/432 [==============================] - 0s 614us/step - loss: 0.6026 - acc: 0.6944 - val_loss: 0.9938 - val_acc: 0.2407\n",
      "Epoch 39/40\n",
      "432/432 [==============================] - 0s 610us/step - loss: 0.6045 - acc: 0.6875 - val_loss: 1.0680 - val_acc: 0.1759\n",
      "Epoch 40/40\n",
      "432/432 [==============================] - 0s 608us/step - loss: 0.6025 - acc: 0.6852 - val_loss: 0.9857 - val_acc: 0.2407\n",
      "60/60 [==============================] - 0s 30us/step\n",
      "Average accuracy of model on the dev set =  0.5611111130979326\n",
      "Training on fold 4/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_78 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 432 samples, validate on 108 samples\n",
      "Epoch 1/40\n",
      "432/432 [==============================] - 1s 3ms/step - loss: 0.7906 - acc: 0.4120 - val_loss: 0.7524 - val_acc: 0.1852\n",
      "Epoch 2/40\n",
      "432/432 [==============================] - 0s 602us/step - loss: 0.6669 - acc: 0.6088 - val_loss: 0.9698 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "432/432 [==============================] - 0s 588us/step - loss: 0.6549 - acc: 0.6250 - val_loss: 0.9816 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "432/432 [==============================] - 0s 604us/step - loss: 0.6496 - acc: 0.6250 - val_loss: 0.9432 - val_acc: 0.0093\n",
      "Epoch 5/40\n",
      "432/432 [==============================] - 0s 587us/step - loss: 0.6465 - acc: 0.6250 - val_loss: 0.9399 - val_acc: 0.0278\n",
      "Epoch 6/40\n",
      "432/432 [==============================] - 0s 596us/step - loss: 0.6417 - acc: 0.6528 - val_loss: 0.9735 - val_acc: 0.0093\n",
      "Epoch 7/40\n",
      "432/432 [==============================] - 0s 591us/step - loss: 0.6379 - acc: 0.6389 - val_loss: 0.8854 - val_acc: 0.1296\n",
      "Epoch 8/40\n",
      "432/432 [==============================] - 0s 729us/step - loss: 0.6387 - acc: 0.6458 - val_loss: 1.0051 - val_acc: 0.0278\n",
      "Epoch 9/40\n",
      "432/432 [==============================] - 0s 623us/step - loss: 0.6365 - acc: 0.6574 - val_loss: 0.9898 - val_acc: 0.0370\n",
      "Epoch 10/40\n",
      "432/432 [==============================] - 0s 613us/step - loss: 0.6358 - acc: 0.6505 - val_loss: 1.0472 - val_acc: 0.0370\n",
      "Epoch 11/40\n",
      "432/432 [==============================] - 0s 558us/step - loss: 0.6349 - acc: 0.6551 - val_loss: 0.9334 - val_acc: 0.0833\n",
      "Epoch 12/40\n",
      "432/432 [==============================] - 0s 550us/step - loss: 0.6321 - acc: 0.6528 - val_loss: 0.9971 - val_acc: 0.0648\n",
      "Epoch 13/40\n",
      "432/432 [==============================] - 0s 548us/step - loss: 0.6317 - acc: 0.6574 - val_loss: 0.9987 - val_acc: 0.0741\n",
      "Epoch 14/40\n",
      "432/432 [==============================] - 0s 613us/step - loss: 0.6310 - acc: 0.6505 - val_loss: 0.9553 - val_acc: 0.1019\n",
      "Epoch 15/40\n",
      "432/432 [==============================] - 0s 571us/step - loss: 0.6285 - acc: 0.6505 - val_loss: 0.9709 - val_acc: 0.0926\n",
      "Epoch 16/40\n",
      "432/432 [==============================] - 0s 593us/step - loss: 0.6292 - acc: 0.6574 - val_loss: 1.0034 - val_acc: 0.0833\n",
      "Epoch 17/40\n",
      "432/432 [==============================] - 0s 598us/step - loss: 0.6271 - acc: 0.6528 - val_loss: 0.9557 - val_acc: 0.1759\n",
      "Epoch 18/40\n",
      "432/432 [==============================] - 0s 602us/step - loss: 0.6266 - acc: 0.6551 - val_loss: 1.0313 - val_acc: 0.0741\n",
      "Epoch 19/40\n",
      "432/432 [==============================] - 0s 560us/step - loss: 0.6290 - acc: 0.6458 - val_loss: 1.0572 - val_acc: 0.0741\n",
      "Epoch 20/40\n",
      "432/432 [==============================] - 0s 605us/step - loss: 0.6247 - acc: 0.6528 - val_loss: 1.0686 - val_acc: 0.0741\n",
      "Epoch 21/40\n",
      "432/432 [==============================] - 0s 589us/step - loss: 0.6241 - acc: 0.6644 - val_loss: 0.9406 - val_acc: 0.2130\n",
      "Epoch 22/40\n",
      "432/432 [==============================] - 0s 555us/step - loss: 0.6243 - acc: 0.6481 - val_loss: 1.0593 - val_acc: 0.0741\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40\n",
      "432/432 [==============================] - 0s 552us/step - loss: 0.6235 - acc: 0.6481 - val_loss: 1.0533 - val_acc: 0.0741\n",
      "Epoch 24/40\n",
      "432/432 [==============================] - 0s 569us/step - loss: 0.6247 - acc: 0.6574 - val_loss: 0.9682 - val_acc: 0.1944\n",
      "Epoch 25/40\n",
      "432/432 [==============================] - 0s 578us/step - loss: 0.6225 - acc: 0.6620 - val_loss: 0.9922 - val_acc: 0.1667\n",
      "Epoch 26/40\n",
      "432/432 [==============================] - 0s 539us/step - loss: 0.6237 - acc: 0.6505 - val_loss: 0.9440 - val_acc: 0.2315\n",
      "Epoch 27/40\n",
      "432/432 [==============================] - 0s 580us/step - loss: 0.6237 - acc: 0.6435 - val_loss: 0.9922 - val_acc: 0.1574\n",
      "Epoch 28/40\n",
      "432/432 [==============================] - 0s 541us/step - loss: 0.6178 - acc: 0.6551 - val_loss: 1.0678 - val_acc: 0.0833\n",
      "Epoch 29/40\n",
      "432/432 [==============================] - 0s 575us/step - loss: 0.6207 - acc: 0.6481 - val_loss: 0.9574 - val_acc: 0.2407\n",
      "Epoch 30/40\n",
      "432/432 [==============================] - 0s 538us/step - loss: 0.6202 - acc: 0.6690 - val_loss: 1.0127 - val_acc: 0.1574\n",
      "Epoch 31/40\n",
      "432/432 [==============================] - 0s 568us/step - loss: 0.6180 - acc: 0.6620 - val_loss: 1.0916 - val_acc: 0.1019\n",
      "Epoch 32/40\n",
      "432/432 [==============================] - 0s 567us/step - loss: 0.6197 - acc: 0.6528 - val_loss: 0.9129 - val_acc: 0.2500\n",
      "Epoch 33/40\n",
      "432/432 [==============================] - 0s 544us/step - loss: 0.6191 - acc: 0.6528 - val_loss: 1.0240 - val_acc: 0.1481\n",
      "Epoch 34/40\n",
      "432/432 [==============================] - 0s 581us/step - loss: 0.6202 - acc: 0.6597 - val_loss: 1.1088 - val_acc: 0.0926\n",
      "Epoch 35/40\n",
      "432/432 [==============================] - 0s 580us/step - loss: 0.6174 - acc: 0.6644 - val_loss: 0.9558 - val_acc: 0.2222\n",
      "Epoch 36/40\n",
      "432/432 [==============================] - 0s 573us/step - loss: 0.6175 - acc: 0.6528 - val_loss: 0.9947 - val_acc: 0.1852\n",
      "Epoch 37/40\n",
      "432/432 [==============================] - 0s 543us/step - loss: 0.6170 - acc: 0.6690 - val_loss: 1.0324 - val_acc: 0.1574\n",
      "Epoch 38/40\n",
      "432/432 [==============================] - 0s 588us/step - loss: 0.6158 - acc: 0.6574 - val_loss: 1.0378 - val_acc: 0.1389\n",
      "Epoch 39/40\n",
      "432/432 [==============================] - 0s 571us/step - loss: 0.6154 - acc: 0.6620 - val_loss: 0.9927 - val_acc: 0.2130\n",
      "Epoch 40/40\n",
      "432/432 [==============================] - 0s 541us/step - loss: 0.6130 - acc: 0.6667 - val_loss: 1.0865 - val_acc: 0.1296\n",
      "60/60 [==============================] - 0s 31us/step\n",
      "Average accuracy of model on the dev set =  0.5583333349476258\n",
      "Training on fold 5/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_84 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_87 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 432 samples, validate on 108 samples\n",
      "Epoch 1/40\n",
      "432/432 [==============================] - 1s 3ms/step - loss: 0.7083 - acc: 0.6250 - val_loss: 1.2698 - val_acc: 0.0000e+00\n",
      "Epoch 2/40\n",
      "432/432 [==============================] - 0s 591us/step - loss: 0.6708 - acc: 0.6250 - val_loss: 1.0753 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "432/432 [==============================] - 0s 557us/step - loss: 0.6593 - acc: 0.6250 - val_loss: 0.9898 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "432/432 [==============================] - 0s 596us/step - loss: 0.6556 - acc: 0.6250 - val_loss: 0.9666 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "432/432 [==============================] - 0s 555us/step - loss: 0.6521 - acc: 0.6250 - val_loss: 0.9084 - val_acc: 0.0185\n",
      "Epoch 6/40\n",
      "432/432 [==============================] - 0s 590us/step - loss: 0.6481 - acc: 0.6273 - val_loss: 0.9852 - val_acc: 0.0000e+00\n",
      "Epoch 7/40\n",
      "432/432 [==============================] - 0s 593us/step - loss: 0.6452 - acc: 0.6481 - val_loss: 1.0086 - val_acc: 0.0000e+00\n",
      "Epoch 8/40\n",
      "432/432 [==============================] - 0s 563us/step - loss: 0.6420 - acc: 0.6412 - val_loss: 0.9955 - val_acc: 0.1204\n",
      "Epoch 9/40\n",
      "432/432 [==============================] - 0s 589us/step - loss: 0.6385 - acc: 0.6551 - val_loss: 0.9844 - val_acc: 0.1204\n",
      "Epoch 10/40\n",
      "432/432 [==============================] - 0s 548us/step - loss: 0.6356 - acc: 0.6597 - val_loss: 1.0217 - val_acc: 0.1204\n",
      "Epoch 11/40\n",
      "432/432 [==============================] - 0s 600us/step - loss: 0.6340 - acc: 0.6597 - val_loss: 1.0385 - val_acc: 0.1204\n",
      "Epoch 12/40\n",
      "432/432 [==============================] - 0s 590us/step - loss: 0.6326 - acc: 0.6597 - val_loss: 1.0045 - val_acc: 0.1204\n",
      "Epoch 13/40\n",
      "432/432 [==============================] - 0s 555us/step - loss: 0.6295 - acc: 0.6597 - val_loss: 1.0120 - val_acc: 0.1204\n",
      "Epoch 14/40\n",
      "432/432 [==============================] - 0s 596us/step - loss: 0.6281 - acc: 0.6597 - val_loss: 0.9565 - val_acc: 0.1204\n",
      "Epoch 15/40\n",
      "432/432 [==============================] - 0s 599us/step - loss: 0.6252 - acc: 0.6620 - val_loss: 0.9244 - val_acc: 0.1667\n",
      "Epoch 16/40\n",
      "432/432 [==============================] - 0s 599us/step - loss: 0.6242 - acc: 0.6644 - val_loss: 1.0224 - val_acc: 0.1296\n",
      "Epoch 17/40\n",
      "432/432 [==============================] - 0s 559us/step - loss: 0.6233 - acc: 0.6620 - val_loss: 0.9921 - val_acc: 0.1667\n",
      "Epoch 18/40\n",
      "432/432 [==============================] - 0s 556us/step - loss: 0.6218 - acc: 0.6620 - val_loss: 0.9079 - val_acc: 0.1852\n",
      "Epoch 19/40\n",
      "432/432 [==============================] - 0s 630us/step - loss: 0.6217 - acc: 0.6644 - val_loss: 0.9787 - val_acc: 0.1667\n",
      "Epoch 20/40\n",
      "432/432 [==============================] - 0s 632us/step - loss: 0.6212 - acc: 0.6574 - val_loss: 0.9879 - val_acc: 0.1667\n",
      "Epoch 21/40\n",
      "432/432 [==============================] - 0s 611us/step - loss: 0.6196 - acc: 0.6620 - val_loss: 1.0007 - val_acc: 0.1667\n",
      "Epoch 22/40\n",
      "432/432 [==============================] - 0s 596us/step - loss: 0.6181 - acc: 0.6736 - val_loss: 0.9926 - val_acc: 0.1667\n",
      "Epoch 23/40\n",
      "432/432 [==============================] - 0s 595us/step - loss: 0.6169 - acc: 0.6528 - val_loss: 1.0696 - val_acc: 0.1667\n",
      "Epoch 24/40\n",
      "432/432 [==============================] - 0s 637us/step - loss: 0.6189 - acc: 0.6574 - val_loss: 1.0509 - val_acc: 0.1667\n",
      "Epoch 25/40\n",
      "432/432 [==============================] - 0s 602us/step - loss: 0.6171 - acc: 0.6667 - val_loss: 0.9888 - val_acc: 0.1852\n",
      "Epoch 26/40\n",
      "432/432 [==============================] - 0s 550us/step - loss: 0.6145 - acc: 0.6759 - val_loss: 1.0909 - val_acc: 0.1667\n",
      "Epoch 27/40\n",
      "432/432 [==============================] - 0s 599us/step - loss: 0.6146 - acc: 0.6574 - val_loss: 0.9869 - val_acc: 0.1852\n",
      "Epoch 28/40\n",
      "432/432 [==============================] - 0s 594us/step - loss: 0.6140 - acc: 0.6736 - val_loss: 1.0310 - val_acc: 0.1667\n",
      "Epoch 29/40\n",
      "432/432 [==============================] - 0s 562us/step - loss: 0.6139 - acc: 0.6551 - val_loss: 1.0257 - val_acc: 0.1759\n",
      "Epoch 30/40\n",
      "432/432 [==============================] - 0s 558us/step - loss: 0.6132 - acc: 0.6644 - val_loss: 0.9415 - val_acc: 0.2593\n",
      "Epoch 31/40\n",
      "432/432 [==============================] - 0s 598us/step - loss: 0.6124 - acc: 0.6690 - val_loss: 1.0172 - val_acc: 0.1759\n",
      "Epoch 32/40\n",
      "432/432 [==============================] - 0s 599us/step - loss: 0.6107 - acc: 0.6736 - val_loss: 1.0159 - val_acc: 0.2500\n",
      "Epoch 33/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - 0s 581us/step - loss: 0.6101 - acc: 0.6574 - val_loss: 1.0061 - val_acc: 0.2500\n",
      "Epoch 34/40\n",
      "432/432 [==============================] - 0s 545us/step - loss: 0.6126 - acc: 0.6597 - val_loss: 0.9793 - val_acc: 0.2685\n",
      "Epoch 35/40\n",
      "432/432 [==============================] - 0s 586us/step - loss: 0.6088 - acc: 0.6736 - val_loss: 1.0363 - val_acc: 0.2130\n",
      "Epoch 36/40\n",
      "432/432 [==============================] - 0s 555us/step - loss: 0.6091 - acc: 0.6620 - val_loss: 1.0075 - val_acc: 0.2500\n",
      "Epoch 37/40\n",
      "432/432 [==============================] - 0s 585us/step - loss: 0.6062 - acc: 0.6713 - val_loss: 0.9339 - val_acc: 0.2593\n",
      "Epoch 38/40\n",
      "432/432 [==============================] - 0s 578us/step - loss: 0.6063 - acc: 0.6759 - val_loss: 0.9943 - val_acc: 0.2500\n",
      "Epoch 39/40\n",
      "432/432 [==============================] - 0s 548us/step - loss: 0.6046 - acc: 0.6806 - val_loss: 1.0242 - val_acc: 0.2593\n",
      "Epoch 40/40\n",
      "432/432 [==============================] - 0s 580us/step - loss: 0.6051 - acc: 0.6829 - val_loss: 1.0355 - val_acc: 0.2593\n",
      "60/60 [==============================] - 0s 29us/step\n",
      "Average accuracy of model on the dev set =  0.5633333348234495\n",
      "Training on fold 6/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_90 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 432 samples, validate on 108 samples\n",
      "Epoch 1/40\n",
      "432/432 [==============================] - 1s 3ms/step - loss: 0.6658 - acc: 0.6250 - val_loss: 1.0373 - val_acc: 0.0000e+00\n",
      "Epoch 2/40\n",
      "432/432 [==============================] - 0s 594us/step - loss: 0.6610 - acc: 0.6250 - val_loss: 1.0015 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "432/432 [==============================] - 0s 593us/step - loss: 0.6588 - acc: 0.6250 - val_loss: 0.9696 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "432/432 [==============================] - 0s 563us/step - loss: 0.6561 - acc: 0.6250 - val_loss: 0.9671 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "432/432 [==============================] - 0s 555us/step - loss: 0.6527 - acc: 0.6204 - val_loss: 0.9743 - val_acc: 0.0000e+00\n",
      "Epoch 6/40\n",
      "432/432 [==============================] - 0s 600us/step - loss: 0.6483 - acc: 0.6273 - val_loss: 1.0055 - val_acc: 0.0370\n",
      "Epoch 7/40\n",
      "432/432 [==============================] - 0s 596us/step - loss: 0.6441 - acc: 0.6458 - val_loss: 0.9501 - val_acc: 0.1204\n",
      "Epoch 8/40\n",
      "432/432 [==============================] - 0s 555us/step - loss: 0.6400 - acc: 0.6551 - val_loss: 0.9138 - val_acc: 0.1204\n",
      "Epoch 9/40\n",
      "432/432 [==============================] - 0s 573us/step - loss: 0.6380 - acc: 0.6597 - val_loss: 1.0312 - val_acc: 0.1111\n",
      "Epoch 10/40\n",
      "432/432 [==============================] - 0s 592us/step - loss: 0.6364 - acc: 0.6597 - val_loss: 0.9894 - val_acc: 0.1204\n",
      "Epoch 11/40\n",
      "432/432 [==============================] - 0s 554us/step - loss: 0.6354 - acc: 0.6597 - val_loss: 0.9341 - val_acc: 0.1204\n",
      "Epoch 12/40\n",
      "432/432 [==============================] - 0s 604us/step - loss: 0.6320 - acc: 0.6597 - val_loss: 1.0404 - val_acc: 0.1204\n",
      "Epoch 13/40\n",
      "432/432 [==============================] - 0s 599us/step - loss: 0.6292 - acc: 0.6574 - val_loss: 0.9940 - val_acc: 0.1204\n",
      "Epoch 14/40\n",
      "432/432 [==============================] - 0s 559us/step - loss: 0.6289 - acc: 0.6574 - val_loss: 0.9535 - val_acc: 0.1296\n",
      "Epoch 15/40\n",
      "432/432 [==============================] - 0s 596us/step - loss: 0.6285 - acc: 0.6551 - val_loss: 0.9108 - val_acc: 0.1574\n",
      "Epoch 16/40\n",
      "432/432 [==============================] - 0s 570us/step - loss: 0.6255 - acc: 0.6574 - val_loss: 0.9415 - val_acc: 0.1667\n",
      "Epoch 17/40\n",
      "432/432 [==============================] - 0s 569us/step - loss: 0.6272 - acc: 0.6713 - val_loss: 0.9981 - val_acc: 0.1389\n",
      "Epoch 18/40\n",
      "432/432 [==============================] - 0s 558us/step - loss: 0.6203 - acc: 0.6667 - val_loss: 1.0623 - val_acc: 0.1296\n",
      "Epoch 19/40\n",
      "432/432 [==============================] - 0s 595us/step - loss: 0.6225 - acc: 0.6620 - val_loss: 0.9619 - val_acc: 0.1852\n",
      "Epoch 20/40\n",
      "432/432 [==============================] - 0s 602us/step - loss: 0.6204 - acc: 0.6690 - val_loss: 0.9894 - val_acc: 0.1389\n",
      "Epoch 21/40\n",
      "432/432 [==============================] - 0s 565us/step - loss: 0.6200 - acc: 0.6458 - val_loss: 1.0004 - val_acc: 0.1667\n",
      "Epoch 22/40\n",
      "432/432 [==============================] - 0s 596us/step - loss: 0.6188 - acc: 0.6574 - val_loss: 1.0352 - val_acc: 0.1389\n",
      "Epoch 23/40\n",
      "432/432 [==============================] - 0s 597us/step - loss: 0.6168 - acc: 0.6667 - val_loss: 0.9438 - val_acc: 0.2407\n",
      "Epoch 24/40\n",
      "432/432 [==============================] - 0s 555us/step - loss: 0.6170 - acc: 0.6644 - val_loss: 1.0305 - val_acc: 0.1574\n",
      "Epoch 25/40\n",
      "432/432 [==============================] - 0s 607us/step - loss: 0.6141 - acc: 0.6597 - val_loss: 1.0212 - val_acc: 0.1667\n",
      "Epoch 26/40\n",
      "432/432 [==============================] - 0s 563us/step - loss: 0.6126 - acc: 0.6644 - val_loss: 0.9899 - val_acc: 0.1944\n",
      "Epoch 27/40\n",
      "432/432 [==============================] - 0s 564us/step - loss: 0.6114 - acc: 0.6620 - val_loss: 0.9452 - val_acc: 0.2315\n",
      "Epoch 28/40\n",
      "432/432 [==============================] - 0s 603us/step - loss: 0.6123 - acc: 0.6690 - val_loss: 1.0123 - val_acc: 0.1667\n",
      "Epoch 29/40\n",
      "432/432 [==============================] - 0s 568us/step - loss: 0.6088 - acc: 0.6713 - val_loss: 0.8942 - val_acc: 0.2963\n",
      "Epoch 30/40\n",
      "432/432 [==============================] - 0s 600us/step - loss: 0.6114 - acc: 0.6806 - val_loss: 0.9838 - val_acc: 0.2222\n",
      "Epoch 31/40\n",
      "432/432 [==============================] - 0s 604us/step - loss: 0.6099 - acc: 0.6667 - val_loss: 0.9285 - val_acc: 0.2500\n",
      "Epoch 32/40\n",
      "432/432 [==============================] - 0s 558us/step - loss: 0.6073 - acc: 0.6620 - val_loss: 0.9450 - val_acc: 0.2593\n",
      "Epoch 33/40\n",
      "432/432 [==============================] - 0s 566us/step - loss: 0.6065 - acc: 0.6644 - val_loss: 0.9570 - val_acc: 0.2407\n",
      "Epoch 34/40\n",
      "432/432 [==============================] - 0s 597us/step - loss: 0.6075 - acc: 0.6713 - val_loss: 0.9445 - val_acc: 0.2500\n",
      "Epoch 35/40\n",
      "432/432 [==============================] - 0s 566us/step - loss: 0.6037 - acc: 0.6736 - val_loss: 0.9642 - val_acc: 0.2315\n",
      "Epoch 36/40\n",
      "432/432 [==============================] - 0s 605us/step - loss: 0.6036 - acc: 0.6829 - val_loss: 1.0044 - val_acc: 0.1759\n",
      "Epoch 37/40\n",
      "432/432 [==============================] - 0s 571us/step - loss: 0.6029 - acc: 0.6713 - val_loss: 0.8958 - val_acc: 0.2778\n",
      "Epoch 38/40\n",
      "432/432 [==============================] - 0s 572us/step - loss: 0.6030 - acc: 0.6736 - val_loss: 0.9666 - val_acc: 0.2315\n",
      "Epoch 39/40\n",
      "432/432 [==============================] - 0s 568us/step - loss: 0.5990 - acc: 0.6736 - val_loss: 1.0438 - val_acc: 0.2130\n",
      "Epoch 40/40\n",
      "432/432 [==============================] - 0s 593us/step - loss: 0.6005 - acc: 0.6690 - val_loss: 0.9390 - val_acc: 0.2500\n",
      "60/60 [==============================] - 0s 32us/step\n",
      "Average accuracy of model on the dev set =  0.569444445355071\n",
      "Training on fold 7/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_96 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 432 samples, validate on 108 samples\n",
      "Epoch 1/40\n",
      "432/432 [==============================] - 2s 4ms/step - loss: 0.6885 - acc: 0.5394 - val_loss: 0.8344 - val_acc: 0.0000e+00\n",
      "Epoch 2/40\n",
      "432/432 [==============================] - 0s 602us/step - loss: 0.6633 - acc: 0.6250 - val_loss: 0.9466 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "432/432 [==============================] - 0s 591us/step - loss: 0.6591 - acc: 0.6250 - val_loss: 0.9584 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "432/432 [==============================] - 0s 566us/step - loss: 0.6547 - acc: 0.6250 - val_loss: 0.9746 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "432/432 [==============================] - 0s 571us/step - loss: 0.6511 - acc: 0.6250 - val_loss: 0.9787 - val_acc: 0.0000e+00\n",
      "Epoch 6/40\n",
      "432/432 [==============================] - 0s 571us/step - loss: 0.6489 - acc: 0.6250 - val_loss: 0.9737 - val_acc: 0.0000e+00\n",
      "Epoch 7/40\n",
      "432/432 [==============================] - 0s 607us/step - loss: 0.6436 - acc: 0.6366 - val_loss: 0.9896 - val_acc: 0.1204\n",
      "Epoch 8/40\n",
      "432/432 [==============================] - 0s 611us/step - loss: 0.6402 - acc: 0.6597 - val_loss: 0.9940 - val_acc: 0.1296\n",
      "Epoch 9/40\n",
      "432/432 [==============================] - 0s 595us/step - loss: 0.6356 - acc: 0.6620 - val_loss: 0.9604 - val_acc: 0.1296\n",
      "Epoch 10/40\n",
      "432/432 [==============================] - 0s 595us/step - loss: 0.6336 - acc: 0.6620 - val_loss: 0.9361 - val_acc: 0.1296\n",
      "Epoch 11/40\n",
      "432/432 [==============================] - 0s 609us/step - loss: 0.6325 - acc: 0.6620 - val_loss: 0.9631 - val_acc: 0.1296\n",
      "Epoch 12/40\n",
      "432/432 [==============================] - 0s 606us/step - loss: 0.6301 - acc: 0.6597 - val_loss: 1.0417 - val_acc: 0.1296\n",
      "Epoch 13/40\n",
      "432/432 [==============================] - 0s 579us/step - loss: 0.6270 - acc: 0.6620 - val_loss: 0.9774 - val_acc: 0.1296\n",
      "Epoch 14/40\n",
      "432/432 [==============================] - 0s 569us/step - loss: 0.6254 - acc: 0.6620 - val_loss: 0.9517 - val_acc: 0.1296\n",
      "Epoch 15/40\n",
      "432/432 [==============================] - 0s 600us/step - loss: 0.6248 - acc: 0.6620 - val_loss: 0.9305 - val_acc: 0.1296\n",
      "Epoch 16/40\n",
      "432/432 [==============================] - 0s 604us/step - loss: 0.6228 - acc: 0.6620 - val_loss: 0.9642 - val_acc: 0.1296\n",
      "Epoch 17/40\n",
      "432/432 [==============================] - 0s 604us/step - loss: 0.6219 - acc: 0.6597 - val_loss: 0.9201 - val_acc: 0.1852\n",
      "Epoch 18/40\n",
      "432/432 [==============================] - 0s 618us/step - loss: 0.6228 - acc: 0.6667 - val_loss: 0.9476 - val_acc: 0.1296\n",
      "Epoch 19/40\n",
      "432/432 [==============================] - 0s 601us/step - loss: 0.6193 - acc: 0.6620 - val_loss: 0.9878 - val_acc: 0.1296\n",
      "Epoch 20/40\n",
      "432/432 [==============================] - 0s 604us/step - loss: 0.6197 - acc: 0.6620 - val_loss: 0.9603 - val_acc: 0.1574\n",
      "Epoch 21/40\n",
      "432/432 [==============================] - 0s 566us/step - loss: 0.6167 - acc: 0.6620 - val_loss: 0.9438 - val_acc: 0.1944\n",
      "Epoch 22/40\n",
      "432/432 [==============================] - 0s 571us/step - loss: 0.6162 - acc: 0.6644 - val_loss: 1.0274 - val_acc: 0.1296\n",
      "Epoch 23/40\n",
      "432/432 [==============================] - 0s 566us/step - loss: 0.6171 - acc: 0.6644 - val_loss: 0.9947 - val_acc: 0.1574\n",
      "Epoch 24/40\n",
      "432/432 [==============================] - 0s 598us/step - loss: 0.6145 - acc: 0.6713 - val_loss: 1.0192 - val_acc: 0.1481\n",
      "Epoch 25/40\n",
      "432/432 [==============================] - 0s 612us/step - loss: 0.6142 - acc: 0.6667 - val_loss: 1.0671 - val_acc: 0.1296\n",
      "Epoch 26/40\n",
      "432/432 [==============================] - 0s 612us/step - loss: 0.6158 - acc: 0.6690 - val_loss: 1.0086 - val_acc: 0.1574\n",
      "Epoch 27/40\n",
      "432/432 [==============================] - 0s 607us/step - loss: 0.6110 - acc: 0.6713 - val_loss: 0.9754 - val_acc: 0.1944\n",
      "Epoch 28/40\n",
      "432/432 [==============================] - 0s 607us/step - loss: 0.6123 - acc: 0.6690 - val_loss: 0.9642 - val_acc: 0.2037\n",
      "Epoch 29/40\n",
      "432/432 [==============================] - 0s 567us/step - loss: 0.6099 - acc: 0.6736 - val_loss: 1.0066 - val_acc: 0.1574\n",
      "Epoch 30/40\n",
      "432/432 [==============================] - 0s 578us/step - loss: 0.6077 - acc: 0.6782 - val_loss: 0.9025 - val_acc: 0.2685\n",
      "Epoch 31/40\n",
      "432/432 [==============================] - 0s 608us/step - loss: 0.6089 - acc: 0.6713 - val_loss: 1.0105 - val_acc: 0.1574\n",
      "Epoch 32/40\n",
      "432/432 [==============================] - 0s 615us/step - loss: 0.6072 - acc: 0.6736 - val_loss: 0.9626 - val_acc: 0.2315\n",
      "Epoch 33/40\n",
      "432/432 [==============================] - 0s 609us/step - loss: 0.6074 - acc: 0.6690 - val_loss: 1.0392 - val_acc: 0.1574\n",
      "Epoch 34/40\n",
      "432/432 [==============================] - 0s 608us/step - loss: 0.6045 - acc: 0.6644 - val_loss: 1.0049 - val_acc: 0.1944\n",
      "Epoch 35/40\n",
      "432/432 [==============================] - 0s 687us/step - loss: 0.6044 - acc: 0.6690 - val_loss: 1.0240 - val_acc: 0.1759\n",
      "Epoch 36/40\n",
      "432/432 [==============================] - 0s 612us/step - loss: 0.6025 - acc: 0.6690 - val_loss: 0.9332 - val_acc: 0.2500\n",
      "Epoch 37/40\n",
      "432/432 [==============================] - 0s 610us/step - loss: 0.6017 - acc: 0.6782 - val_loss: 0.9401 - val_acc: 0.2500\n",
      "Epoch 38/40\n",
      "432/432 [==============================] - 0s 610us/step - loss: 0.6018 - acc: 0.6759 - val_loss: 0.9490 - val_acc: 0.2500\n",
      "Epoch 39/40\n",
      "432/432 [==============================] - 0s 608us/step - loss: 0.6009 - acc: 0.6759 - val_loss: 0.9676 - val_acc: 0.2500\n",
      "Epoch 40/40\n",
      "432/432 [==============================] - 0s 626us/step - loss: 0.6004 - acc: 0.6713 - val_loss: 0.9544 - val_acc: 0.2500\n",
      "60/60 [==============================] - 0s 28us/step\n",
      "Average accuracy of model on the dev set =  0.569047620679651\n",
      "Training on fold 8/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_102 (Dense)            (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 432 samples, validate on 108 samples\n",
      "Epoch 1/40\n",
      "432/432 [==============================] - 2s 4ms/step - loss: 0.6878 - acc: 0.5370 - val_loss: 0.8368 - val_acc: 0.0000e+00\n",
      "Epoch 2/40\n",
      "432/432 [==============================] - 0s 629us/step - loss: 0.6645 - acc: 0.6250 - val_loss: 0.9285 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "432/432 [==============================] - 0s 625us/step - loss: 0.6595 - acc: 0.6250 - val_loss: 0.9412 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "432/432 [==============================] - 0s 617us/step - loss: 0.6571 - acc: 0.6250 - val_loss: 0.9824 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "432/432 [==============================] - 0s 599us/step - loss: 0.6536 - acc: 0.6250 - val_loss: 0.9814 - val_acc: 0.0000e+00\n",
      "Epoch 6/40\n",
      "432/432 [==============================] - 0s 625us/step - loss: 0.6495 - acc: 0.6250 - val_loss: 0.9183 - val_acc: 0.0000e+00\n",
      "Epoch 7/40\n",
      "432/432 [==============================] - 0s 630us/step - loss: 0.6458 - acc: 0.6250 - val_loss: 0.9423 - val_acc: 0.1296\n",
      "Epoch 8/40\n",
      "432/432 [==============================] - 0s 627us/step - loss: 0.6417 - acc: 0.6551 - val_loss: 0.9507 - val_acc: 0.1389\n",
      "Epoch 9/40\n",
      "432/432 [==============================] - 0s 620us/step - loss: 0.6391 - acc: 0.6620 - val_loss: 1.0189 - val_acc: 0.1389\n",
      "Epoch 10/40\n",
      "432/432 [==============================] - 0s 581us/step - loss: 0.6356 - acc: 0.6620 - val_loss: 0.9886 - val_acc: 0.1389\n",
      "Epoch 11/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - 0s 665us/step - loss: 0.6359 - acc: 0.6620 - val_loss: 0.9623 - val_acc: 0.1389\n",
      "Epoch 12/40\n",
      "432/432 [==============================] - 0s 635us/step - loss: 0.6314 - acc: 0.6620 - val_loss: 0.9683 - val_acc: 0.1389\n",
      "Epoch 13/40\n",
      "432/432 [==============================] - 0s 624us/step - loss: 0.6289 - acc: 0.6620 - val_loss: 1.0505 - val_acc: 0.1389\n",
      "Epoch 14/40\n",
      "432/432 [==============================] - 0s 633us/step - loss: 0.6299 - acc: 0.6644 - val_loss: 0.8855 - val_acc: 0.1944\n",
      "Epoch 15/40\n",
      "432/432 [==============================] - 0s 626us/step - loss: 0.6281 - acc: 0.6597 - val_loss: 0.8908 - val_acc: 0.2130\n",
      "Epoch 16/40\n",
      "432/432 [==============================] - 0s 626us/step - loss: 0.6287 - acc: 0.6597 - val_loss: 0.9999 - val_acc: 0.1389\n",
      "Epoch 17/40\n",
      "432/432 [==============================] - 0s 631us/step - loss: 0.6239 - acc: 0.6644 - val_loss: 0.9492 - val_acc: 0.1852\n",
      "Epoch 18/40\n",
      "432/432 [==============================] - 0s 626us/step - loss: 0.6213 - acc: 0.6597 - val_loss: 1.0586 - val_acc: 0.1389\n",
      "Epoch 19/40\n",
      "432/432 [==============================] - 0s 630us/step - loss: 0.6235 - acc: 0.6644 - val_loss: 0.9629 - val_acc: 0.1667\n",
      "Epoch 20/40\n",
      "432/432 [==============================] - 0s 638us/step - loss: 0.6215 - acc: 0.6644 - val_loss: 0.9821 - val_acc: 0.1667\n",
      "Epoch 21/40\n",
      "432/432 [==============================] - 0s 620us/step - loss: 0.6203 - acc: 0.6667 - val_loss: 0.8997 - val_acc: 0.2130\n",
      "Epoch 22/40\n",
      "432/432 [==============================] - 0s 663us/step - loss: 0.6209 - acc: 0.6667 - val_loss: 0.9504 - val_acc: 0.1944\n",
      "Epoch 23/40\n",
      "432/432 [==============================] - 0s 615us/step - loss: 0.6196 - acc: 0.6667 - val_loss: 1.0146 - val_acc: 0.1389\n",
      "Epoch 24/40\n",
      "432/432 [==============================] - 0s 612us/step - loss: 0.6183 - acc: 0.6759 - val_loss: 0.8968 - val_acc: 0.2593\n",
      "Epoch 25/40\n",
      "432/432 [==============================] - 0s 616us/step - loss: 0.6186 - acc: 0.6713 - val_loss: 0.9857 - val_acc: 0.1944\n",
      "Epoch 26/40\n",
      "432/432 [==============================] - 0s 607us/step - loss: 0.6172 - acc: 0.6667 - val_loss: 1.0401 - val_acc: 0.1389\n",
      "Epoch 27/40\n",
      "432/432 [==============================] - 0s 571us/step - loss: 0.6153 - acc: 0.6690 - val_loss: 0.9574 - val_acc: 0.2130\n",
      "Epoch 28/40\n",
      "432/432 [==============================] - 0s 575us/step - loss: 0.6164 - acc: 0.6736 - val_loss: 1.0059 - val_acc: 0.2130\n",
      "Epoch 29/40\n",
      "432/432 [==============================] - 0s 606us/step - loss: 0.6161 - acc: 0.6736 - val_loss: 0.9903 - val_acc: 0.2130\n",
      "Epoch 30/40\n",
      "432/432 [==============================] - 0s 617us/step - loss: 0.6118 - acc: 0.6852 - val_loss: 1.0531 - val_acc: 0.1852\n",
      "Epoch 31/40\n",
      "432/432 [==============================] - 0s 617us/step - loss: 0.6116 - acc: 0.6782 - val_loss: 0.9828 - val_acc: 0.2130\n",
      "Epoch 32/40\n",
      "432/432 [==============================] - 0s 615us/step - loss: 0.6123 - acc: 0.6782 - val_loss: 0.9451 - val_acc: 0.2407\n",
      "Epoch 33/40\n",
      "432/432 [==============================] - 0s 607us/step - loss: 0.6132 - acc: 0.6736 - val_loss: 0.9949 - val_acc: 0.2130\n",
      "Epoch 34/40\n",
      "432/432 [==============================] - 0s 618us/step - loss: 0.6129 - acc: 0.6759 - val_loss: 1.0509 - val_acc: 0.1667\n",
      "Epoch 35/40\n",
      "432/432 [==============================] - 0s 613us/step - loss: 0.6109 - acc: 0.6968 - val_loss: 0.9870 - val_acc: 0.2130\n",
      "Epoch 36/40\n",
      "432/432 [==============================] - 0s 616us/step - loss: 0.6112 - acc: 0.6713 - val_loss: 0.9506 - val_acc: 0.2222\n",
      "Epoch 37/40\n",
      "432/432 [==============================] - 0s 619us/step - loss: 0.6112 - acc: 0.6806 - val_loss: 1.0052 - val_acc: 0.2130\n",
      "Epoch 38/40\n",
      "432/432 [==============================] - 0s 608us/step - loss: 0.6089 - acc: 0.6852 - val_loss: 1.0403 - val_acc: 0.2037\n",
      "Epoch 39/40\n",
      "432/432 [==============================] - 0s 613us/step - loss: 0.6103 - acc: 0.6806 - val_loss: 1.0539 - val_acc: 0.2130\n",
      "Epoch 40/40\n",
      "432/432 [==============================] - 0s 619us/step - loss: 0.6101 - acc: 0.6898 - val_loss: 1.0131 - val_acc: 0.2130\n",
      "60/60 [==============================] - 0s 30us/step\n",
      "Average accuracy of model on the dev set =  0.5604166684672236\n",
      "Training on fold 9/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_108 (Dense)            (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 432 samples, validate on 108 samples\n",
      "Epoch 1/40\n",
      "432/432 [==============================] - 2s 4ms/step - loss: 0.8497 - acc: 0.3750 - val_loss: 0.6295 - val_acc: 0.8241\n",
      "Epoch 2/40\n",
      "432/432 [==============================] - 0s 658us/step - loss: 0.6849 - acc: 0.5463 - val_loss: 0.8936 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "432/432 [==============================] - 0s 635us/step - loss: 0.6642 - acc: 0.6250 - val_loss: 1.0042 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "432/432 [==============================] - 0s 633us/step - loss: 0.6591 - acc: 0.6227 - val_loss: 0.9913 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "432/432 [==============================] - 0s 640us/step - loss: 0.6574 - acc: 0.6273 - val_loss: 0.9304 - val_acc: 0.0648\n",
      "Epoch 6/40\n",
      "432/432 [==============================] - 0s 641us/step - loss: 0.6505 - acc: 0.6458 - val_loss: 1.0234 - val_acc: 0.0463\n",
      "Epoch 7/40\n",
      "432/432 [==============================] - 0s 696us/step - loss: 0.6491 - acc: 0.6505 - val_loss: 1.0103 - val_acc: 0.0648\n",
      "Epoch 8/40\n",
      "432/432 [==============================] - 0s 645us/step - loss: 0.6474 - acc: 0.6551 - val_loss: 1.0371 - val_acc: 0.0648\n",
      "Epoch 9/40\n",
      "432/432 [==============================] - 0s 643us/step - loss: 0.6463 - acc: 0.6574 - val_loss: 1.0416 - val_acc: 0.0741\n",
      "Epoch 10/40\n",
      "432/432 [==============================] - 0s 675us/step - loss: 0.6430 - acc: 0.6551 - val_loss: 0.9731 - val_acc: 0.1019\n",
      "Epoch 11/40\n",
      "432/432 [==============================] - 0s 600us/step - loss: 0.6404 - acc: 0.6574 - val_loss: 1.0286 - val_acc: 0.1019\n",
      "Epoch 12/40\n",
      "432/432 [==============================] - 0s 597us/step - loss: 0.6381 - acc: 0.6597 - val_loss: 0.9271 - val_acc: 0.1111\n",
      "Epoch 13/40\n",
      "432/432 [==============================] - 0s 638us/step - loss: 0.6396 - acc: 0.6528 - val_loss: 0.9550 - val_acc: 0.1019\n",
      "Epoch 14/40\n",
      "432/432 [==============================] - 0s 628us/step - loss: 0.6387 - acc: 0.6620 - val_loss: 0.9929 - val_acc: 0.1019\n",
      "Epoch 15/40\n",
      "432/432 [==============================] - 0s 641us/step - loss: 0.6343 - acc: 0.6505 - val_loss: 1.0449 - val_acc: 0.1019\n",
      "Epoch 16/40\n",
      "432/432 [==============================] - 0s 639us/step - loss: 0.6339 - acc: 0.6620 - val_loss: 1.0044 - val_acc: 0.1019\n",
      "Epoch 17/40\n",
      "432/432 [==============================] - 0s 634us/step - loss: 0.6327 - acc: 0.6644 - val_loss: 1.0585 - val_acc: 0.1019\n",
      "Epoch 18/40\n",
      "432/432 [==============================] - 0s 587us/step - loss: 0.6305 - acc: 0.6620 - val_loss: 1.0638 - val_acc: 0.1019\n",
      "Epoch 19/40\n",
      "432/432 [==============================] - 0s 656us/step - loss: 0.6300 - acc: 0.6620 - val_loss: 1.0832 - val_acc: 0.1019\n",
      "Epoch 20/40\n",
      "432/432 [==============================] - 0s 653us/step - loss: 0.6306 - acc: 0.6620 - val_loss: 1.0327 - val_acc: 0.1019\n",
      "Epoch 21/40\n",
      "432/432 [==============================] - 0s 659us/step - loss: 0.6276 - acc: 0.6620 - val_loss: 0.9855 - val_acc: 0.1019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40\n",
      "432/432 [==============================] - 0s 661us/step - loss: 0.6276 - acc: 0.6620 - val_loss: 0.9899 - val_acc: 0.1019\n",
      "Epoch 23/40\n",
      "432/432 [==============================] - 0s 661us/step - loss: 0.6270 - acc: 0.6597 - val_loss: 1.0307 - val_acc: 0.1019\n",
      "Epoch 24/40\n",
      "432/432 [==============================] - 0s 672us/step - loss: 0.6256 - acc: 0.6667 - val_loss: 0.8965 - val_acc: 0.1759\n",
      "Epoch 25/40\n",
      "432/432 [==============================] - 0s 604us/step - loss: 0.6268 - acc: 0.6667 - val_loss: 0.9668 - val_acc: 0.1296\n",
      "Epoch 26/40\n",
      "432/432 [==============================] - 0s 663us/step - loss: 0.6232 - acc: 0.6667 - val_loss: 0.9142 - val_acc: 0.1759\n",
      "Epoch 27/40\n",
      "432/432 [==============================] - 0s 601us/step - loss: 0.6229 - acc: 0.6667 - val_loss: 1.0390 - val_acc: 0.1019\n",
      "Epoch 28/40\n",
      "432/432 [==============================] - 0s 633us/step - loss: 0.6220 - acc: 0.6736 - val_loss: 1.0039 - val_acc: 0.1296\n",
      "Epoch 29/40\n",
      "432/432 [==============================] - 0s 632us/step - loss: 0.6205 - acc: 0.6644 - val_loss: 1.1011 - val_acc: 0.1019\n",
      "Epoch 30/40\n",
      "432/432 [==============================] - 0s 636us/step - loss: 0.6207 - acc: 0.6713 - val_loss: 0.9629 - val_acc: 0.1667\n",
      "Epoch 31/40\n",
      "432/432 [==============================] - 0s 636us/step - loss: 0.6187 - acc: 0.6736 - val_loss: 1.0686 - val_acc: 0.1111\n",
      "Epoch 32/40\n",
      "432/432 [==============================] - 0s 629us/step - loss: 0.6193 - acc: 0.6667 - val_loss: 0.9741 - val_acc: 0.1481\n",
      "Epoch 33/40\n",
      "432/432 [==============================] - 0s 629us/step - loss: 0.6175 - acc: 0.6667 - val_loss: 1.0526 - val_acc: 0.1111\n",
      "Epoch 34/40\n",
      "432/432 [==============================] - 0s 650us/step - loss: 0.6172 - acc: 0.6667 - val_loss: 0.9475 - val_acc: 0.1667\n",
      "Epoch 35/40\n",
      "432/432 [==============================] - 0s 631us/step - loss: 0.6186 - acc: 0.6690 - val_loss: 0.9867 - val_acc: 0.1574\n",
      "Epoch 36/40\n",
      "432/432 [==============================] - 0s 630us/step - loss: 0.6123 - acc: 0.6713 - val_loss: 0.8867 - val_acc: 0.2315\n",
      "Epoch 37/40\n",
      "432/432 [==============================] - 0s 631us/step - loss: 0.6149 - acc: 0.6713 - val_loss: 0.9446 - val_acc: 0.1759\n",
      "Epoch 38/40\n",
      "432/432 [==============================] - 0s 622us/step - loss: 0.6130 - acc: 0.6782 - val_loss: 0.9864 - val_acc: 0.1389\n",
      "Epoch 39/40\n",
      "432/432 [==============================] - 0s 651us/step - loss: 0.6110 - acc: 0.6736 - val_loss: 0.9379 - val_acc: 0.1852\n",
      "Epoch 40/40\n",
      "432/432 [==============================] - 0s 632us/step - loss: 0.6115 - acc: 0.6736 - val_loss: 0.9590 - val_acc: 0.1759\n",
      "60/60 [==============================] - 0s 32us/step\n",
      "Average accuracy of model on the dev set =  0.5666666687086777\n",
      "Training on fold 10/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_114 (Dense)            (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 432 samples, validate on 108 samples\n",
      "Epoch 1/40\n",
      "432/432 [==============================] - 2s 4ms/step - loss: 0.8270 - acc: 0.3773 - val_loss: 0.6364 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "432/432 [==============================] - 0s 683us/step - loss: 0.6781 - acc: 0.5810 - val_loss: 0.8974 - val_acc: 0.0000e+00\n",
      "Epoch 3/40\n",
      "432/432 [==============================] - 0s 655us/step - loss: 0.6507 - acc: 0.6250 - val_loss: 1.0135 - val_acc: 0.0000e+00\n",
      "Epoch 4/40\n",
      "432/432 [==============================] - 0s 644us/step - loss: 0.6477 - acc: 0.6227 - val_loss: 0.9362 - val_acc: 0.0000e+00\n",
      "Epoch 5/40\n",
      "432/432 [==============================] - 0s 673us/step - loss: 0.6435 - acc: 0.6343 - val_loss: 1.0141 - val_acc: 0.0000e+00\n",
      "Epoch 6/40\n",
      "432/432 [==============================] - 0s 603us/step - loss: 0.6414 - acc: 0.6366 - val_loss: 0.9717 - val_acc: 0.0926\n",
      "Epoch 7/40\n",
      "432/432 [==============================] - 0s 602us/step - loss: 0.6379 - acc: 0.6551 - val_loss: 1.0127 - val_acc: 0.0833\n",
      "Epoch 8/40\n",
      "432/432 [==============================] - 0s 656us/step - loss: 0.6384 - acc: 0.6574 - val_loss: 0.9340 - val_acc: 0.1481\n",
      "Epoch 9/40\n",
      "432/432 [==============================] - 0s 670us/step - loss: 0.6349 - acc: 0.6644 - val_loss: 0.9402 - val_acc: 0.1389\n",
      "Epoch 10/40\n",
      "432/432 [==============================] - 0s 655us/step - loss: 0.6332 - acc: 0.6667 - val_loss: 0.9399 - val_acc: 0.1481\n",
      "Epoch 11/40\n",
      "432/432 [==============================] - 0s 631us/step - loss: 0.6318 - acc: 0.6690 - val_loss: 0.9641 - val_acc: 0.1481\n",
      "Epoch 12/40\n",
      "432/432 [==============================] - 0s 670us/step - loss: 0.6293 - acc: 0.6713 - val_loss: 0.9407 - val_acc: 0.1481\n",
      "Epoch 13/40\n",
      "432/432 [==============================] - 0s 661us/step - loss: 0.6276 - acc: 0.6690 - val_loss: 0.9662 - val_acc: 0.1481\n",
      "Epoch 14/40\n",
      "432/432 [==============================] - 0s 653us/step - loss: 0.6266 - acc: 0.6713 - val_loss: 0.9466 - val_acc: 0.1481\n",
      "Epoch 15/40\n",
      "432/432 [==============================] - 0s 959us/step - loss: 0.6258 - acc: 0.6667 - val_loss: 0.8917 - val_acc: 0.1944\n",
      "Epoch 16/40\n",
      "432/432 [==============================] - 0s 876us/step - loss: 0.6230 - acc: 0.6690 - val_loss: 0.8976 - val_acc: 0.2037\n",
      "Epoch 17/40\n",
      "432/432 [==============================] - 0s 751us/step - loss: 0.6235 - acc: 0.6644 - val_loss: 0.9230 - val_acc: 0.1852\n",
      "Epoch 18/40\n",
      "432/432 [==============================] - 0s 732us/step - loss: 0.6192 - acc: 0.6782 - val_loss: 0.9908 - val_acc: 0.1667\n",
      "Epoch 19/40\n",
      "432/432 [==============================] - 0s 715us/step - loss: 0.6209 - acc: 0.6690 - val_loss: 0.9894 - val_acc: 0.1667\n",
      "Epoch 20/40\n",
      "432/432 [==============================] - 0s 730us/step - loss: 0.6199 - acc: 0.6667 - val_loss: 0.9381 - val_acc: 0.1759\n",
      "Epoch 21/40\n",
      "432/432 [==============================] - 0s 756us/step - loss: 0.6146 - acc: 0.6829 - val_loss: 1.0775 - val_acc: 0.1204\n",
      "Epoch 22/40\n",
      "432/432 [==============================] - 0s 729us/step - loss: 0.6203 - acc: 0.6736 - val_loss: 0.9851 - val_acc: 0.1759\n",
      "Epoch 23/40\n",
      "432/432 [==============================] - 0s 664us/step - loss: 0.6162 - acc: 0.6620 - val_loss: 1.0125 - val_acc: 0.1667\n",
      "Epoch 24/40\n",
      "432/432 [==============================] - 0s 687us/step - loss: 0.6149 - acc: 0.6736 - val_loss: 0.9969 - val_acc: 0.1667\n",
      "Epoch 25/40\n",
      "432/432 [==============================] - 0s 690us/step - loss: 0.6146 - acc: 0.6736 - val_loss: 0.9924 - val_acc: 0.1759\n",
      "Epoch 26/40\n",
      "432/432 [==============================] - 0s 667us/step - loss: 0.6135 - acc: 0.6620 - val_loss: 0.9918 - val_acc: 0.1944\n",
      "Epoch 27/40\n",
      "432/432 [==============================] - 0s 593us/step - loss: 0.6116 - acc: 0.6713 - val_loss: 0.9971 - val_acc: 0.1759\n",
      "Epoch 28/40\n",
      "432/432 [==============================] - 0s 673us/step - loss: 0.6083 - acc: 0.6736 - val_loss: 0.9892 - val_acc: 0.1944\n",
      "Epoch 29/40\n",
      "432/432 [==============================] - 0s 645us/step - loss: 0.6069 - acc: 0.6782 - val_loss: 1.0121 - val_acc: 0.1667\n",
      "Epoch 30/40\n",
      "432/432 [==============================] - 0s 663us/step - loss: 0.6108 - acc: 0.6667 - val_loss: 0.9420 - val_acc: 0.2315\n",
      "Epoch 31/40\n",
      "432/432 [==============================] - 0s 666us/step - loss: 0.6068 - acc: 0.6921 - val_loss: 1.0011 - val_acc: 0.1944\n",
      "Epoch 32/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "432/432 [==============================] - 0s 699us/step - loss: 0.6080 - acc: 0.6736 - val_loss: 1.0096 - val_acc: 0.1667\n",
      "Epoch 33/40\n",
      "432/432 [==============================] - 0s 638us/step - loss: 0.6025 - acc: 0.6667 - val_loss: 1.0022 - val_acc: 0.1944\n",
      "Epoch 34/40\n",
      "432/432 [==============================] - 0s 633us/step - loss: 0.6023 - acc: 0.6898 - val_loss: 1.1055 - val_acc: 0.1667\n",
      "Epoch 35/40\n",
      "432/432 [==============================] - 0s 631us/step - loss: 0.6026 - acc: 0.6852 - val_loss: 0.9711 - val_acc: 0.2037\n",
      "Epoch 36/40\n",
      "432/432 [==============================] - 0s 638us/step - loss: 0.6024 - acc: 0.6829 - val_loss: 0.9662 - val_acc: 0.2315\n",
      "Epoch 37/40\n",
      "432/432 [==============================] - 0s 633us/step - loss: 0.5975 - acc: 0.6782 - val_loss: 0.8270 - val_acc: 0.3519\n",
      "Epoch 38/40\n",
      "432/432 [==============================] - 0s 632us/step - loss: 0.6010 - acc: 0.6898 - val_loss: 1.0452 - val_acc: 0.1667\n",
      "Epoch 39/40\n",
      "432/432 [==============================] - 0s 641us/step - loss: 0.5994 - acc: 0.6875 - val_loss: 0.9999 - val_acc: 0.2500\n",
      "Epoch 40/40\n",
      "432/432 [==============================] - 0s 630us/step - loss: 0.5991 - acc: 0.6829 - val_loss: 0.9130 - val_acc: 0.2778\n",
      "60/60 [==============================] - 0s 36us/step\n",
      "Average accuracy of model on the dev set =  0.5550000019371509\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=12)\n",
    "avg_loss = []\n",
    "avg_acc = []\n",
    "# Loop through the indices the split() method returns\n",
    "for index, (train_index, test_index) in enumerate(skf.split(all_data, labels)):\n",
    "    print(\"Training on fold \" + str(index + 1) + \"/10.............................................\")\n",
    "    # Generate batches from indices\n",
    "    x_train, x_test = all_data[train_index], all_data[test_index]\n",
    "    # use one-hot vectors as labels\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    network = models.Sequential()\n",
    "    \n",
    "\n",
    "    network.add(layers.Dense(32, input_shape=(8,)))\n",
    "    network.add(layers.Dense(32, activation=\"relu\"))\n",
    "    network.add(layers.Dense(16, activation=\"relu\"))\n",
    "    # network.add(layers.Dropout(0.3))\n",
    "    network.add(layers.Dense(16, activation=\"relu\"))\n",
    "    # network.add(layers.Dropout(0.3))\n",
    "    network.add(layers.Dense(32, activation=\"sigmoid\"))\n",
    "    network.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Adam = Adam(lr=0.05)\n",
    "    network.compile(optimizer=Adam(lr=0.0004),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['acc'])\n",
    "\n",
    "    network.summary()\n",
    "\n",
    "    history = network.fit(x_train, y_train, validation_split=0.2,\n",
    "                          epochs=40, verbose=1, batch_size=3)\n",
    "\n",
    "    loss, accuracy = network.evaluate(x_test, y_test)\n",
    "\n",
    "    # evaluate and store the accuracy\n",
    "#     loss, accuracy = model.evaluate(xtest_imagelist, ytest, verbose=1)\n",
    "    avg_loss.append(loss)\n",
    "    avg_acc.append(accuracy)\n",
    "\n",
    "    # cross validation score\n",
    "    print(\"Average accuracy of model on the dev set = \", np.mean(avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
