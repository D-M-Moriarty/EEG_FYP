{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn import preprocessing\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "rest = pd.read_csv(\"../../data_files/data_from_android_api/rest/rest_25_mins.csv\")\n",
    "\n",
    "up = pd.read_csv(\"../../data_files/data_from_android_api/up_down_still/up5mins_still.csv\")\n",
    "down = pd.read_csv(\"../../data_files/data_from_android_api/up_down_still/down5mins_still.csv\")\n",
    "\n",
    "\n",
    "dataDF = pd.concat([up, down])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "u = [up.delta.mean(), up.theta.mean(), up.alphaLow.mean(), \n",
    "     up.betaHigh.mean(), up.betaLow.mean(), up.alphaHigh.mean(), \n",
    "     up.gammaLow.mean(), up.gammaMid.mean()]\n",
    "\n",
    "d = [down.delta.mean(), down.theta.mean(), down.alphaLow.mean(), \n",
    "     down.betaHigh.mean(), down.betaLow.mean(), down.alphaHigh.mean(), \n",
    "     down.gammaLow.mean(), down.gammaMid.mean()]\n",
    "\n",
    "\n",
    "index = ['delta', 'theta', 'alphaLow','alphaHigh', 'betaLow', 'betaHigh', 'gammaLow', 'gammaMid']\n",
    "\n",
    "df = pd.DataFrame({'up': u, 'down': d}, index=index)\n",
    "ax = df.plot.bar(rot=0, figsize=(20,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# create an array of shape 30706, 9 = number of records by the features\n",
    "data = np.array([[0 for x in range(8)] for y in range(len(dataDF))])\n",
    "for i in range(len(dataDF)):\n",
    "    data[i] = [dataDF.delta.values[i],\n",
    "                       dataDF.theta.values[i],\n",
    "                       dataDF.alphaLow.values[i],\n",
    "                       dataDF.alphaHigh.values[i],\n",
    "                       dataDF.betaLow.values[i],\n",
    "                       dataDF.betaHigh.values[i],\n",
    "                       dataDF.gammaLow.values[i],\n",
    "                       dataDF.gammaMid.values[i]]\n",
    "    \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "labels = encoder.fit_transform(dataDF.action.values)\n",
    "\n",
    "# creating training and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(data, labels)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "stan_scaler = StandardScaler()\n",
    "\n",
    "x_train = stan_scaler.fit_transform(x_train)\n",
    "x_test = stan_scaler.transform(x_test)\n",
    "\n",
    "all_data = dataDF.drop(['action'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:8: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12600197 0.12819726 0.11712773 0.09682568 0.10940296 0.147043\n",
      " 0.14410301 0.1312984 ]\n",
      "The score for Random Forest  0.43231441048034935\n",
      "684\n",
      "Accuracy for x_test: 0.43231441048034935\n",
      "Cross Validation Accuracy: 0.37 (+/- 0.57)\n",
      "[0.34408602 0.         0.01098901 0.02197802 0.08791209 0.63736264\n",
      " 0.63736264 0.65934066 0.63736264 0.61538462]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/model_selection/_validation.py:528: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  estimator.fit(X_train, y_train, **fit_params)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Random Forrest\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(x_train, y_train)\n",
    "\n",
    "print(rfc.feature_importances_)\n",
    "\n",
    "print(\"The score for Random Forest \", rfc.score(x_test, y_test))\n",
    "y_pred = rfc.predict(x_test)\n",
    "#Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "# Model Accuracy, how often is the classifier correct?\n",
    "print(len(y_train))\n",
    "print(\"Accuracy for x_test:\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "scores = cross_val_score(rfc, all_data, labels, cv=10, scoring='accuracy')\n",
    "print(\"Cross Validation Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n",
      "The score for XGBoost  0.5109170305676856\n",
      "Accuracy for x_test: 0.5109170305676856\n",
      "Accuracy: 51.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:219: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:252: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross Validation Accuracy: 0.52 (+/- 0.34)\n",
      "[0.51612903 0.26086957 0.30769231 0.30769231 0.40659341 0.67032967\n",
      " 0.67032967 0.67032967 0.67032967 0.67032967]\n"
     ]
    }
   ],
   "source": [
    "# XGBoost\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "xgb.fit(x_train, y_train)\n",
    "print(xgb)\n",
    "print(\"The score for XGBoost \", xgb.score(x_test, y_test))\n",
    "y_pred = xgb.predict(x_test)\n",
    "\n",
    "print(\"Accuracy for x_test:\", metrics.accuracy_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "predictions = [round(value) for value in y_pred]\n",
    "# evaluate predictions\n",
    "accuracy = metrics.accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n",
    "\n",
    "scores = cross_val_score(xgb, all_data, labels, cv=10, scoring='accuracy')\n",
    "print(\"Cross Validation Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 1)                 9         \n",
      "=================================================================\n",
      "Total params: 9\n",
      "Trainable params: 9\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/40\n",
      "684/684 [==============================] - 0s 404us/step - loss: 0.7104 - acc: 0.5439\n",
      "Epoch 2/40\n",
      "684/684 [==============================] - 0s 130us/step - loss: 0.6932 - acc: 0.5775\n",
      "Epoch 3/40\n",
      "684/684 [==============================] - 0s 147us/step - loss: 0.6805 - acc: 0.6067\n",
      "Epoch 4/40\n",
      "684/684 [==============================] - 0s 143us/step - loss: 0.6707 - acc: 0.6213\n",
      "Epoch 5/40\n",
      "684/684 [==============================] - 0s 156us/step - loss: 0.6636 - acc: 0.6272\n",
      "Epoch 6/40\n",
      "684/684 [==============================] - 0s 145us/step - loss: 0.6582 - acc: 0.6272\n",
      "Epoch 7/40\n",
      "684/684 [==============================] - 0s 142us/step - loss: 0.6542 - acc: 0.6491\n",
      "Epoch 8/40\n",
      "684/684 [==============================] - 0s 145us/step - loss: 0.6510 - acc: 0.6520\n",
      "Epoch 9/40\n",
      "684/684 [==============================] - 0s 143us/step - loss: 0.6486 - acc: 0.6550\n",
      "Epoch 10/40\n",
      "684/684 [==============================] - 0s 146us/step - loss: 0.6467 - acc: 0.6564\n",
      "Epoch 11/40\n",
      "684/684 [==============================] - 0s 174us/step - loss: 0.6450 - acc: 0.6564\n",
      "Epoch 12/40\n",
      "684/684 [==============================] - 0s 171us/step - loss: 0.6438 - acc: 0.6637\n",
      "Epoch 13/40\n",
      "684/684 [==============================] - 0s 156us/step - loss: 0.6426 - acc: 0.6667\n",
      "Epoch 14/40\n",
      "684/684 [==============================] - 0s 148us/step - loss: 0.6416 - acc: 0.6667\n",
      "Epoch 15/40\n",
      "684/684 [==============================] - 0s 157us/step - loss: 0.6409 - acc: 0.6667\n",
      "Epoch 16/40\n",
      "684/684 [==============================] - 0s 171us/step - loss: 0.6401 - acc: 0.6681\n",
      "Epoch 17/40\n",
      "684/684 [==============================] - 0s 180us/step - loss: 0.6394 - acc: 0.6711\n",
      "Epoch 18/40\n",
      "684/684 [==============================] - 0s 159us/step - loss: 0.6388 - acc: 0.6725\n",
      "Epoch 19/40\n",
      "684/684 [==============================] - 0s 153us/step - loss: 0.6384 - acc: 0.6725\n",
      "Epoch 20/40\n",
      "684/684 [==============================] - 0s 149us/step - loss: 0.6378 - acc: 0.6725\n",
      "Epoch 21/40\n",
      "684/684 [==============================] - 0s 148us/step - loss: 0.6375 - acc: 0.6725\n",
      "Epoch 22/40\n",
      "684/684 [==============================] - 0s 145us/step - loss: 0.6370 - acc: 0.6725\n",
      "Epoch 23/40\n",
      "684/684 [==============================] - 0s 159us/step - loss: 0.6367 - acc: 0.6725\n",
      "Epoch 24/40\n",
      "684/684 [==============================] - 0s 171us/step - loss: 0.6364 - acc: 0.6696\n",
      "Epoch 25/40\n",
      "684/684 [==============================] - 0s 159us/step - loss: 0.6361 - acc: 0.6711\n",
      "Epoch 26/40\n",
      "684/684 [==============================] - 0s 147us/step - loss: 0.6358 - acc: 0.6711\n",
      "Epoch 27/40\n",
      "684/684 [==============================] - 0s 154us/step - loss: 0.6356 - acc: 0.6711\n",
      "Epoch 28/40\n",
      "684/684 [==============================] - 0s 154us/step - loss: 0.6352 - acc: 0.6711\n",
      "Epoch 29/40\n",
      "684/684 [==============================] - 0s 152us/step - loss: 0.6350 - acc: 0.6711\n",
      "Epoch 30/40\n",
      "684/684 [==============================] - 0s 147us/step - loss: 0.6349 - acc: 0.6711\n",
      "Epoch 31/40\n",
      "684/684 [==============================] - 0s 152us/step - loss: 0.6347 - acc: 0.6711\n",
      "Epoch 32/40\n",
      "684/684 [==============================] - 0s 152us/step - loss: 0.6345 - acc: 0.6711\n",
      "Epoch 33/40\n",
      "684/684 [==============================] - 0s 156us/step - loss: 0.6343 - acc: 0.6711\n",
      "Epoch 34/40\n",
      "684/684 [==============================] - 0s 151us/step - loss: 0.6344 - acc: 0.6711\n",
      "Epoch 35/40\n",
      "684/684 [==============================] - 0s 154us/step - loss: 0.6340 - acc: 0.6711\n",
      "Epoch 36/40\n",
      "684/684 [==============================] - 0s 146us/step - loss: 0.6340 - acc: 0.6711\n",
      "Epoch 37/40\n",
      "684/684 [==============================] - 0s 147us/step - loss: 0.6338 - acc: 0.6711\n",
      "Epoch 38/40\n",
      "684/684 [==============================] - 0s 153us/step - loss: 0.6337 - acc: 0.6711\n",
      "Epoch 39/40\n",
      "684/684 [==============================] - 0s 152us/step - loss: 0.6336 - acc: 0.6711\n",
      "Epoch 40/40\n",
      "684/684 [==============================] - 0s 153us/step - loss: 0.6335 - acc: 0.6711\n",
      "229/229 [==============================] - 0s 145us/step\n",
      "loss and metrics [0.6356973065038956, 0.6681222712629227]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "stan_scaler = StandardScaler()\n",
    "\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)\n",
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "# from keras import regularizers kernel_regularizer=regularizers.l2(0.01), \n",
    "from keras.optimizers import Adam\n",
    "\n",
    "network = models.Sequential()\n",
    "\n",
    "network.add(layers.Dense(1, input_shape=(8,), activation='sigmoid'))\n",
    "\n",
    "# Adam = Adam(lr=0.05)\n",
    "network.compile(optimizer='Adam',\n",
    "                loss='binary_crossentropy',\n",
    "                metrics=['acc'])\n",
    "\n",
    "network.summary()\n",
    "\n",
    "history = network.fit(x_train, y_train,\n",
    "                      epochs=40, verbose=1, batch_size=5)\n",
    "\n",
    "loss_and_metrics = network.evaluate(x_test, y_test)\n",
    "print('loss and metrics', loss_and_metrics)\n",
    "\n",
    "# print('prediction: ', network.predict(test_data))\n",
    "\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/darrenmoriarty/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:595: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "all_labels = dataDF.action.values\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "all_labels = encoder.fit_transform(all_labels)\n",
    "    \n",
    "# create an array of shape 30706, 9 = number of records by the features\n",
    "all_data = np.array([[0 for x in range(8)] for y in range(len(dataDF))])\n",
    "for i in range(len(dataDF)):\n",
    "    all_data[i] = [dataDF.delta.values[i],\n",
    "                       dataDF.theta.values[i],\n",
    "                       dataDF.alphaLow.values[i],\n",
    "                       dataDF.alphaHigh.values[i],\n",
    "                       dataDF.betaLow.values[i],\n",
    "                       dataDF.betaHigh.values[i],\n",
    "                       dataDF.gammaLow.values[i],\n",
    "                       dataDF.gammaMid.values[i]]\n",
    "    \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "scaler = MinMaxScaler()\n",
    "stan_scaler = StandardScaler()\n",
    "\n",
    "all_data = scaler.fit_transform(all_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_51 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_52 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_53 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 656 samples, validate on 164 samples\n",
      "Epoch 1/40\n",
      "656/656 [==============================] - 1s 2ms/step - loss: 0.6811 - acc: 0.5884 - val_loss: 0.5104 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "656/656 [==============================] - 0s 521us/step - loss: 0.6784 - acc: 0.5884 - val_loss: 0.5202 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "656/656 [==============================] - 0s 523us/step - loss: 0.6782 - acc: 0.5884 - val_loss: 0.5342 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "656/656 [==============================] - 0s 600us/step - loss: 0.6783 - acc: 0.5884 - val_loss: 0.5121 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "656/656 [==============================] - 0s 629us/step - loss: 0.6778 - acc: 0.5884 - val_loss: 0.5167 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "656/656 [==============================] - 0s 501us/step - loss: 0.6783 - acc: 0.5884 - val_loss: 0.5156 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "656/656 [==============================] - 0s 519us/step - loss: 0.6781 - acc: 0.5884 - val_loss: 0.5328 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "656/656 [==============================] - 0s 650us/step - loss: 0.6776 - acc: 0.5884 - val_loss: 0.5232 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "656/656 [==============================] - 0s 512us/step - loss: 0.6783 - acc: 0.5884 - val_loss: 0.5243 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "656/656 [==============================] - 0s 516us/step - loss: 0.6777 - acc: 0.5884 - val_loss: 0.5297 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "656/656 [==============================] - 0s 518us/step - loss: 0.6775 - acc: 0.5884 - val_loss: 0.5301 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "656/656 [==============================] - 0s 523us/step - loss: 0.6775 - acc: 0.5884 - val_loss: 0.5197 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "656/656 [==============================] - 0s 495us/step - loss: 0.6771 - acc: 0.5884 - val_loss: 0.5430 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "656/656 [==============================] - 0s 494us/step - loss: 0.6776 - acc: 0.5884 - val_loss: 0.5388 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "656/656 [==============================] - 0s 500us/step - loss: 0.6768 - acc: 0.5884 - val_loss: 0.5123 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "656/656 [==============================] - 0s 505us/step - loss: 0.6771 - acc: 0.5884 - val_loss: 0.5264 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "656/656 [==============================] - 0s 540us/step - loss: 0.6771 - acc: 0.5884 - val_loss: 0.5314 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "656/656 [==============================] - 0s 495us/step - loss: 0.6775 - acc: 0.5884 - val_loss: 0.5227 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "656/656 [==============================] - 0s 497us/step - loss: 0.6764 - acc: 0.5884 - val_loss: 0.5160 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "656/656 [==============================] - 0s 510us/step - loss: 0.6766 - acc: 0.5884 - val_loss: 0.5250 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "656/656 [==============================] - 0s 518us/step - loss: 0.6768 - acc: 0.5884 - val_loss: 0.5230 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "656/656 [==============================] - 0s 517us/step - loss: 0.6765 - acc: 0.5884 - val_loss: 0.5349 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "656/656 [==============================] - 0s 538us/step - loss: 0.6764 - acc: 0.5884 - val_loss: 0.5350 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "656/656 [==============================] - 0s 520us/step - loss: 0.6778 - acc: 0.5884 - val_loss: 0.5266 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "656/656 [==============================] - 0s 526us/step - loss: 0.6766 - acc: 0.5884 - val_loss: 0.5457 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "656/656 [==============================] - 0s 528us/step - loss: 0.6770 - acc: 0.5884 - val_loss: 0.5348 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "656/656 [==============================] - 0s 534us/step - loss: 0.6764 - acc: 0.5884 - val_loss: 0.5350 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "656/656 [==============================] - 0s 533us/step - loss: 0.6769 - acc: 0.5869 - val_loss: 0.5361 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "656/656 [==============================] - 0s 523us/step - loss: 0.6762 - acc: 0.5884 - val_loss: 0.5236 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "656/656 [==============================] - 0s 552us/step - loss: 0.6765 - acc: 0.5884 - val_loss: 0.5389 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "656/656 [==============================] - 0s 501us/step - loss: 0.6760 - acc: 0.5884 - val_loss: 0.5323 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "656/656 [==============================] - 0s 538us/step - loss: 0.6761 - acc: 0.5899 - val_loss: 0.5230 - val_acc: 1.0000\n",
      "Epoch 33/40\n",
      "656/656 [==============================] - 0s 536us/step - loss: 0.6762 - acc: 0.5884 - val_loss: 0.5434 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "656/656 [==============================] - 0s 526us/step - loss: 0.6761 - acc: 0.5854 - val_loss: 0.5395 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "656/656 [==============================] - 0s 539us/step - loss: 0.6760 - acc: 0.5899 - val_loss: 0.5249 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "656/656 [==============================] - 0s 512us/step - loss: 0.6762 - acc: 0.5899 - val_loss: 0.5424 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "656/656 [==============================] - 0s 512us/step - loss: 0.6763 - acc: 0.5884 - val_loss: 0.5417 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "656/656 [==============================] - 0s 494us/step - loss: 0.6756 - acc: 0.5884 - val_loss: 0.5241 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "656/656 [==============================] - 0s 510us/step - loss: 0.6762 - acc: 0.5884 - val_loss: 0.5299 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "656/656 [==============================] - 0s 530us/step - loss: 0.6762 - acc: 0.5884 - val_loss: 0.5334 - val_acc: 1.0000\n",
      "93/93 [==============================] - 0s 24us/step\n",
      "Average accuracy of model on the dev set =  0.6559139727264323\n",
      "Training on fold 2/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_57 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 656 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "656/656 [==============================] - 1s 2ms/step - loss: 0.6936 - acc: 0.5869 - val_loss: 0.4812 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "656/656 [==============================] - 0s 508us/step - loss: 0.6791 - acc: 0.5869 - val_loss: 0.5321 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "656/656 [==============================] - 0s 500us/step - loss: 0.6780 - acc: 0.5869 - val_loss: 0.5498 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "656/656 [==============================] - 0s 523us/step - loss: 0.6773 - acc: 0.5869 - val_loss: 0.5394 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "656/656 [==============================] - 0s 491us/step - loss: 0.6776 - acc: 0.5869 - val_loss: 0.5512 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "656/656 [==============================] - 0s 501us/step - loss: 0.6775 - acc: 0.5869 - val_loss: 0.5408 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "656/656 [==============================] - 0s 495us/step - loss: 0.6775 - acc: 0.5869 - val_loss: 0.5274 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "656/656 [==============================] - 0s 497us/step - loss: 0.6767 - acc: 0.5869 - val_loss: 0.5672 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "656/656 [==============================] - 0s 518us/step - loss: 0.6763 - acc: 0.5869 - val_loss: 0.5471 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "656/656 [==============================] - 0s 499us/step - loss: 0.6766 - acc: 0.5869 - val_loss: 0.5319 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "656/656 [==============================] - 0s 500us/step - loss: 0.6764 - acc: 0.5869 - val_loss: 0.5425 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "656/656 [==============================] - 0s 507us/step - loss: 0.6762 - acc: 0.5869 - val_loss: 0.5424 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "656/656 [==============================] - 0s 502us/step - loss: 0.6762 - acc: 0.5869 - val_loss: 0.5456 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "656/656 [==============================] - 0s 571us/step - loss: 0.6767 - acc: 0.5869 - val_loss: 0.5392 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "656/656 [==============================] - 0s 498us/step - loss: 0.6761 - acc: 0.5869 - val_loss: 0.5327 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "656/656 [==============================] - 0s 503us/step - loss: 0.6759 - acc: 0.5869 - val_loss: 0.5385 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "656/656 [==============================] - 0s 510us/step - loss: 0.6759 - acc: 0.5869 - val_loss: 0.5520 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "656/656 [==============================] - 0s 502us/step - loss: 0.6760 - acc: 0.5884 - val_loss: 0.5367 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "656/656 [==============================] - 0s 503us/step - loss: 0.6761 - acc: 0.5884 - val_loss: 0.5430 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "656/656 [==============================] - 0s 517us/step - loss: 0.6757 - acc: 0.5869 - val_loss: 0.5394 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "656/656 [==============================] - 0s 550us/step - loss: 0.6761 - acc: 0.5884 - val_loss: 0.5688 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "656/656 [==============================] - 0s 558us/step - loss: 0.6758 - acc: 0.5869 - val_loss: 0.5403 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "656/656 [==============================] - 0s 505us/step - loss: 0.6757 - acc: 0.5884 - val_loss: 0.5521 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "656/656 [==============================] - 0s 502us/step - loss: 0.6754 - acc: 0.5884 - val_loss: 0.5524 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "656/656 [==============================] - 0s 508us/step - loss: 0.6758 - acc: 0.5884 - val_loss: 0.5520 - val_acc: 0.9939\n",
      "Epoch 26/40\n",
      "656/656 [==============================] - 0s 511us/step - loss: 0.6756 - acc: 0.5854 - val_loss: 0.5386 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "656/656 [==============================] - 0s 516us/step - loss: 0.6756 - acc: 0.5884 - val_loss: 0.5418 - val_acc: 0.9939\n",
      "Epoch 28/40\n",
      "656/656 [==============================] - 0s 541us/step - loss: 0.6756 - acc: 0.5854 - val_loss: 0.5362 - val_acc: 0.9939\n",
      "Epoch 29/40\n",
      "656/656 [==============================] - 0s 515us/step - loss: 0.6755 - acc: 0.5884 - val_loss: 0.5511 - val_acc: 0.9939\n",
      "Epoch 30/40\n",
      "656/656 [==============================] - 0s 517us/step - loss: 0.6762 - acc: 0.5884 - val_loss: 0.5590 - val_acc: 0.9939\n",
      "Epoch 31/40\n",
      "656/656 [==============================] - 0s 517us/step - loss: 0.6753 - acc: 0.5854 - val_loss: 0.5588 - val_acc: 0.9939\n",
      "Epoch 32/40\n",
      "656/656 [==============================] - 0s 545us/step - loss: 0.6756 - acc: 0.5884 - val_loss: 0.5539 - val_acc: 0.9939\n",
      "Epoch 33/40\n",
      "656/656 [==============================] - 0s 512us/step - loss: 0.6748 - acc: 0.5899 - val_loss: 0.5517 - val_acc: 0.9939\n",
      "Epoch 34/40\n",
      "656/656 [==============================] - 0s 527us/step - loss: 0.6749 - acc: 0.5884 - val_loss: 0.5433 - val_acc: 0.9939\n",
      "Epoch 35/40\n",
      "656/656 [==============================] - 0s 547us/step - loss: 0.6751 - acc: 0.5884 - val_loss: 0.5391 - val_acc: 0.9939\n",
      "Epoch 36/40\n",
      "656/656 [==============================] - 0s 526us/step - loss: 0.6751 - acc: 0.5869 - val_loss: 0.5441 - val_acc: 0.9939\n",
      "Epoch 37/40\n",
      "656/656 [==============================] - 0s 543us/step - loss: 0.6748 - acc: 0.5899 - val_loss: 0.5440 - val_acc: 0.9939\n",
      "Epoch 38/40\n",
      "656/656 [==============================] - 0s 534us/step - loss: 0.6752 - acc: 0.5899 - val_loss: 0.5615 - val_acc: 0.9091\n",
      "Epoch 39/40\n",
      "656/656 [==============================] - 0s 501us/step - loss: 0.6747 - acc: 0.5884 - val_loss: 0.5463 - val_acc: 0.9939\n",
      "Epoch 40/40\n",
      "656/656 [==============================] - 0s 519us/step - loss: 0.6756 - acc: 0.5915 - val_loss: 0.5422 - val_acc: 0.9939\n",
      "92/92 [==============================] - 0s 25us/step\n",
      "Average accuracy of model on the dev set =  0.6540439454764615\n",
      "Training on fold 3/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_68 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5203 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 520us/step - loss: 0.6786 - acc: 0.5875 - val_loss: 0.5475 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 528us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5496 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 523us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5361 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 528us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5425 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 528us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5267 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 527us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5164 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 512us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5315 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 503us/step - loss: 0.6777 - acc: 0.5875 - val_loss: 0.5179 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 519us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5244 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 524us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5389 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 531us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5238 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 520us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.4999 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 515us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5222 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 517us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5400 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 509us/step - loss: 0.6771 - acc: 0.5875 - val_loss: 0.5239 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 515us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5461 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 518us/step - loss: 0.6773 - acc: 0.5875 - val_loss: 0.5269 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 516us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5397 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 512us/step - loss: 0.6766 - acc: 0.5875 - val_loss: 0.5438 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 517us/step - loss: 0.6773 - acc: 0.5875 - val_loss: 0.5264 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 504us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5335 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 521us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5186 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 554us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5200 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 612us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5143 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 584us/step - loss: 0.6764 - acc: 0.5875 - val_loss: 0.5367 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 670us/step - loss: 0.6773 - acc: 0.5875 - val_loss: 0.5272 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 559us/step - loss: 0.6758 - acc: 0.5875 - val_loss: 0.5409 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 565us/step - loss: 0.6766 - acc: 0.5875 - val_loss: 0.5253 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 559us/step - loss: 0.6764 - acc: 0.5875 - val_loss: 0.4947 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 572us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5083 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 574us/step - loss: 0.6762 - acc: 0.5875 - val_loss: 0.5329 - val_acc: 1.0000\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 564us/step - loss: 0.6759 - acc: 0.5875 - val_loss: 0.5246 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 565us/step - loss: 0.6758 - acc: 0.5875 - val_loss: 0.4966 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 559us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5185 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 594us/step - loss: 0.6764 - acc: 0.5875 - val_loss: 0.5390 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6764 - acc: 0.5875 - val_loss: 0.5157 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 561us/step - loss: 0.6764 - acc: 0.5875 - val_loss: 0.5417 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 545us/step - loss: 0.6759 - acc: 0.5875 - val_loss: 0.5533 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 520us/step - loss: 0.6755 - acc: 0.5875 - val_loss: 0.5079 - val_acc: 1.0000\n",
      "91/91 [==============================] - 0s 26us/step\n",
      "Average accuracy of model on the dev set =  0.659472520427531\n",
      "Training on fold 4/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_69 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_70 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_71 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_72 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_73 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_74 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.6800 - acc: 0.5875 - val_loss: 0.5187 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 563us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5078 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 559us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5435 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 517us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5508 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 530us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5249 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 564us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5229 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 558us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5487 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 559us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5377 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 576us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5310 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 563us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5410 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 607us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5352 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 619us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5220 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 597us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5379 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 573us/step - loss: 0.6762 - acc: 0.5875 - val_loss: 0.5228 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 600us/step - loss: 0.6761 - acc: 0.5875 - val_loss: 0.5094 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 636us/step - loss: 0.6765 - acc: 0.5875 - val_loss: 0.5137 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 657us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5305 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 672us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5247 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 604us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5283 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 566us/step - loss: 0.6756 - acc: 0.5875 - val_loss: 0.5450 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 567us/step - loss: 0.6752 - acc: 0.5875 - val_loss: 0.5201 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 534us/step - loss: 0.6756 - acc: 0.5875 - val_loss: 0.5258 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 556us/step - loss: 0.6758 - acc: 0.5875 - val_loss: 0.5240 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 536us/step - loss: 0.6752 - acc: 0.5906 - val_loss: 0.5041 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 545us/step - loss: 0.6758 - acc: 0.5890 - val_loss: 0.5303 - val_acc: 0.9697\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 562us/step - loss: 0.6745 - acc: 0.5921 - val_loss: 0.5175 - val_acc: 0.9697\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 538us/step - loss: 0.6749 - acc: 0.5906 - val_loss: 0.5184 - val_acc: 0.9697\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 556us/step - loss: 0.6747 - acc: 0.5921 - val_loss: 0.5283 - val_acc: 0.9697\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 527us/step - loss: 0.6752 - acc: 0.5921 - val_loss: 0.5423 - val_acc: 0.9697\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6745 - acc: 0.5921 - val_loss: 0.5485 - val_acc: 0.9697\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 548us/step - loss: 0.6744 - acc: 0.5921 - val_loss: 0.5476 - val_acc: 0.9697\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 563us/step - loss: 0.6744 - acc: 0.5921 - val_loss: 0.5702 - val_acc: 0.9697\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 571us/step - loss: 0.6744 - acc: 0.5921 - val_loss: 0.5404 - val_acc: 0.9697\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 552us/step - loss: 0.6737 - acc: 0.5921 - val_loss: 0.5529 - val_acc: 0.9697\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 549us/step - loss: 0.6738 - acc: 0.5921 - val_loss: 0.5405 - val_acc: 0.9697\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 532us/step - loss: 0.6739 - acc: 0.5921 - val_loss: 0.5420 - val_acc: 0.9697\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 532us/step - loss: 0.6739 - acc: 0.5921 - val_loss: 0.5228 - val_acc: 0.9697\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 557us/step - loss: 0.6732 - acc: 0.5921 - val_loss: 0.5363 - val_acc: 0.9697\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 613us/step - loss: 0.6736 - acc: 0.5921 - val_loss: 0.5497 - val_acc: 0.9697\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 581us/step - loss: 0.6730 - acc: 0.5921 - val_loss: 0.5316 - val_acc: 0.9697\n",
      "91/91 [==============================] - 0s 34us/step\n",
      "Average accuracy of model on the dev set =  0.6539450512987979\n",
      "Training on fold 5/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_75 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 1s 2ms/step - loss: 0.6969 - acc: 0.5145 - val_loss: 0.5913 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 562us/step - loss: 0.6800 - acc: 0.5875 - val_loss: 0.5570 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 584us/step - loss: 0.6795 - acc: 0.5875 - val_loss: 0.5575 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 647us/step - loss: 0.6790 - acc: 0.5875 - val_loss: 0.5427 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 570us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5260 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 557us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5178 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 592us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5225 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 503us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5542 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 589us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5408 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 552us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5475 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 541us/step - loss: 0.6770 - acc: 0.5875 - val_loss: 0.5326 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 563us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5535 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 558us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5347 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 593us/step - loss: 0.6770 - acc: 0.5875 - val_loss: 0.5413 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 644us/step - loss: 0.6777 - acc: 0.5875 - val_loss: 0.5455 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 598us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5459 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 562us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5545 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 552us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5525 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 531us/step - loss: 0.6766 - acc: 0.5875 - val_loss: 0.5081 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 571us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5477 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 554us/step - loss: 0.6762 - acc: 0.5875 - val_loss: 0.5447 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 548us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5319 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 532us/step - loss: 0.6761 - acc: 0.5875 - val_loss: 0.5392 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 567us/step - loss: 0.6764 - acc: 0.5860 - val_loss: 0.5367 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 553us/step - loss: 0.6758 - acc: 0.5875 - val_loss: 0.5612 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 606us/step - loss: 0.6758 - acc: 0.5845 - val_loss: 0.5259 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 610us/step - loss: 0.6763 - acc: 0.5906 - val_loss: 0.5573 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 604us/step - loss: 0.6766 - acc: 0.5906 - val_loss: 0.5500 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 560us/step - loss: 0.6769 - acc: 0.5860 - val_loss: 0.5620 - val_acc: 0.9758\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 584us/step - loss: 0.6759 - acc: 0.5875 - val_loss: 0.5347 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 574us/step - loss: 0.6763 - acc: 0.5860 - val_loss: 0.5481 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 564us/step - loss: 0.6759 - acc: 0.5906 - val_loss: 0.5693 - val_acc: 0.9697\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 605us/step - loss: 0.6760 - acc: 0.5845 - val_loss: 0.5470 - val_acc: 0.9758\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 567us/step - loss: 0.6758 - acc: 0.5845 - val_loss: 0.5396 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 565us/step - loss: 0.6766 - acc: 0.5845 - val_loss: 0.5399 - val_acc: 0.9758\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 619us/step - loss: 0.6756 - acc: 0.5860 - val_loss: 0.5482 - val_acc: 0.9758\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 618us/step - loss: 0.6764 - acc: 0.5860 - val_loss: 0.5583 - val_acc: 0.9697\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 573us/step - loss: 0.6765 - acc: 0.5906 - val_loss: 0.5716 - val_acc: 0.9697\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 604us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5557 - val_acc: 0.9697\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 531us/step - loss: 0.6778 - acc: 0.5860 - val_loss: 0.5506 - val_acc: 0.9697\n",
      "91/91 [==============================] - 0s 26us/step\n",
      "Average accuracy of model on the dev set =  0.6550241729071702\n",
      "Training on fold 6/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_81 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_82 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_83 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_84 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_85 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_86 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.7004 - acc: 0.5875 - val_loss: 0.4717 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 560us/step - loss: 0.6807 - acc: 0.5875 - val_loss: 0.5057 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 580us/step - loss: 0.6793 - acc: 0.5875 - val_loss: 0.5378 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 579us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5320 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 567us/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5177 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5407 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5362 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 558us/step - loss: 0.6786 - acc: 0.5875 - val_loss: 0.5453 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 536us/step - loss: 0.6790 - acc: 0.5875 - val_loss: 0.5479 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 538us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5422 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 542us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5442 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 560us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5572 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 552us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5270 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 536us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5639 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 558us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5460 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 567us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5440 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 549us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5618 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 537us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5652 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 557us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5269 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 556us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5354 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 554us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5215 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 561us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5250 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 580us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5397 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 575us/step - loss: 0.6775 - acc: 0.5860 - val_loss: 0.5282 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5256 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 562us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5347 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 561us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5091 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 587us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5215 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 552us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5679 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 548us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5456 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 543us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5298 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 550us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5278 - val_acc: 1.0000\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 547us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5183 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 556us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5457 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 560us/step - loss: 0.6774 - acc: 0.5860 - val_loss: 0.5299 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 559us/step - loss: 0.6770 - acc: 0.5875 - val_loss: 0.5387 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 545us/step - loss: 0.6773 - acc: 0.5860 - val_loss: 0.5490 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 541us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5474 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 543us/step - loss: 0.6765 - acc: 0.5875 - val_loss: 0.5626 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 563us/step - loss: 0.6770 - acc: 0.5875 - val_loss: 0.5429 - val_acc: 1.0000\n",
      "91/91 [==============================] - 0s 31us/step\n",
      "Average accuracy of model on the dev set =  0.6575750891442534\n",
      "Training on fold 7/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_87 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_88 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_89 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_90 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_91 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 2s 2ms/step - loss: 0.6794 - acc: 0.5875 - val_loss: 0.5365 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5283 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 538us/step - loss: 0.6789 - acc: 0.5875 - val_loss: 0.5171 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 549us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5074 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 600us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5232 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 575us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5192 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 634us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5339 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 596us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5347 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 544us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5265 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 554us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5163 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 588us/step - loss: 0.6765 - acc: 0.5875 - val_loss: 0.5510 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 576us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5360 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 572us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5533 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 573us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5633 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 577us/step - loss: 0.6775 - acc: 0.5830 - val_loss: 0.5427 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 587us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5385 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 575us/step - loss: 0.6765 - acc: 0.5860 - val_loss: 0.5318 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 577us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5362 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 584us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5509 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 568us/step - loss: 0.6771 - acc: 0.5875 - val_loss: 0.5615 - val_acc: 0.9576\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 571us/step - loss: 0.6770 - acc: 0.5860 - val_loss: 0.5440 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 577us/step - loss: 0.6761 - acc: 0.5845 - val_loss: 0.5280 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 578us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5247 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 569us/step - loss: 0.6762 - acc: 0.5875 - val_loss: 0.5341 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 588us/step - loss: 0.6766 - acc: 0.5860 - val_loss: 0.5325 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 583us/step - loss: 0.6759 - acc: 0.5890 - val_loss: 0.5316 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 575us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5321 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 565us/step - loss: 0.6763 - acc: 0.5890 - val_loss: 0.5323 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 573us/step - loss: 0.6767 - acc: 0.5860 - val_loss: 0.5416 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 586us/step - loss: 0.6764 - acc: 0.5845 - val_loss: 0.5448 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 583us/step - loss: 0.6760 - acc: 0.5845 - val_loss: 0.5285 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 594us/step - loss: 0.6759 - acc: 0.5875 - val_loss: 0.5284 - val_acc: 1.0000\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 584us/step - loss: 0.6764 - acc: 0.5860 - val_loss: 0.5342 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 585us/step - loss: 0.6754 - acc: 0.5814 - val_loss: 0.5098 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 594us/step - loss: 0.6759 - acc: 0.5875 - val_loss: 0.5247 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 591us/step - loss: 0.6766 - acc: 0.5875 - val_loss: 0.5367 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 555us/step - loss: 0.6762 - acc: 0.5860 - val_loss: 0.5309 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 556us/step - loss: 0.6753 - acc: 0.5814 - val_loss: 0.5024 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 566us/step - loss: 0.6766 - acc: 0.5860 - val_loss: 0.5221 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 661us/step - loss: 0.6766 - acc: 0.5860 - val_loss: 0.5144 - val_acc: 1.0000\n",
      "91/91 [==============================] - 0s 28us/step\n",
      "Average accuracy of model on the dev set =  0.6593971721707417\n",
      "Training on fold 8/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_93 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.7028 - acc: 0.4916 - val_loss: 0.6020 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 579us/step - loss: 0.6804 - acc: 0.5875 - val_loss: 0.5431 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 575us/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5718 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 554us/step - loss: 0.6789 - acc: 0.5875 - val_loss: 0.5251 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 572us/step - loss: 0.6796 - acc: 0.5875 - val_loss: 0.5268 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 578us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5314 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 687us/step - loss: 0.6794 - acc: 0.5875 - val_loss: 0.5185 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 572us/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5349 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 574us/step - loss: 0.6796 - acc: 0.5875 - val_loss: 0.5392 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 690us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5331 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 675us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5189 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 596us/step - loss: 0.6793 - acc: 0.5875 - val_loss: 0.5370 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 692us/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5458 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 684us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5381 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 1s 774us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5070 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 578us/step - loss: 0.6791 - acc: 0.5875 - val_loss: 0.5209 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 580us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5194 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 600us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5495 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 607us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5290 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 619us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5327 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 589us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5523 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 605us/step - loss: 0.6786 - acc: 0.5875 - val_loss: 0.5171 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 582us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5403 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 612us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5260 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 661us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5088 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 610us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5256 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 609us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5469 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 602us/step - loss: 0.6777 - acc: 0.5875 - val_loss: 0.5137 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 626us/step - loss: 0.6777 - acc: 0.5875 - val_loss: 0.5081 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 664us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5252 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 574us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5064 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 576us/step - loss: 0.6777 - acc: 0.5875 - val_loss: 0.5266 - val_acc: 1.0000\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 583us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5361 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 614us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5103 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 664us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5437 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 623us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5256 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 628us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5008 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 585us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5233 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 617us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5322 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 601us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5297 - val_acc: 1.0000\n",
      "91/91 [==============================] - 0s 40us/step\n",
      "Average accuracy of model on the dev set =  0.6607637344406077\n",
      "Training on fold 9/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_99 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_103 (Dense)            (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.6951 - acc: 0.5875 - val_loss: 0.4649 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 608us/step - loss: 0.6798 - acc: 0.5875 - val_loss: 0.5180 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 611us/step - loss: 0.6790 - acc: 0.5875 - val_loss: 0.5389 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 590us/step - loss: 0.6790 - acc: 0.5875 - val_loss: 0.5346 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 580us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5501 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 612us/step - loss: 0.6789 - acc: 0.5875 - val_loss: 0.5471 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 612us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5512 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 671us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5476 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 699us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5185 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 714us/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5354 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 733us/step - loss: 0.6786 - acc: 0.5875 - val_loss: 0.5329 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 750us/step - loss: 0.6791 - acc: 0.5875 - val_loss: 0.5174 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 727us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5376 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 1s 761us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5390 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 707us/step - loss: 0.6786 - acc: 0.5875 - val_loss: 0.5562 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 677us/step - loss: 0.6788 - acc: 0.5875 - val_loss: 0.5472 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 706us/step - loss: 0.6784 - acc: 0.5875 - val_loss: 0.5374 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 631us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5370 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 612us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5189 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 609us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5241 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 647us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5485 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 587us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5495 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 571us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5348 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 601us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5381 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 588us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5061 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 611us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5425 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 591us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5243 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 617us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5376 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 611us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5462 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 609us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5440 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 591us/step - loss: 0.6789 - acc: 0.5875 - val_loss: 0.5633 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 606us/step - loss: 0.6783 - acc: 0.5860 - val_loss: 0.5570 - val_acc: 1.0000\n",
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 593us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5215 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 583us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5208 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 591us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5304 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 587us/step - loss: 0.6777 - acc: 0.5875 - val_loss: 0.5370 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 581us/step - loss: 0.6782 - acc: 0.5875 - val_loss: 0.5334 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 578us/step - loss: 0.6786 - acc: 0.5875 - val_loss: 0.5423 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 586us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5377 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 584us/step - loss: 0.6780 - acc: 0.5875 - val_loss: 0.5470 - val_acc: 1.0000\n",
      "91/91 [==============================] - 0s 29us/step\n",
      "Average accuracy of model on the dev set =  0.6618266162060591\n",
      "Training on fold 10/10.............................................\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_105 (Dense)            (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 16)                528       \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 16)                272       \n",
      "_________________________________________________________________\n",
      "dense_109 (Dense)            (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,721\n",
      "Trainable params: 2,721\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 657 samples, validate on 165 samples\n",
      "Epoch 1/40\n",
      "657/657 [==============================] - 2s 3ms/step - loss: 0.6932 - acc: 0.5875 - val_loss: 0.4771 - val_acc: 1.0000\n",
      "Epoch 2/40\n",
      "657/657 [==============================] - 0s 613us/step - loss: 0.6794 - acc: 0.5875 - val_loss: 0.5241 - val_acc: 1.0000\n",
      "Epoch 3/40\n",
      "657/657 [==============================] - 0s 606us/step - loss: 0.6787 - acc: 0.5875 - val_loss: 0.5423 - val_acc: 1.0000\n",
      "Epoch 4/40\n",
      "657/657 [==============================] - 0s 601us/step - loss: 0.6785 - acc: 0.5875 - val_loss: 0.5141 - val_acc: 1.0000\n",
      "Epoch 5/40\n",
      "657/657 [==============================] - 0s 607us/step - loss: 0.6789 - acc: 0.5875 - val_loss: 0.5099 - val_acc: 1.0000\n",
      "Epoch 6/40\n",
      "657/657 [==============================] - 0s 573us/step - loss: 0.6783 - acc: 0.5875 - val_loss: 0.5195 - val_acc: 1.0000\n",
      "Epoch 7/40\n",
      "657/657 [==============================] - 0s 644us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5202 - val_acc: 1.0000\n",
      "Epoch 8/40\n",
      "657/657 [==============================] - 0s 707us/step - loss: 0.6779 - acc: 0.5875 - val_loss: 0.5172 - val_acc: 1.0000\n",
      "Epoch 9/40\n",
      "657/657 [==============================] - 0s 730us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5343 - val_acc: 1.0000\n",
      "Epoch 10/40\n",
      "657/657 [==============================] - 0s 702us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5271 - val_acc: 1.0000\n",
      "Epoch 11/40\n",
      "657/657 [==============================] - 0s 640us/step - loss: 0.6778 - acc: 0.5875 - val_loss: 0.5314 - val_acc: 1.0000\n",
      "Epoch 12/40\n",
      "657/657 [==============================] - 0s 640us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5373 - val_acc: 1.0000\n",
      "Epoch 13/40\n",
      "657/657 [==============================] - 0s 662us/step - loss: 0.6781 - acc: 0.5875 - val_loss: 0.5412 - val_acc: 1.0000\n",
      "Epoch 14/40\n",
      "657/657 [==============================] - 0s 645us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5420 - val_acc: 1.0000\n",
      "Epoch 15/40\n",
      "657/657 [==============================] - 0s 719us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5282 - val_acc: 1.0000\n",
      "Epoch 16/40\n",
      "657/657 [==============================] - 0s 710us/step - loss: 0.6776 - acc: 0.5875 - val_loss: 0.5276 - val_acc: 1.0000\n",
      "Epoch 17/40\n",
      "657/657 [==============================] - 0s 711us/step - loss: 0.6774 - acc: 0.5875 - val_loss: 0.5154 - val_acc: 1.0000\n",
      "Epoch 18/40\n",
      "657/657 [==============================] - 0s 708us/step - loss: 0.6772 - acc: 0.5875 - val_loss: 0.5210 - val_acc: 1.0000\n",
      "Epoch 19/40\n",
      "657/657 [==============================] - 0s 667us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5287 - val_acc: 1.0000\n",
      "Epoch 20/40\n",
      "657/657 [==============================] - 0s 618us/step - loss: 0.6765 - acc: 0.5875 - val_loss: 0.5288 - val_acc: 1.0000\n",
      "Epoch 21/40\n",
      "657/657 [==============================] - 0s 618us/step - loss: 0.6769 - acc: 0.5875 - val_loss: 0.5302 - val_acc: 1.0000\n",
      "Epoch 22/40\n",
      "657/657 [==============================] - 0s 746us/step - loss: 0.6761 - acc: 0.5875 - val_loss: 0.5127 - val_acc: 1.0000\n",
      "Epoch 23/40\n",
      "657/657 [==============================] - 0s 638us/step - loss: 0.6762 - acc: 0.5875 - val_loss: 0.4855 - val_acc: 1.0000\n",
      "Epoch 24/40\n",
      "657/657 [==============================] - 0s 638us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5191 - val_acc: 1.0000\n",
      "Epoch 25/40\n",
      "657/657 [==============================] - 0s 656us/step - loss: 0.6775 - acc: 0.5875 - val_loss: 0.5170 - val_acc: 1.0000\n",
      "Epoch 26/40\n",
      "657/657 [==============================] - 0s 587us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5211 - val_acc: 1.0000\n",
      "Epoch 27/40\n",
      "657/657 [==============================] - 0s 608us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5275 - val_acc: 1.0000\n",
      "Epoch 28/40\n",
      "657/657 [==============================] - 0s 605us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5011 - val_acc: 1.0000\n",
      "Epoch 29/40\n",
      "657/657 [==============================] - 0s 689us/step - loss: 0.6766 - acc: 0.5875 - val_loss: 0.5200 - val_acc: 1.0000\n",
      "Epoch 30/40\n",
      "657/657 [==============================] - 0s 712us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5204 - val_acc: 1.0000\n",
      "Epoch 31/40\n",
      "657/657 [==============================] - 0s 710us/step - loss: 0.6761 - acc: 0.5875 - val_loss: 0.5128 - val_acc: 1.0000\n",
      "Epoch 32/40\n",
      "657/657 [==============================] - 0s 727us/step - loss: 0.6768 - acc: 0.5875 - val_loss: 0.5315 - val_acc: 1.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/40\n",
      "657/657 [==============================] - 0s 615us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5422 - val_acc: 1.0000\n",
      "Epoch 34/40\n",
      "657/657 [==============================] - 0s 744us/step - loss: 0.6766 - acc: 0.5875 - val_loss: 0.5424 - val_acc: 1.0000\n",
      "Epoch 35/40\n",
      "657/657 [==============================] - 0s 608us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5312 - val_acc: 1.0000\n",
      "Epoch 36/40\n",
      "657/657 [==============================] - 0s 619us/step - loss: 0.6762 - acc: 0.5875 - val_loss: 0.5119 - val_acc: 1.0000\n",
      "Epoch 37/40\n",
      "657/657 [==============================] - 0s 621us/step - loss: 0.6767 - acc: 0.5875 - val_loss: 0.5141 - val_acc: 1.0000\n",
      "Epoch 38/40\n",
      "657/657 [==============================] - 0s 644us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5156 - val_acc: 1.0000\n",
      "Epoch 39/40\n",
      "657/657 [==============================] - 0s 567us/step - loss: 0.6763 - acc: 0.5875 - val_loss: 0.5031 - val_acc: 1.0000\n",
      "Epoch 40/40\n",
      "657/657 [==============================] - 0s 622us/step - loss: 0.6760 - acc: 0.5875 - val_loss: 0.5110 - val_acc: 1.0000\n",
      "91/91 [==============================] - 0s 38us/step\n",
      "Average accuracy of model on the dev set =  0.6626769216184203\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=10, random_state=12)\n",
    "avg_loss = []\n",
    "avg_acc = []\n",
    "# Loop through the indices the split() method returns\n",
    "for index, (train_index, test_index) in enumerate(skf.split(all_data, labels)):\n",
    "    print(\"Training on fold \" + str(index + 1) + \"/10.............................................\")\n",
    "    # Generate batches from indices\n",
    "    x_train, x_test = all_data[train_index], all_data[test_index]\n",
    "    # use one-hot vectors as labels\n",
    "    y_train, y_test = labels[train_index], labels[test_index]\n",
    "\n",
    "    network = models.Sequential()\n",
    "    \n",
    "\n",
    "    network.add(layers.Dense(32, input_shape=(8,)))\n",
    "    network.add(layers.Dense(32, activation=\"relu\"))\n",
    "    network.add(layers.Dense(16, activation=\"relu\"))\n",
    "    # network.add(layers.Dropout(0.3))\n",
    "    network.add(layers.Dense(16, activation=\"relu\"))\n",
    "    # network.add(layers.Dropout(0.3))\n",
    "    network.add(layers.Dense(32, activation=\"sigmoid\"))\n",
    "    network.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Adam = Adam(lr=0.05)\n",
    "    network.compile(optimizer=Adam(lr=0.0004),\n",
    "                    loss='binary_crossentropy',\n",
    "                    metrics=['acc'])\n",
    "\n",
    "    network.summary()\n",
    "\n",
    "    history = network.fit(x_train, y_train, validation_split=0.2,\n",
    "                          epochs=40, verbose=1, batch_size=3)\n",
    "\n",
    "    loss, accuracy = network.evaluate(x_test, y_test)\n",
    "\n",
    "    # evaluate and store the accuracy\n",
    "#     loss, accuracy = model.evaluate(xtest_imagelist, ytest, verbose=1)\n",
    "    avg_loss.append(loss)\n",
    "    avg_acc.append(accuracy)\n",
    "\n",
    "    # cross validation score\n",
    "    print(\"Average accuracy of model on the dev set = \", np.mean(avg_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
